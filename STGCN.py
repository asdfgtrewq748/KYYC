import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import math
import time
import warnings
from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt

# å¿½ç•¥ CUDA å…¼å®¹æ€§è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, message='.*CUDA capability.*')
warnings.filterwarnings('ignore', category=UserWarning, message='.*NVIDIA.*')

# è®¾ç½® CUDA ç¯å¢ƒ
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

# è§£å†³ OpenMP å†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# ----------------------------------------------------------------------
# 1. å¸®åŠ©å‡½æ•° (Utils)
# ----------------------------------------------------------------------

def add_time_features(X, feature_names=None):
    """
    â­ æ–°å¢ï¼šæ·»åŠ æ—¶é—´ç›¸å…³ç‰¹å¾ï¼ˆæ—¶é—´ç´¢å¼•ã€å‘¨æœŸæ€§ï¼‰
    :param X: è¾“å…¥æ•°æ® (samples, seq_len, features)
    :return: å¢å¼ºåçš„X, æ–°ç‰¹å¾ååˆ—è¡¨
    """
    samples, seq_len, F = X.shape
    X_time_features = []
    
    new_feature_names = feature_names.copy() if feature_names else [f'feat_{i}' for i in range(F)]
    
    # 1. æ—¶é—´ç´¢å¼•ç‰¹å¾ï¼ˆå½’ä¸€åŒ–åˆ°[0,1]ï¼‰
    time_idx = np.arange(seq_len).reshape(1, -1, 1) / seq_len
    time_idx = np.repeat(time_idx, samples, axis=0)
    X_time_features.append(time_idx)
    new_feature_names.append('time_index')
    
    # 2. ä½ç½®ç¼–ç ï¼ˆç±»ä¼¼Transformerï¼‰
    position = np.arange(seq_len).reshape(-1, 1)
    div_term = np.exp(np.arange(0, 8, 2) * -(np.log(10000.0) / 8))
    pos_encoding = np.zeros((seq_len, 8))
    pos_encoding[:, 0::2] = np.sin(position * div_term)
    pos_encoding[:, 1::2] = np.cos(position * div_term)
    pos_encoding = np.repeat(pos_encoding[np.newaxis, :, :], samples, axis=0)
    
    for i in range(8):
        X_time_features.append(pos_encoding[:, :, i:i+1])
        new_feature_names.append(f'pos_enc_{i}')
    
    # åˆå¹¶
    X_enhanced = np.concatenate([X] + X_time_features, axis=2)
    
    return X_enhanced, new_feature_names

def add_engineered_features(X, feature_names=None):
    """
    æ·»åŠ å·¥ç¨‹ç‰¹å¾ï¼Œæå‡æ¨¡å‹è¡¨ç°
    :param X: è¾“å…¥æ•°æ® (samples, seq_len, features) æˆ– (T, N, seq_len, features)
    :param feature_names: åŸå§‹ç‰¹å¾ååˆ—è¡¨
    :return: å¢å¼ºåçš„X, æ–°ç‰¹å¾ååˆ—è¡¨
    """
    is_spatial = (X.ndim == 4)  # åˆ¤æ–­æ˜¯å¦ä¸ºæ—¶ç©ºæ•°æ®
    
    if is_spatial:
        T, N, seq_len, F = X.shape
        X_new_features = []
    else:
        samples, seq_len, F = X.shape
        X_new_features = []
    
    new_feature_names = feature_names.copy() if feature_names else [f'feat_{i}' for i in range(F)]
    
    # â­ å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹ - ç›®æ ‡RÂ²â‰¥0.8
    
    # 1. ç»Ÿè®¡ç‰¹å¾ï¼ˆé’ˆå¯¹æ—¶åºç»´åº¦ï¼‰- å¤šç§ç»Ÿè®¡é‡
    if is_spatial:
        for feat_idx in range(F):
            feat_data = X[:, :, :, feat_idx]  # (T, N, seq_len)
            
            # åŸºç¡€ç»Ÿè®¡
            feat_mean = feat_data.mean(axis=2, keepdims=True)
            feat_mean = np.repeat(feat_mean, seq_len, axis=2)
            X_new_features.append(feat_mean[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_mean')
            
            feat_std = feat_data.std(axis=2, keepdims=True)
            feat_std = np.repeat(feat_std, seq_len, axis=2)
            X_new_features.append(feat_std[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_std')
            
            # æå€¼ç‰¹å¾
            feat_max = feat_data.max(axis=2, keepdims=True)
            feat_max = np.repeat(feat_max, seq_len, axis=2)
            X_new_features.append(feat_max[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_max')
            
            feat_min = feat_data.min(axis=2, keepdims=True)
            feat_min = np.repeat(feat_min, seq_len, axis=2)
            X_new_features.append(feat_min[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_min')
            
            # â­ æ–°å¢ï¼šèŒƒå›´å’Œååº¦
            feat_range = feat_max - feat_min
            X_new_features.append(feat_range[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_range')
            
            # â­ æ–°å¢ï¼šå˜å¼‚ç³»æ•°ï¼ˆCVï¼‰
            feat_cv = feat_std / (feat_mean + 1e-8)
            X_new_features.append(feat_cv[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_cv')
    else:
        for feat_idx in range(F):
            feat_data = X[:, :, feat_idx]  # (samples, seq_len)
            
            # åŸºç¡€ç»Ÿè®¡
            feat_mean = feat_data.mean(axis=1, keepdims=True)
            feat_mean = np.repeat(feat_mean, seq_len, axis=1)
            X_new_features.append(feat_mean[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_mean')
            
            feat_std = feat_data.std(axis=1, keepdims=True)
            feat_std = np.repeat(feat_std, seq_len, axis=1)
            X_new_features.append(feat_std[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_std')
            
            # æå€¼
            feat_max = feat_data.max(axis=1, keepdims=True)
            feat_max = np.repeat(feat_max, seq_len, axis=1)
            X_new_features.append(feat_max[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_max')
            
            feat_min = feat_data.min(axis=1, keepdims=True)
            feat_min = np.repeat(feat_min, seq_len, axis=1)
            X_new_features.append(feat_min[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_min')
            
            # â­ æ–°å¢ç‰¹å¾
            feat_range = feat_max - feat_min
            X_new_features.append(feat_range[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_range')
            
            feat_cv = feat_std / (feat_mean + 1e-8)
            X_new_features.append(feat_cv[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_cv')
    
    # 2. å·®åˆ†ç‰¹å¾ï¼ˆå¤šé˜¶ï¼‰
    if is_spatial:
        for feat_idx in range(F):
            feat_data = X[:, :, :, feat_idx]
            
            # ä¸€é˜¶å·®åˆ†ï¼ˆå˜åŒ–ç‡ï¼‰
            feat_diff1 = np.diff(feat_data, axis=2)
            feat_diff1 = np.concatenate([np.zeros((T, N, 1)), feat_diff1], axis=2)
            X_new_features.append(feat_diff1[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff1')
            
            # â­ äºŒé˜¶å·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼‰
            feat_diff2 = np.diff(feat_diff1, axis=2)
            feat_diff2 = np.concatenate([np.zeros((T, N, 1)), feat_diff2], axis=2)
            X_new_features.append(feat_diff2[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff2')
    else:
        for feat_idx in range(F):
            feat_data = X[:, :, feat_idx]
            
            # ä¸€é˜¶å·®åˆ†
            feat_diff1 = np.diff(feat_data, axis=1)
            feat_diff1 = np.concatenate([np.zeros((samples, 1)), feat_diff1], axis=1)
            X_new_features.append(feat_diff1[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff1')
            
            # â­ äºŒé˜¶å·®åˆ†
            feat_diff2 = np.diff(feat_diff1, axis=1)
            feat_diff2 = np.concatenate([np.zeros((samples, 1)), feat_diff2], axis=1)
            X_new_features.append(feat_diff2[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff2')
    
    # â­ 3. æ»‘åŠ¨çª—å£ç‰¹å¾ï¼ˆçŸ­æœŸå’Œé•¿æœŸï¼‰
    if is_spatial:
        for feat_idx in range(F):
            feat_data = X[:, :, :, feat_idx]
            
            # çŸ­æœŸè¶‹åŠ¿ï¼ˆæœ€è¿‘3æ­¥ï¼‰
            if seq_len >= 3:
                feat_recent_mean = np.zeros_like(feat_data)
                for i in range(seq_len):
                    start = max(0, i - 2)
                    feat_recent_mean[:, :, i] = feat_data[:, :, start:i+1].mean(axis=2)
                X_new_features.append(feat_recent_mean[:, :, :, np.newaxis])
                new_feature_names.append(f'{new_feature_names[feat_idx]}_recent3_mean')
    else:
        for feat_idx in range(F):
            feat_data = X[:, :, feat_idx]
            
            if seq_len >= 3:
                feat_recent_mean = np.zeros_like(feat_data)
                for i in range(seq_len):
                    start = max(0, i - 2)
                    feat_recent_mean[:, i] = feat_data[:, start:i+1].mean(axis=1)
                X_new_features.append(feat_recent_mean[:, :, np.newaxis])
                new_feature_names.append(f'{new_feature_names[feat_idx]}_recent3_mean')
    
    # â­ 4. äº¤å‰ç‰¹å¾ï¼ˆé’ˆå¯¹ç¬¬ä¸€ä¸ªç‰¹å¾ï¼Œé€šå¸¸æ˜¯å‹åŠ›å€¼ï¼‰
    if F > 1 and is_spatial:
        # å‹åŠ›å€¼ä¸å…¶ä»–ç‰¹å¾çš„æ¯”å€¼
        pressure_data = X[:, :, :, 0]  # å‡è®¾ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯å‹åŠ›
        for feat_idx in range(1, min(F, 5)):  # åªå–å‰5ä¸ªç‰¹å¾é¿å…è¿‡å¤š
            other_data = X[:, :, :, feat_idx]
            ratio = pressure_data / (other_data + 1e-8)
            X_new_features.append(ratio[:, :, :, np.newaxis])
            new_feature_names.append(f'pressure_to_{new_feature_names[feat_idx]}_ratio')
    elif F > 1:
        pressure_data = X[:, :, 0]
        for feat_idx in range(1, min(F, 5)):
            other_data = X[:, :, feat_idx]
            ratio = pressure_data / (other_data + 1e-8)
            X_new_features.append(ratio[:, :, np.newaxis])
            new_feature_names.append(f'pressure_to_{new_feature_names[feat_idx]}_ratio')
    
    # åˆå¹¶åŸå§‹ç‰¹å¾å’Œæ–°ç‰¹å¾
    if is_spatial:
        X_enhanced = np.concatenate([X] + X_new_features, axis=3)
    else:
        X_enhanced = np.concatenate([X] + X_new_features, axis=2)
    
    # â­ å…³é”®ä¿®å¤ï¼šæ£€æµ‹å¹¶å¤„ç†NaNå’ŒInfå€¼
    nan_mask = np.isnan(X_enhanced)
    inf_mask = np.isinf(X_enhanced)
    
    if nan_mask.any():
        print(f"âš ï¸ è­¦å‘Šï¼šç‰¹å¾å·¥ç¨‹äº§ç”Ÿäº† {nan_mask.sum()} ä¸ªNaNå€¼ï¼Œå·²æ›¿æ¢ä¸º0")
        X_enhanced[nan_mask] = 0
    
    if inf_mask.any():
        print(f"âš ï¸ è­¦å‘Šï¼šç‰¹å¾å·¥ç¨‹äº§ç”Ÿäº† {inf_mask.sum()} ä¸ªInfå€¼ï¼Œå·²è£å‰ª")
        X_enhanced[inf_mask] = np.sign(X_enhanced[inf_mask]) * 1e6  # æ›¿æ¢ä¸ºå¤§æ•°å€¼ä½†ä¸æ˜¯Inf
    
    # å†æ¬¡æ£€æŸ¥æ•°å€¼èŒƒå›´
    if np.abs(X_enhanced).max() > 1e10:
        print(f"âš ï¸ è­¦å‘Šï¼šç‰¹å¾å€¼è¿‡å¤§ï¼ˆmax={np.abs(X_enhanced).max():.2e}ï¼‰ï¼Œè¿›è¡Œè£å‰ª")
        X_enhanced = np.clip(X_enhanced, -1e10, 1e10)
    
    return X_enhanced, new_feature_names

def load_csv_data(csv_file, time_col=None):
    """
    ä» CSV æ–‡ä»¶åŠ è½½çŸ¿å‹æ•°æ®
    :param csv_file: CSV æ–‡ä»¶å¯¹è±¡
    :param time_col: æ—¶é—´åˆ—åç§°(å¦‚æœæœ‰)
    :return: numpy array (num_samples, num_nodes, num_features), column_names
    """
    df = pd.read_csv(csv_file)
    
    # å¦‚æœæœ‰æ—¶é—´åˆ—,åˆ é™¤å®ƒ
    if time_col and time_col in df.columns:
        df = df.drop(columns=[time_col])
    
    # å°è¯•è‡ªåŠ¨æ£€æµ‹æ—¶é—´åˆ—(å¸¸è§åç§°)
    time_cols = ['æ—¶é—´', 'time', 'Time', 'DATE', 'date', 'datetime', 'Datetime', 'æ—¥æœŸ']
    for col in time_cols:
        if col in df.columns:
            df = df.drop(columns=[col])
            break
    
    # ä¿å­˜åˆ—å(æ”¯æ¶åç§°)
    column_names = df.columns.tolist()
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    data = df.values  # (num_samples, num_nodes)
    
    # æ·»åŠ ç‰¹å¾ç»´åº¦ (å‡è®¾åªæœ‰ä¸€ä¸ªç‰¹å¾:å‹åŠ›å€¼)
    data = np.expand_dims(data, axis=-1)  # (num_samples, num_nodes, 1)
    
    return data, column_names

def augment_data(X, y, method='noise', strength=0.01):
    """
    â­ æ–°å¢ï¼šæ•°æ®å¢å¼ºå‡½æ•°ï¼Œå¢åŠ è®­ç»ƒæ ·æœ¬å¤šæ ·æ€§
    :param X: è¾“å…¥æ•°æ® (samples, seq_len, features)
    :param y: ç›®æ ‡æ•°æ® (samples,)
    :param method: å¢å¼ºæ–¹æ³• 'noise', 'scale', 'shift', 'flip'
    :param strength: å¢å¼ºå¼ºåº¦
    :return: å¢å¼ºåçš„X, y
    """
    X_aug = X.copy()
    
    if method == 'noise':
        # æ·»åŠ é«˜æ–¯å™ªå£°
        noise = np.random.normal(0, strength, X.shape)
        X_aug = X + noise
    
    elif method == 'scale':
        # éšæœºç¼©æ”¾
        scale = np.random.uniform(1-strength, 1+strength, (X.shape[0], 1, 1))
        X_aug = X * scale
    
    elif method == 'shift':
        # éšæœºå¹³ç§»
        shift = np.random.uniform(-strength, strength, (X.shape[0], 1, X.shape[2]))
        X_aug = X + shift
    
    elif method == 'flip':
        # æ—¶é—´åè½¬ï¼ˆé€‚ç”¨äºæŸäº›æ—¶åºæ•°æ®ï¼‰
        X_aug = np.flip(X, axis=1).copy()
    
    return X_aug, y

def load_processed_sequence_data(npz_file):
    """
    åŠ è½½é¢„å¤„ç†å¥½çš„åºåˆ—æ•°æ®é›†
    :param npz_file: .npz æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡
    :return: X, y_final, support_ids, feature_names
    """
    if isinstance(npz_file, str):
        data = np.load(npz_file, allow_pickle=True)
    else:
        data = np.load(npz_file, allow_pickle=True)
    
    X = data['X']  # (num_samples, seq_len, num_features)
    y_final = data['y_final']  # (num_samples, pred_len)
    support_ids = data['support_ids']  # (num_samples,)
    feature_names = data['feature_names'].tolist() if 'feature_names' in data else []
    
    return X, y_final, support_ids, feature_names

def find_optimal_learning_rate(model, train_loader, criterion, device, 
                               start_lr=1e-7, end_lr=1.0, num_iter=100):
    """
    â­ å­¦ä¹ ç‡èŒƒå›´æµ‹è¯• - è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡
    :param model: æ¨¡å‹
    :param train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
    :param criterion: æŸå¤±å‡½æ•°
    :param device: è®¾å¤‡
    :param start_lr: èµ·å§‹å­¦ä¹ ç‡
    :param end_lr: ç»“æŸå­¦ä¹ ç‡
    :param num_iter: è¿­ä»£æ¬¡æ•°
    :return: optimal_lr, lrs, losses
    """
    import streamlit as st
    
    model.train()
    optimizer = optim.Adam(model.parameters(), lr=start_lr)
    lr_mult = (end_lr / start_lr) ** (1 / num_iter)
    
    losses = []
    lrs = []
    best_loss = float('inf')
    
    st.write("ğŸ” æ­£åœ¨å¯»æ‰¾æœ€ä¼˜å­¦ä¹ ç‡...")
    progress_bar = st.progress(0)
    
    batch_iterator = iter(train_loader)
    
    for iteration in range(num_iter):
        try:
            batch_X, batch_y = next(batch_iterator)
        except StopIteration:
            batch_iterator = iter(train_loader)
            batch_X, batch_y = next(batch_iterator)
        
        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)
        
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # è®°å½•
        current_lr = optimizer.param_groups[0]['lr']
        losses.append(loss.item())
        lrs.append(current_lr)
        
        # æ›´æ–°å­¦ä¹ ç‡
        optimizer.param_groups[0]['lr'] *= lr_mult
        
        # å¦‚æœæŸå¤±çˆ†ç‚¸ï¼Œåœæ­¢
        if loss.item() < best_loss:
            best_loss = loss.item()
        if loss.item() > 4 * best_loss or np.isnan(loss.item()):
            break
        
        progress_bar.progress((iteration + 1) / num_iter)
    
    progress_bar.empty()
    
    # æ‰¾åˆ°lossä¸‹é™æœ€å¿«çš„ç‚¹ï¼ˆæ¢¯åº¦æœ€è´Ÿï¼‰
    if len(losses) > 10:
        # å¹³æ»‘å¤„ç†
        smoothed_losses = pd.Series(losses).rolling(window=5, min_periods=1).mean().values
        gradients = np.gradient(smoothed_losses)
        
        # æ‰¾åˆ°æ¢¯åº¦æœ€è´Ÿçš„ç‚¹ï¼Œä½†ä¸è¦å¤ªé è¿‘èµ·å§‹æˆ–ç»“æŸ
        valid_range = slice(len(gradients) // 10, len(gradients) * 9 // 10)
        optimal_idx = valid_range.start + np.argmin(gradients[valid_range])
        optimal_lr = lrs[optimal_idx]
    else:
        optimal_idx = len(losses) // 2
        optimal_lr = lrs[optimal_idx]
    
    return optimal_lr, lrs, losses

def analyze_feature_importance(model, X_val, y_val, feature_names, device, n_repeats=5):
    """
    â­ åˆ†æç‰¹å¾é‡è¦æ€§ï¼ˆæ’åˆ—é‡è¦æ€§æ³•ï¼‰
    :param model: è®­ç»ƒå¥½çš„æ¨¡å‹
    :param X_val: éªŒè¯é›†ç‰¹å¾
    :param y_val: éªŒè¯é›†æ ‡ç­¾
    :param feature_names: ç‰¹å¾ååˆ—è¡¨
    :param device: è®¾å¤‡
    :param n_repeats: é‡å¤æ¬¡æ•°
    :return: importance_dict
    """
    import streamlit as st
    
    model.eval()
    
    # è®¡ç®—åŸºå‡†åˆ†æ•°
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_val).to(device)
        y_tensor = torch.FloatTensor(y_val).to(device)
        baseline_pred = model(X_tensor)
        baseline_mse = torch.mean((baseline_pred.squeeze() - y_tensor.squeeze())**2).item()
    
    importances = np.zeros(X_val.shape[2])  # num_features
    
    st.write("ğŸ”¬ æ­£åœ¨åˆ†æç‰¹å¾é‡è¦æ€§...")
    progress_bar = st.progress(0)
    
    for feat_idx in range(X_val.shape[2]):
        feat_importances = []
        
        for _ in range(n_repeats):
            # å¤åˆ¶æ•°æ®å¹¶æ‰“ä¹±å½“å‰ç‰¹å¾
            X_permuted = X_val.copy()
            np.random.shuffle(X_permuted[:, :, feat_idx])
            
            # è®¡ç®—æ‰“ä¹±åçš„åˆ†æ•°
            with torch.no_grad():
                X_perm_tensor = torch.FloatTensor(X_permuted).to(device)
                perm_pred = model(X_perm_tensor)
                perm_mse = torch.mean((perm_pred.squeeze() - y_tensor.squeeze())**2).item()
            
            # é‡è¦æ€§ = æ€§èƒ½ä¸‹é™ç¨‹åº¦
            importance = perm_mse - baseline_mse
            feat_importances.append(importance)
        
        importances[feat_idx] = np.mean(feat_importances)
        progress_bar.progress((feat_idx + 1) / X_val.shape[2])
    
    progress_bar.empty()
    
    # å½’ä¸€åŒ–é‡è¦æ€§
    if importances.max() > 0:
        importances = importances / importances.max()
    
    # åˆ›å»ºå­—å…¸
    importance_dict = {
        feature_names[i]: importances[i] 
        for i in range(len(feature_names))
    }
    
    return importance_dict

class MultiMetricEarlyStopping:
    """
    â­ åŸºäºå¤šä¸ªæŒ‡æ ‡çš„æ—©åœç­–ç•¥
    """
    def __init__(self, patience=25, min_delta=1e-6, metrics=['val_loss', 'r2']):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_scores = {}
        self.metrics = metrics
        self.improved_count = 0
        
        # åˆå§‹åŒ–æœ€ä½³åˆ†æ•°
        for metric in metrics:
            if metric in ['val_loss', 'mae', 'rmse']:
                self.best_scores[metric] = float('inf')
            else:  # r2, accuracyç­‰
                self.best_scores[metric] = float('-inf')
    
    def __call__(self, current_scores):
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        :param current_scores: dict, å¦‚ {'val_loss': 0.1, 'r2': 0.8}
        :return: Trueè¡¨ç¤ºåº”è¯¥åœæ­¢ï¼ŒFalseè¡¨ç¤ºç»§ç»­
        """
        improved = 0
        
        for metric in self.metrics:
            if metric not in current_scores:
                continue
            
            current = current_scores[metric]
            best = self.best_scores[metric]
            
            # åˆ¤æ–­æ˜¯å¦æ”¹å–„
            if metric in ['val_loss', 'mae', 'rmse']:
                # è¶Šå°è¶Šå¥½
                if current < best - self.min_delta:
                    self.best_scores[metric] = current
                    improved += 1
            else:
                # è¶Šå¤§è¶Šå¥½ (r2ç­‰)
                if current > best + self.min_delta:
                    self.best_scores[metric] = current
                    improved += 1
        
        # å¦‚æœè‡³å°‘ä¸€åŠæŒ‡æ ‡æ”¹å–„ï¼Œé‡ç½®è®¡æ•°å™¨
        if improved >= len(self.metrics) / 2:
            self.counter = 0
            self.improved_count += 1
            return False, True  # ä¸åœæ­¢ï¼Œæœ‰æ”¹å–„
        else:
            self.counter += 1
            if self.counter >= self.patience:
                return True, False  # åœæ­¢ï¼Œæ— æ”¹å–„
        
        return False, False  # ä¸åœæ­¢ï¼Œæ— æ”¹å–„
    
    def get_best_scores(self):
        """è¿”å›æœ€ä½³åˆ†æ•°"""
        return self.best_scores

def combined_loss(pred, target, alpha=0.5, beta=0.3, gamma=0.2):
    """
    ç»„åˆæŸå¤±å‡½æ•°ï¼šHuber + MAE + MAPE
    
    ä¼˜ç‚¹ï¼š
    1. Huber: å¯¹å¼‚å¸¸å€¼é²æ£’
    2. MAE: ä¼˜åŒ–ç»å¯¹è¯¯å·®
    3. MAPE: ä¼˜åŒ–ç›¸å¯¹è¯¯å·®ï¼ˆå¯¹å¤§å°å€¼éƒ½å…¬å¹³ï¼‰
    
    :param pred: é¢„æµ‹å€¼ (batch_size, 1)
    :param target: çœŸå®å€¼ (batch_size, 1)
    :param alpha: HuberæŸå¤±æƒé‡
    :param beta: MAEæŸå¤±æƒé‡
    :param gamma: MAPEæŸå¤±æƒé‡
    :return: ç»„åˆæŸå¤±
    """
    import torch.nn.functional as F
    
    # 1. HuberæŸå¤± (å¯¹å¼‚å¸¸å€¼é²æ£’)
    huber_loss = F.smooth_l1_loss(pred, target)
    
    # 2. MAEæŸå¤± (L1èŒƒæ•°)
    mae_loss = torch.mean(torch.abs(pred - target))
    
    # 3. MAPEæŸå¤± (Mean Absolute Percentage Error)
    # ä¸ºé¿å…é™¤é›¶ï¼Œæ·»åŠ å°å¸¸æ•°epsilon
    epsilon = 1e-8
    mape_loss = torch.mean(torch.abs((target - pred) / (target + epsilon)))
    
    # ç»„åˆæŸå¤±
    total_loss = alpha * huber_loss + beta * mae_loss + gamma * mape_loss
    
    return total_loss

def train_ensemble_models(model_class, model_params, train_loader, val_loader, 
                         device, criterion, epochs, n_models=3, seed_base=42):
    """
    è®­ç»ƒé›†æˆæ¨¡å‹ - ä½¿ç”¨ä¸åŒéšæœºç§å­è®­ç»ƒå¤šä¸ªæ¨¡å‹
    
    :param model_class: æ¨¡å‹ç±» (SimpleLSTM, AttentionLSTM, TransformerPredictor)
    :param model_params: æ¨¡å‹å‚æ•°å­—å…¸
    :param train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
    :param val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
    :param device: è®¾å¤‡
    :param criterion: æŸå¤±å‡½æ•°
    :param epochs: è®­ç»ƒè½®æ•°
    :param n_models: é›†æˆæ¨¡å‹æ•°é‡
    :param seed_base: éšæœºç§å­åŸºæ•°
    :return: è®­ç»ƒå¥½çš„æ¨¡å‹åˆ—è¡¨
    """
    import torch
    import streamlit as st
    
    models = []
    st.info(f"ğŸ”„ å¼€å§‹è®­ç»ƒ {n_models} ä¸ªé›†æˆæ¨¡å‹...")
    
    for i in range(n_models):
        # è®¾ç½®ä¸åŒçš„éšæœºç§å­
        seed = seed_base + i * 100
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        
        # åˆ›å»ºæ–°æ¨¡å‹
        model = model_class(**model_params).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
        
        st.write(f"  è®­ç»ƒç¬¬ {i+1}/{n_models} ä¸ªæ¨¡å‹ (seed={seed})...")
        
        # ç®€åŒ–è®­ç»ƒï¼ˆåªè®­ç»ƒè¾ƒå°‘è½®æ¬¡ï¼‰
        model.train()
        for epoch in range(epochs // n_models):  # æ¯ä¸ªæ¨¡å‹è®­ç»ƒè¾ƒå°‘è½®æ•°
            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(device)
                batch_y = batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
        
        models.append(model)
    
    st.success(f"âœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼å…± {n_models} ä¸ªæ¨¡å‹")
    return models

def ensemble_predict(models, X, device):
    """
    é›†æˆé¢„æµ‹ - å¤šä¸ªæ¨¡å‹é¢„æµ‹ç»“æœçš„å¹³å‡
    
    :param models: æ¨¡å‹åˆ—è¡¨
    :param X: è¾“å…¥æ•°æ®
    :param device: è®¾å¤‡
    :return: å¹³å‡é¢„æµ‹ç»“æœ
    """
    import torch
    
    predictions = []
    for model in models:
        model.eval()
        with torch.no_grad():
            pred = model(X.to(device))
            predictions.append(pred)
    
    # å¹³å‡æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
    ensemble_pred = torch.mean(torch.stack(predictions), dim=0)
    return ensemble_pred

def find_optimal_batch_size(model, sample_input, device, start_size=16, max_size=1024):
    """
    åŠ¨æ€æŸ¥æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å° - è‡ªåŠ¨æµ‹è¯•æœ€å¤§å¯ç”¨æ‰¹æ¬¡
    
    ç­–ç•¥ï¼šäºŒåˆ†æŸ¥æ‰¾ + OOMæ•è·
    
    :param model: æ¨¡å‹å®ä¾‹
    :param sample_input: æ ·æœ¬è¾“å…¥ (seq_len, num_features)
    :param device: è®¾å¤‡
    :param start_size: èµ·å§‹æ‰¹æ¬¡å¤§å°
    :param max_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
    :return: æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    """
    import torch
    import streamlit as st
    
    def test_batch_size(batch_size):
        """æµ‹è¯•æŒ‡å®šæ‰¹æ¬¡å¤§å°æ˜¯å¦å¯è¡Œ"""
        try:
            # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
            test_batch = sample_input.unsqueeze(0).repeat(batch_size, 1, 1).to(device)
            
            # å‰å‘ä¼ æ’­
            model.eval()
            with torch.no_grad():
                _ = model(test_batch)
            
            # æ¸…ç†å†…å­˜
            del test_batch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return True
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                return False
            raise e
    
    # äºŒåˆ†æŸ¥æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    left, right = start_size, max_size
    optimal_size = start_size
    
    st.info("ğŸ” æ­£åœ¨æŸ¥æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°...")
    
    while left <= right:
        mid = (left + right) // 2
        
        if test_batch_size(mid):
            optimal_size = mid
            left = mid + 1
            st.write(f"  âœ… batch_size={mid} å¯è¡Œï¼Œå°è¯•æ›´å¤§...")
        else:
            right = mid - 1
            st.write(f"  âŒ batch_size={mid} å†…å­˜ä¸è¶³ï¼Œå°è¯•æ›´å°...")
    
    # æ·»åŠ å®‰å…¨è¾¹é™… (ä½¿ç”¨90%çš„æœ€å¤§å€¼)
    safe_size = int(optimal_size * 0.9)
    st.success(f"âœ… æ‰¾åˆ°æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {optimal_size} â†’ å®‰å…¨æ‰¹æ¬¡: {safe_size}")
    
    return safe_size

def k_fold_cross_validation(X, y, model_class, model_params, device, 
                            k_folds=5, epochs=50, batch_size=32):
    """
    KæŠ˜äº¤å‰éªŒè¯ - æé«˜æ¨¡å‹è¯„ä¼°çš„é²æ£’æ€§
    
    :param X: ç‰¹å¾æ•°æ® (num_samples, seq_len, num_features)
    :param y: ç›®æ ‡æ•°æ® (num_samples, 1)
    :param model_class: æ¨¡å‹ç±»
    :param model_params: æ¨¡å‹å‚æ•°
    :param device: è®¾å¤‡
    :param k_folds: æŠ˜æ•°
    :param epochs: æ¯æŠ˜è®­ç»ƒè½®æ•°
    :param batch_size: æ‰¹æ¬¡å¤§å°
    :return: äº¤å‰éªŒè¯ç»“æœå­—å…¸
    """
    import torch
    from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler
    from sklearn.model_selection import KFold
    import streamlit as st
    import numpy as np
    
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    
    results = {
        'fold_r2': [],
        'fold_mae': [],
        'fold_rmse': [],
        'models': []
    }
    
    st.info(f"ğŸ”„ å¼€å§‹ {k_folds} æŠ˜äº¤å‰éªŒè¯...")
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):
        st.write(f"  è®­ç»ƒç¬¬ {fold+1}/{k_folds} æŠ˜...")
        
        # åˆ’åˆ†æ•°æ®
        train_X = X[train_idx]
        train_y = y[train_idx]
        val_X = X[val_idx]
        val_y = y[val_idx]
        
        # è½¬æ¢ä¸ºTensor
        train_X_tensor = torch.FloatTensor(train_X).to(device)
        train_y_tensor = torch.FloatTensor(train_y).to(device)
        val_X_tensor = torch.FloatTensor(val_X).to(device)
        val_y_tensor = torch.FloatTensor(val_y).to(device)
        
        # åˆ›å»ºDataLoader
        train_dataset = TensorDataset(train_X_tensor, train_y_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # åˆ›å»ºæ¨¡å‹
        model = model_class(**model_params).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = torch.nn.SmoothL1Loss()
        
        # è®­ç»ƒ
        for epoch in range(epochs):
            model.train()
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
        
        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_pred = model(val_X_tensor)
            val_pred_np = val_pred.cpu().numpy()
            val_y_np = val_y
            
            # è®¡ç®—æŒ‡æ ‡
            from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
            r2 = r2_score(val_y_np, val_pred_np)
            mae = mean_absolute_error(val_y_np, val_pred_np)
            rmse = np.sqrt(mean_squared_error(val_y_np, val_pred_np))
            
            results['fold_r2'].append(r2)
            results['fold_mae'].append(mae)
            results['fold_rmse'].append(rmse)
            results['models'].append(model)
            
            st.write(f"    ç¬¬{fold+1}æŠ˜: RÂ²={r2:.4f}, MAE={mae:.2f}, RMSE={rmse:.2f}")
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_r2 = np.mean(results['fold_r2'])
    avg_mae = np.mean(results['fold_mae'])
    avg_rmse = np.mean(results['fold_rmse'])
    
    st.success(f"""
    âœ… äº¤å‰éªŒè¯å®Œæˆï¼
    å¹³å‡ RÂ² = {avg_r2:.4f} (Â±{np.std(results['fold_r2']):.4f})
    å¹³å‡ MAE = {avg_mae:.2f} (Â±{np.std(results['fold_mae']):.2f})
    å¹³å‡ RMSE = {avg_rmse:.2f} (Â±{np.std(results['fold_rmse']):.2f})
    """)
    
    results['avg_r2'] = avg_r2
    results['avg_mae'] = avg_mae
    results['avg_rmse'] = avg_rmse
    
    return results

def plot_advanced_diagnostics(y_true, y_pred, train_losses, val_losses):
    """
    é«˜çº§è¯Šæ–­å¯è§†åŒ– - ä½¿ç”¨plotlyåˆ›å»ºäº¤äº’å¼å›¾è¡¨
    
    åŒ…æ‹¬ï¼š
    1. æ®‹å·®å›¾ (Residual Plot)
    2. Q-Qå›¾ (æ­£æ€æ€§æ£€éªŒ)
    3. å­¦ä¹ æ›²çº¿ (è®­ç»ƒ/éªŒè¯æŸå¤±)
    4. é¢„æµ‹vsçœŸå®æ•£ç‚¹å›¾
    
    :param y_true: çœŸå®å€¼
    :param y_pred: é¢„æµ‹å€¼
    :param train_losses: è®­ç»ƒæŸå¤±å†å²
    :param val_losses: éªŒè¯æŸå¤±å†å²
    """
    import streamlit as st
    import numpy as np
    
    try:
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import scipy.stats as stats
        
        # è®¡ç®—æ®‹å·®
        residuals = y_true - y_pred
        
        # åˆ›å»º2x2å­å›¾
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('æ®‹å·®å›¾', 'Q-Qå›¾ (æ­£æ€æ€§æ£€éªŒ)', 
                          'å­¦ä¹ æ›²çº¿', 'é¢„æµ‹ vs çœŸå®')
        )
        
        # 1. æ®‹å·®å›¾
        fig.add_trace(
            go.Scatter(x=y_pred.flatten(), y=residuals.flatten(), 
                      mode='markers', marker=dict(size=3, opacity=0.5),
                      name='æ®‹å·®'),
            row=1, col=1
        )
        fig.add_hline(y=0, line_dash="dash", line_color="red", row=1, col=1)
        
        # 2. Q-Qå›¾
        theoretical_quantiles = stats.probplot(residuals.flatten(), dist="norm")[0][0]
        sample_quantiles = stats.probplot(residuals.flatten(), dist="norm")[0][1]
        fig.add_trace(
            go.Scatter(x=theoretical_quantiles, y=sample_quantiles,
                      mode='markers', marker=dict(size=3),
                      name='Q-Qç‚¹'),
            row=1, col=2
        )
        # æ·»åŠ ç†æƒ³çº¿
        fig.add_trace(
            go.Scatter(x=theoretical_quantiles, y=theoretical_quantiles,
                      mode='lines', line=dict(color='red', dash='dash'),
                      name='ç†æƒ³çº¿'),
            row=1, col=2
        )
        
        # 3. å­¦ä¹ æ›²çº¿
        epochs = list(range(1, len(train_losses) + 1))
        fig.add_trace(
            go.Scatter(x=epochs, y=train_losses, mode='lines',
                      name='è®­ç»ƒæŸå¤±', line=dict(color='blue')),
            row=2, col=1
        )
        fig.add_trace(
            go.Scatter(x=epochs, y=val_losses, mode='lines',
                      name='éªŒè¯æŸå¤±', line=dict(color='orange')),
            row=2, col=1
        )
        
        # 4. é¢„æµ‹vsçœŸå®
        fig.add_trace(
            go.Scatter(x=y_true.flatten(), y=y_pred.flatten(),
                      mode='markers', marker=dict(size=3, opacity=0.5),
                      name='é¢„æµ‹å€¼'),
            row=2, col=2
        )
        # æ·»åŠ ç†æƒ³çº¿ y=x
        min_val = min(y_true.min(), y_pred.min())
        max_val = max(y_true.max(), y_pred.max())
        fig.add_trace(
            go.Scatter(x=[min_val, max_val], y=[min_val, max_val],
                      mode='lines', line=dict(color='red', dash='dash'),
                      name='y=x'),
            row=2, col=2
        )
        
        # æ›´æ–°å¸ƒå±€
        fig.update_xaxes(title_text="é¢„æµ‹å€¼", row=1, col=1)
        fig.update_yaxes(title_text="æ®‹å·®", row=1, col=1)
        fig.update_xaxes(title_text="ç†è®ºåˆ†ä½æ•°", row=1, col=2)
        fig.update_yaxes(title_text="æ ·æœ¬åˆ†ä½æ•°", row=1, col=2)
        fig.update_xaxes(title_text="è®­ç»ƒè½®æ¬¡", row=2, col=1)
        fig.update_yaxes(title_text="æŸå¤±", row=2, col=1)
        fig.update_xaxes(title_text="çœŸå®å€¼", row=2, col=2)
        fig.update_yaxes(title_text="é¢„æµ‹å€¼", row=2, col=2)
        
        fig.update_layout(height=800, showlegend=True, title_text="æ¨¡å‹è¯Šæ–­å¯è§†åŒ–")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        st.write("**æ®‹å·®ç»Ÿè®¡**")
        st.write(f"- å‡å€¼: {np.mean(residuals):.4f}")
        st.write(f"- æ ‡å‡†å·®: {np.std(residuals):.4f}")
        st.write(f"- ååº¦: {stats.skew(residuals.flatten()):.4f}")
        st.write(f"- å³°åº¦: {stats.kurtosis(residuals.flatten()):.4f}")
        
        # Shapiro-Wilkæ­£æ€æ€§æ£€éªŒ
        if len(residuals) < 5000:  # æ ·æœ¬é‡å¤ªå¤§æ—¶ä¼šå¾ˆæ…¢
            shapiro_stat, shapiro_p = stats.shapiro(residuals.flatten()[:5000])
            st.write(f"- Shapiro-Wilkæ£€éªŒ på€¼: {shapiro_p:.4f}")
            if shapiro_p > 0.05:
                st.success("âœ… æ®‹å·®æ¥è¿‘æ­£æ€åˆ†å¸ƒ (p>0.05)")
            else:
                st.warning("âš ï¸ æ®‹å·®åç¦»æ­£æ€åˆ†å¸ƒ (p<0.05)")
        
    except ImportError:
        st.warning("âš ï¸ æœªå®‰è£…plotlyï¼Œè·³è¿‡é«˜çº§å¯è§†åŒ–ã€‚è¯·è¿è¡Œ: pip install plotly scipy")
    except Exception as e:
        st.error(f"å¯è§†åŒ–å‡ºé”™: {str(e)}")

def grid_search_hyperparameters(X, y, model_class, device, 
                                param_grid=None, n_trials=10):
    """
    è¶…å‚æ•°ç½‘æ ¼æœç´¢ - è‡ªåŠ¨æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°ç»„åˆ
    
    ä½¿ç”¨éšæœºæœç´¢ç­–ç•¥ï¼ˆæ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆï¼‰
    
    :param X: ç‰¹å¾æ•°æ®
    :param y: ç›®æ ‡æ•°æ®
    :param model_class: æ¨¡å‹ç±»
    :param device: è®¾å¤‡
    :param param_grid: å‚æ•°æœç´¢ç©ºé—´ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ï¼‰
    :param n_trials: æœç´¢æ¬¡æ•°
    :return: æœ€ä½³å‚æ•°å’Œç»“æœ
    """
    import torch
    from torch.utils.data import TensorDataset, DataLoader
    from sklearn.model_selection import train_test_split
    import streamlit as st
    import numpy as np
    import random
    
    # é»˜è®¤æœç´¢ç©ºé—´
    if param_grid is None:
        param_grid = {
            'hidden_dim': [64, 128, 256, 512],
            'num_layers': [1, 2, 3, 4],
            'dropout': [0.1, 0.2, 0.3, 0.4],
            'learning_rate': [0.0001, 0.0005, 0.001, 0.005]
        }
    
    st.info(f"ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢ (å…±{n_trials}æ¬¡å°è¯•)...")
    
    # åˆ’åˆ†æ•°æ®
    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)
    
    results = []
    best_score = -float('inf')
    best_params = None
    
    for trial in range(n_trials):
        # éšæœºé‡‡æ ·å‚æ•°
        params = {
            'hidden_dim': random.choice(param_grid['hidden_dim']),
            'num_layers': random.choice(param_grid['num_layers']),
            'dropout': random.choice(param_grid['dropout'])
        }
        lr = random.choice(param_grid['learning_rate'])
        
        st.write(f"  è¯•éªŒ {trial+1}/{n_trials}: {params}, lr={lr}")
        
        try:
            # æ„å»ºæ¨¡å‹å‚æ•°
            if model_class.__name__ == 'SimpleLSTM':
                model_params = {
                    'input_dim': X.shape[2],
                    'hidden_dim': params['hidden_dim'],
                    'output_dim': 1,
                    'num_layers': params['num_layers'],
                    'dropout': params['dropout']
                }
            elif model_class.__name__ == 'AttentionLSTM':
                model_params = {
                    'input_dim': X.shape[2],
                    'hidden_dim': params['hidden_dim'],
                    'output_dim': 1,
                    'num_layers': params['num_layers'],
                    'dropout': params['dropout']
                }
            elif model_class.__name__ == 'TransformerPredictor':
                model_params = {
                    'input_dim': X.shape[2],
                    'd_model': params['hidden_dim'],
                    'nhead': 8 if params['hidden_dim'] >= 128 else 4,
                    'num_layers': params['num_layers'],
                    'dropout': params['dropout']
                }
            else:
                continue
            
            # åˆ›å»ºæ¨¡å‹
            model = model_class(**model_params).to(device)
            optimizer = torch.optim.Adam(model.parameters(), lr=lr)
            criterion = torch.nn.SmoothL1Loss()
            
            # å¿«é€Ÿè®­ç»ƒï¼ˆåªè®­ç»ƒ20è½®ï¼‰
            train_X_tensor = torch.FloatTensor(train_X).to(device)
            train_y_tensor = torch.FloatTensor(train_y).to(device)
            val_X_tensor = torch.FloatTensor(val_X).to(device)
            val_y_tensor = torch.FloatTensor(val_y).to(device)
            
            train_dataset = TensorDataset(train_X_tensor, train_y_tensor)
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            
            for epoch in range(20):
                model.train()
                for batch_X, batch_y in train_loader:
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
            
            # è¯„ä¼°
            model.eval()
            with torch.no_grad():
                val_pred = model(val_X_tensor).cpu().numpy()
                val_y_np = val_y
                
                from sklearn.metrics import r2_score
                r2 = r2_score(val_y_np, val_pred)
                
                results.append({
                    'params': params.copy(),
                    'lr': lr,
                    'r2': r2
                })
                
                st.write(f"    â†’ RÂ² = {r2:.4f}")
                
                if r2 > best_score:
                    best_score = r2
                    best_params = params.copy()
                    best_params['learning_rate'] = lr
                    st.success(f"    âœ¨ æ–°æœ€ä½³å‚æ•°! RÂ² = {r2:.4f}")
        
        except Exception as e:
            st.warning(f"    âš ï¸ è¯•éªŒå¤±è´¥: {str(e)}")
            continue
    
    st.success(f"""
    âœ… è¶…å‚æ•°æœç´¢å®Œæˆï¼
    æœ€ä½³å‚æ•°: {best_params}
    æœ€ä½³ RÂ² = {best_score:.4f}
    """)
    
    return best_params, results

def reconstruct_spatiotemporal_data(X, y_final, support_ids, num_supports=125):
    """
    å°†å•æ”¯æ¶åºåˆ—æ•°æ®é‡æ„ä¸ºå®Œæ•´çš„æ—¶ç©ºæ•°æ®
    è¿™æ˜¯è§£å†³RÂ²ä½çš„å…³é”®ï¼
    
    :param X: (num_samples, seq_len, num_features) - å•æ”¯æ¶åºåˆ—
    :param y_final: (num_samples,) - å•æ”¯æ¶ç›®æ ‡å€¼
    :param support_ids: (num_samples,) - æ”¯æ¶ID
    :param num_supports: æ”¯æ¶æ€»æ•°
    :return: X_spatial, y_spatial, valid_time_indices
    """
    import pandas as pd
    import streamlit as st
    
    # æ­¥éª¤1ï¼šç¡®å®šæ—¶é—´ç´¢å¼•ï¼ˆå‡è®¾æ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰
    # ç”±äºæ¯ä¸ªæ—¶é—´ç‚¹æœ‰125ä¸ªæ”¯æ¶ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾å‡ºæ—¶é—´æ­¥æ•°
    num_samples = len(X)
    samples_per_timestep = num_supports
    
    # è®¡ç®—å¯èƒ½çš„æ—¶é—´æ­¥æ•°
    num_timesteps = num_samples // samples_per_timestep
    
    # â­ æ·»åŠ è¯¦ç»†æ—¥å¿—
    st.info(f"""
    ğŸ”„ **æ—¶ç©ºæ•°æ®é‡æ„ä¸­...**
    - åŸå§‹æ ·æœ¬æ•°: {num_samples:,}
    - æ”¯æ¶æ•°: {num_supports}
    - é¢„æœŸæ—¶é—´æ­¥æ•°: {num_timesteps}
    - æ¯æ—¶é—´æ­¥æ ·æœ¬æ•°: {samples_per_timestep}
    """)
    
    # æ­¥éª¤2ï¼šåˆ›å»ºæ”¯æ¶IDåˆ°ç´¢å¼•çš„æ˜ å°„
    unique_supports = np.unique(support_ids)
    support_to_idx = {sup_id: idx for idx, sup_id in enumerate(sorted(unique_supports))}
    
    st.write(f"âœ“ æ‰¾åˆ° {len(unique_supports)} ä¸ªå”¯ä¸€æ”¯æ¶")
    
    seq_len = X.shape[1]
    num_features = X.shape[2]
    
    # æ­¥éª¤3ï¼šé‡æ„ä¸ºæ—¶ç©ºæ ¼å¼
    # æ–°æ ¼å¼ï¼š(num_timesteps, num_supports, seq_len, num_features)
    X_spatial = np.zeros((num_timesteps, num_supports, seq_len, num_features))
    y_spatial = np.zeros((num_timesteps, num_supports))
    
    # æ ‡è®°å“ªäº›ä½ç½®æœ‰æœ‰æ•ˆæ•°æ®
    valid_mask = np.zeros((num_timesteps, num_supports), dtype=bool)
    
    # æ­¥éª¤4ï¼šå¡«å……æ•°æ®
    for i in range(num_samples):
        support_id = support_ids[i]
        support_idx = support_to_idx.get(support_id, None)
        
        if support_idx is None:
            continue
        
        # è®¡ç®—è¯¥æ ·æœ¬å±äºå“ªä¸ªæ—¶é—´æ­¥
        time_idx = i // samples_per_timestep
        
        if time_idx >= num_timesteps:
            break
        
        X_spatial[time_idx, support_idx, :, :] = X[i]
        y_spatial[time_idx, support_idx] = y_final[i]
        valid_mask[time_idx, support_idx] = True
    
    # æ­¥éª¤5ï¼šæ‰¾å‡ºæ‰€æœ‰æ”¯æ¶éƒ½æœ‰æ•°æ®çš„æ—¶é—´ç‚¹ï¼ˆå®Œæ•´æ—¶é—´æ­¥ï¼‰
    complete_timesteps = valid_mask.sum(axis=1) == num_supports
    valid_time_indices = np.where(complete_timesteps)[0]
    
    st.write(f"âœ“ æ‰¾åˆ° {len(valid_time_indices)} ä¸ªå®Œæ•´æ—¶é—´æ­¥ï¼ˆæ‰€æœ‰æ”¯æ¶éƒ½æœ‰æ•°æ®ï¼‰")
    
    # â­ æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å®Œæ•´æ—¶é—´æ­¥
    if len(valid_time_indices) < 10:
        st.warning(f"""
        âš ï¸ **å®Œæ•´æ—¶é—´æ­¥æ•°é‡è¾ƒå°‘ ({len(valid_time_indices)})ï¼**
        
        **å¯èƒ½åŸå› ï¼š**
        1. æ•°æ®ä¸­ä¸åŒæ”¯æ¶çš„æ—¶é—´ç‚¹ä¸å¯¹é½
        2. éƒ¨åˆ†æ”¯æ¶ç¼ºå°‘æ•°æ®
        3. æ”¯æ¶æ•°é‡ä¸å®é™…ä¸ç¬¦ï¼ˆé¢„æœŸ{num_supports}ä¸ªï¼‰
        
        **å»ºè®®ï¼š**
        - å¦‚æœ<10ä¸ªæ—¶é—´æ­¥ï¼š**å¼ºçƒˆå»ºè®®ä½¿ç”¨"å•æ ·æœ¬åºåˆ—æ ¼å¼"**
        - å¦‚æœ10-100ä¸ªï¼šå¯ä»¥å°è¯•ï¼Œä½†æ•ˆæœå¯èƒ½å—é™
        - å¦‚æœ>100ä¸ªï¼šæ•ˆæœè¾ƒå¥½
        
        å½“å‰ä¼šç»§ç»­å¤„ç†ï¼Œä½†å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡ã€‚
        """)
    
    # åªä¿ç•™å®Œæ•´çš„æ—¶é—´æ­¥
    X_spatial_complete = X_spatial[valid_time_indices]
    y_spatial_complete = y_spatial[valid_time_indices]
    
    st.success(f"""
    âœ… **æ—¶ç©ºæ•°æ®é‡æ„å®Œæˆï¼**
    - è¾“å‡ºå½¢çŠ¶: {X_spatial_complete.shape}
    - ç›®æ ‡å½¢çŠ¶: {y_spatial_complete.shape}
    - æ•°æ®å®Œæ•´æ€§: {len(valid_time_indices)}/{num_timesteps} ({len(valid_time_indices)/num_timesteps*100:.1f}%)
    """)
    
    return X_spatial_complete, y_spatial_complete, valid_time_indices, support_to_idx

def load_coordinate_file(coord_file):
    """
    åŠ è½½æ”¯æ¶åæ ‡æ–‡ä»¶
    :param coord_file: åæ ‡æ–‡ä»¶å¯¹è±¡ (CSV æˆ– Excel)
    :return: DataFrame with columns [æ”¯æ¶ID/åç§°, Xåæ ‡, Yåæ ‡, (å¯é€‰)Zåæ ‡]
    """
    if coord_file.name.endswith('.csv'):
        df = pd.read_csv(coord_file)
    elif coord_file.name.endswith(('.xls', '.xlsx')):
        df = pd.read_excel(coord_file)
    else:
        raise ValueError("åæ ‡æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ,è¯·ä½¿ç”¨ CSV æˆ– Excel æ ¼å¼")
    
    return df

def align_data_with_coordinates(column_names, coord_df):
    """
    å¯¹é½çŸ¿å‹æ•°æ®å’Œåæ ‡æ•°æ®
    :param column_names: çŸ¿å‹æ•°æ®çš„åˆ—å(æ”¯æ¶åç§°)
    :param coord_df: åæ ‡æ•°æ®DataFrame
    :return: å¯¹é½åçš„åæ ‡æ•°ç»„ (num_nodes, 2 æˆ– 3), å¯¹é½ä¿¡æ¯
    """
    # å°è¯•æ‰¾åˆ°åæ ‡DataFrameä¸­çš„æ”¯æ¶IDåˆ—
    possible_id_cols = ['æ”¯æ¶ID', 'æ”¯æ¶ç¼–å·', 'æ”¯æ¶åç§°', 'ID', 'id', 'Name', 'name', 'ç¼–å·']
    id_col = None
    for col in possible_id_cols:
        if col in coord_df.columns:
            id_col = col
            break
    
    if id_col is None:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°,ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºIDåˆ—
        id_col = coord_df.columns[0]
    
    # å°è¯•æ‰¾åˆ°X,Yåæ ‡åˆ—
    possible_x_cols = ['X', 'x', 'Xåæ ‡', 'xåæ ‡', 'lon', 'longitude', 'ç»åº¦']
    possible_y_cols = ['Y', 'y', 'Yåæ ‡', 'yåæ ‡', 'lat', 'latitude', 'çº¬åº¦']
    possible_z_cols = ['Z', 'z', 'Zåæ ‡', 'zåæ ‡', 'elevation', 'é«˜ç¨‹']
    
    x_col = next((col for col in possible_x_cols if col in coord_df.columns), None)
    y_col = next((col for col in possible_y_cols if col in coord_df.columns), None)
    z_col = next((col for col in possible_z_cols if col in coord_df.columns), None)
    
    if x_col is None or y_col is None:
        raise ValueError("æ— æ³•è¯†åˆ«åæ ‡åˆ—,è¯·ç¡®ä¿åæ ‡æ–‡ä»¶åŒ…å« X å’Œ Y åˆ—")
    
    # åˆ›å»ºåæ ‡å­—å…¸
    coord_dict = {}
    for _, row in coord_df.iterrows():
        support_id = str(row[id_col]).strip()
        if z_col:
            coord_dict[support_id] = [row[x_col], row[y_col], row[z_col]]
        else:
            coord_dict[support_id] = [row[x_col], row[y_col]]
    
    # å¯¹é½åæ ‡
    aligned_coords = []
    missing_coords = []
    
    for col_name in column_names:
        col_name_str = str(col_name).strip()
        if col_name_str in coord_dict:
            aligned_coords.append(coord_dict[col_name_str])
        else:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…
            matched = False
            for key in coord_dict.keys():
                if col_name_str in key or key in col_name_str:
                    aligned_coords.append(coord_dict[key])
                    matched = True
                    break
            if not matched:
                missing_coords.append(col_name_str)
                # ä½¿ç”¨é»˜è®¤åæ ‡æˆ–è·³è¿‡
                if len(aligned_coords) > 0:
                    aligned_coords.append(aligned_coords[-1])  # ä½¿ç”¨ä¸Šä¸€ä¸ªåæ ‡
                else:
                    aligned_coords.append([0, 0] if z_col is None else [0, 0, 0])
    
    coords_array = np.array(aligned_coords)
    
    alignment_info = {
        'total_supports': len(column_names),
        'matched': len(column_names) - len(missing_coords),
        'missing': missing_coords,
        'has_z': z_col is not None
    }
    
    return coords_array, alignment_info

def load_geological_features(geo_file, coords_array, column_names=None):
    """
    åŠ è½½åœ°è´¨ç‰¹å¾æ•°æ®å¹¶æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
    :param geo_file: åœ°è´¨ç‰¹å¾æ–‡ä»¶ (CSV, Excel, æˆ–è·¯å¾„å­—ç¬¦ä¸²)
    :param coords_array: æ”¯æ¶åæ ‡æ•°ç»„ (num_nodes, 2 æˆ– 3)
    :param column_names: æ”¯æ¶åç§°åˆ—è¡¨(å¯é€‰,ç”¨äºç›´æ¥åŒ¹é…é’»å­”åç§°)
    :return: åœ°è´¨ç‰¹å¾çŸ©é˜µ (num_nodes, num_geo_features), ç‰¹å¾åˆ—å
    """
    # æ”¯æŒæ–‡ä»¶å¯¹è±¡æˆ–è·¯å¾„å­—ç¬¦ä¸²
    if isinstance(geo_file, str):
        if geo_file.endswith('.csv'):
            geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        elif geo_file.endswith(('.xls', '.xlsx')):
            geo_df = pd.read_excel(geo_file)
        else:
            raise ValueError("åœ°è´¨æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")
    else:
        if geo_file.name.endswith('.csv'):
            geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        elif geo_file.name.endswith(('.xls', '.xlsx')):
            geo_df = pd.read_excel(geo_file)
        else:
            raise ValueError("åœ°è´¨æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")
    
    from scipy.spatial import KDTree
    
    # æå–åœ°è´¨ç‚¹åæ ‡
    x_col = next((col for col in ['x', 'X', 'Xåæ ‡', 'åæ ‡x'] if col in geo_df.columns), None)
    y_col = next((col for col in ['y', 'Y', 'Yåæ ‡', 'åæ ‡y'] if col in geo_df.columns), None)
    
    if x_col is None or y_col is None:
        raise ValueError("åœ°è´¨æ–‡ä»¶å¿…é¡»åŒ…å« X å’Œ Y åæ ‡åˆ—")
    
    geo_coords = geo_df[[x_col, y_col]].values
    
    # æå–åœ°è´¨ç‰¹å¾åˆ—(æ’é™¤åæ ‡ã€é’»å­”åç­‰IDåˆ—)
    exclude_cols = [x_col, y_col, 'borehole', 'é’»å­”å', 'id', 'ID', 'name']
    feature_cols = [col for col in geo_df.columns if col not in exclude_cols]
    
    # å¦‚æœæ²¡æœ‰æ•°å€¼ç‰¹å¾,è¿”å›ç©ºæ•°ç»„
    if len(feature_cols) == 0:
        return np.zeros((len(coords_array), 1)), ['dummy_feature']
    
    geo_features = geo_df[feature_cols].values
    
    # å¡«å……ç¼ºå¤±å€¼
    geo_features = np.nan_to_num(geo_features, nan=0.0)
    
    # ä½¿ç”¨ KNN æ’å€¼æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
    tree = KDTree(geo_coords)
    distances, indices = tree.query(coords_array[:, :2], k=1)
    
    # ä¸ºæ¯ä¸ªæ”¯æ¶åˆ†é…æœ€è¿‘é’»å­”çš„åœ°è´¨ç‰¹å¾
    support_geo_features = geo_features[indices.flatten()]
    
    return support_geo_features, feature_cols

def generate_adjacency_matrix(num_nodes, method='chain', **kwargs):
    """
    ç”Ÿæˆé‚»æ¥çŸ©é˜µ
    :param num_nodes: èŠ‚ç‚¹æ•°é‡
    :param method: ç”Ÿæˆæ–¹æ³•
        - 'chain': é“¾å¼ç»“æ„(ç›¸é‚»æ”¯æ¶è¿æ¥)
        - 'grid': ç½‘æ ¼ç»“æ„(å¦‚æœæ”¯æ¶æ’åˆ—æˆç½‘æ ¼)
        - 'distance': åŸºäºè·ç¦»(éœ€è¦æä¾›åæ ‡)
        - 'full': å…¨è¿æ¥
        - 'knn': Kè¿‘é‚»
        - 'adaptive': â­è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾(æ¨èç”¨äºRÂ²â‰¥0.8)
    :param kwargs: é¢å¤–å‚æ•°
    :return: é‚»æ¥çŸ©é˜µ (num_nodes, num_nodes)
    """
    adj_mx = np.zeros((num_nodes, num_nodes))
    
    if method == 'adaptive':
        # â­ è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾ - ç›®æ ‡RÂ²â‰¥0.8
        # å‡è®¾æ”¯æ¶çº¿æ€§æ’åˆ—ï¼ˆå¯æ ¹æ®å®é™…å¸ƒå±€è°ƒæ•´ï¼‰
        positions = np.arange(num_nodes).reshape(-1, 1).astype(float)
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distances = squareform(pdist(positions, metric='euclidean'))
        
        # è‡ªé€‚åº”é˜ˆå€¼ï¼šè¿æ¥è·ç¦»åœ¨thresholdä»¥å†…çš„æ”¯æ¶
        threshold = kwargs.get('threshold', 10.0)
        sigma = kwargs.get('sigma', 5.0)  # é«˜æ–¯æ ¸å‚æ•°
        
        # ä½¿ç”¨é«˜æ–¯æ ¸è®¡ç®—æƒé‡ï¼ˆè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
        adj_mx = np.exp(-distances**2 / (2 * sigma**2))
        
        # å¯é€‰ï¼šç¡¬é˜ˆå€¼ï¼Œåªä¿ç•™ä¸€å®šèŒƒå›´å†…çš„è¿æ¥
        adj_mx[distances > threshold] = 0
        
        # ç¡®ä¿å¯¹ç§°æ€§
        adj_mx = (adj_mx + adj_mx.T) / 2
        
        # è‡ªè¿æ¥è®¾ä¸º1
        np.fill_diagonal(adj_mx, 1.0)
        
        return adj_mx
    
    elif method == 'chain':
        # é“¾å¼ç»“æ„:æ¯ä¸ªèŠ‚ç‚¹ä¸ç›¸é‚»èŠ‚ç‚¹è¿æ¥
        for i in range(num_nodes - 1):
            adj_mx[i, i + 1] = 1
            adj_mx[i + 1, i] = 1
    
    elif method == 'grid':
        # ç½‘æ ¼ç»“æ„(éœ€è¦æŒ‡å®šè¡Œåˆ—æ•°)
        rows = kwargs.get('rows', int(np.sqrt(num_nodes)))
        cols = num_nodes // rows
        for i in range(num_nodes):
            row, col = i // cols, i % cols
            # è¿æ¥ä¸Šä¸‹å·¦å³é‚»å±…
            neighbors = []
            if row > 0: neighbors.append((row - 1) * cols + col)
            if row < rows - 1: neighbors.append((row + 1) * cols + col)
            if col > 0: neighbors.append(row * cols + col - 1)
            if col < cols - 1: neighbors.append(row * cols + col + 1)
            for j in neighbors:
                adj_mx[i, j] = 1
    
    elif method == 'distance':
        # åŸºäºè·ç¦»(éœ€è¦æä¾›åæ ‡)
        coords = kwargs.get('coords')
        threshold = kwargs.get('threshold', 1.0)
        if coords is not None:
            distances = squareform(pdist(coords))
            adj_mx = (distances <= threshold).astype(float)
            np.fill_diagonal(adj_mx, 0)
    
    elif method == 'full':
        # å…¨è¿æ¥
        adj_mx = np.ones((num_nodes, num_nodes))
        np.fill_diagonal(adj_mx, 0)
    
    elif method == 'knn':
        # Kè¿‘é‚»(éœ€è¦æä¾›åæ ‡)
        coords = kwargs.get('coords')
        k = kwargs.get('k', 3)
        if coords is not None:
            distances = squareform(pdist(coords))
            for i in range(num_nodes):
                # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»å±…
                neighbors = np.argsort(distances[i])[1:k+1]
                adj_mx[i, neighbors] = 1
                adj_mx[neighbors, i] = 1
    
    return adj_mx

def calculate_normalized_laplacian(adj_mx):
    """
    è®¡ç®—æ ‡å‡†åŒ–çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ A_hat = D^{-1/2} * (A + I) * D^{-1/2}
    
    :param adj_mx: é‚»æ¥çŸ©é˜µ (N, N)
    :return: A_hat (N, N)
    """
    adj_mx = adj_mx + np.eye(adj_mx.shape[0])
    d = np.array(adj_mx.sum(1)).flatten()
    d_inv_sqrt = np.power(d, -0.5)
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = np.diag(d_inv_sqrt)
    normalized_laplacian = d_mat_inv_sqrt.dot(adj_mx).dot(d_mat_inv_sqrt)
    return normalized_laplacian.astype(np.float32)

def generate_dataloader(data, batch_size, seq_len=12, pre_len=1, train_ratio=0.7, val_ratio=0.1):
    """
    ç”Ÿæˆ PyTorch DataLoaders
    :param data: (num_samples, num_nodes, num_features)
    :param batch_size: æ‰¹é‡å¤§å°
    :param seq_len: å†å²æ—¶é—´æ­¥
    :param pre_len: é¢„æµ‹æ—¶é—´æ­¥
    :return: train_loader, val_loader, test_loader, scaler
    """
    # å½’ä¸€åŒ– (è¿™é‡Œä½¿ç”¨ç®€å•çš„ Z-Score)
    mean = data.mean()
    std = data.std()
    scaler = {'mean': mean, 'std': std}
    data = (data - mean) / std

    x, y = [], []
    for i in range(len(data) - seq_len - pre_len + 1):
        x.append(data[i : i + seq_len])
        y.append(data[i + seq_len : i + seq_len + pre_len, :, 0:1]) # å‡è®¾åªé¢„æµ‹ç¬¬ä¸€ä¸ªç‰¹å¾

    x = np.array(x) # (N_samples, seq_len, num_nodes, num_features)
    y = np.array(y) # (N_samples, pre_len, num_nodes, 1)

    # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹ (Batch, Features, Nodes, Time)
    x = np.transpose(x, (0, 3, 2, 1)) 
    # (Batch, Time_out, Nodes, Features_out) -> (Batch, Features_out, Nodes, Time_out)
    y = np.transpose(y, (0, 3, 2, 1))

    # åˆ’åˆ†æ•°æ®é›†
    num_samples = x.shape[0]
    train_end = int(num_samples * train_ratio)
    val_end = int(num_samples * (train_ratio + val_ratio))

    train_x, train_y = x[:train_end], y[:train_end]
    val_x, val_y = x[train_end:val_end], y[val_end:]
    test_x, test_y = x[val_end:], y[val_end:]

    # åˆ›å»º TensorDataset å’Œ DataLoader
    def create_loader(x_data, y_data):
        tensor_x = torch.tensor(x_data, dtype=torch.float32)
        tensor_y = torch.tensor(y_data, dtype=torch.float32)
        dataset = torch.utils.data.TensorDataset(tensor_x, tensor_y)
        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    train_loader = create_loader(train_x, train_y)
    val_loader = create_loader(val_x, val_y)
    test_loader = create_loader(test_x, test_y)

    return train_loader, val_loader, test_loader, scaler

# ----------------------------------------------------------------------
# 2. STGCN æ¨¡å‹å®šä¹‰ (PyTorch)
# ----------------------------------------------------------------------

class SimpleLSTM(nn.Module):
    """
    ç®€å•çš„ LSTM æ¨¡å‹ï¼ˆä¸ä½¿ç”¨å›¾ç»“æ„ï¼Œç›´æ¥åºåˆ—é¢„æµ‹ï¼‰
    é€‚ç”¨äºç¨€ç–å›¾æ•°æ®
    """
    def __init__(self, num_features, hidden_dim=128, num_layers=2):
        super(SimpleLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTM å±‚
        self.lstm = nn.LSTM(
            num_features, 
            hidden_dim, 
            num_layers, 
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        
        self.dropout = nn.Dropout(0.2)
        self.relu = nn.ReLU()
        
    def forward(self, X):
        """
        X: (Batch, seq_len, num_features)
        è¾“å‡º: (Batch, 1) - é¢„æµ‹å€¼
        """
        # LSTM
        lstm_out, _ = self.lstm(X)  # (B, T, hidden)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_hidden = lstm_out[:, -1, :]  # (B, hidden)
        
        # å…¨è¿æ¥å±‚
        x = self.relu(self.fc1(last_hidden))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        out = self.fc3(x)  # (B, 1)
        
        return out

class AttentionLSTM(nn.Module):
    """
    å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„LSTMæ¨¡å‹ - æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
    """
    def __init__(self, num_features, hidden_dim=128, num_layers=2):
        super(AttentionLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            num_features,
            hidden_dim,
            num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # æ³¨æ„åŠ›å±‚
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, X):
        """
        X: (Batch, seq_len, num_features)
        è¾“å‡º: (Batch, 1)
        """
        # LSTM
        lstm_out, _ = self.lstm(X)  # (B, T, hidden)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = self.attention(lstm_out)  # (B, T, 1)
        attention_weights = torch.softmax(attention_weights, dim=1)  # (B, T, 1)
        
        # åŠ æƒæ±‚å’Œ
        context = torch.sum(lstm_out * attention_weights, dim=1)  # (B, hidden)
        
        # å…¨è¿æ¥å±‚
        x = self.relu(self.fc1(context))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        out = self.fc3(x)  # (B, 1)
        
        return out

class TransformerPredictor(nn.Module):
    """
    â­ Transformeræ—¶åºé¢„æµ‹æ¨¡å‹ - æœ€å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œç›®æ ‡RÂ²â‰¥0.8
    åˆ©ç”¨Self-Attentionæœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–
    """
    def __init__(self, num_features, d_model=128, nhead=8, num_encoder_layers=3, dim_feedforward=512):
        super(TransformerPredictor, self).__init__()
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±å±‚ï¼ˆå°†ç‰¹å¾ç»´åº¦æŠ•å½±åˆ°d_modelï¼‰
        self.input_projection = nn.Linear(num_features, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoder = PositionalEncoding(d_model, max_len=50)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
        
        # è¾“å‡ºå±‚
        self.fc1 = nn.Linear(d_model, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        self.layer_norm = nn.LayerNorm(d_model)
        
        # â­ ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–ï¼ˆXavierå‡åŒ€åˆ†å¸ƒï¼‰
        self._init_weights()
    
    def _init_weights(self):
        """ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸"""
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() >= 2:
                nn.init.xavier_uniform_(param, gain=0.5)  # gain=0.5æ›´ä¿å®ˆ
            elif 'bias' in name:
                nn.init.constant_(param, 0)
    
    def forward(self, X):
        """
        X: (Batch, seq_len, num_features)
        è¾“å‡º: (Batch, 1)
        """
        # è¾“å…¥æŠ•å½±
        X = self.input_projection(X)  # (B, T, d_model)
        X = self.layer_norm(X)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        X = self.pos_encoder(X)  # (B, T, d_model)
        
        # Transformerç¼–ç 
        encoded = self.transformer_encoder(X)  # (B, T, d_model)
        
        # â­ ä¼˜åŒ–ï¼šå¤šå¤´æ± åŒ–ç­–ç•¥ï¼ˆç»“åˆæœ€åæ—¶é—´æ­¥ã€å¹³å‡æ± åŒ–ã€æœ€å¤§æ± åŒ–ï¼‰
        last_hidden = encoded[:, -1, :]  # (B, d_model)
        avg_pool = torch.mean(encoded, dim=1)  # (B, d_model)
        max_pool, _ = torch.max(encoded, dim=1)  # (B, d_model)
        
        # æ‹¼æ¥ä¸‰ç§æ± åŒ–ç»“æœ
        out = torch.cat([last_hidden, avg_pool, max_pool], dim=1)  # (B, 3*d_model)
        
        # â­ éœ€è¦è°ƒæ•´ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„è¾“å…¥ç»´åº¦
        if not hasattr(self, 'fc1_adjusted'):
            self.fc1 = nn.Linear(self.d_model * 3, 128).to(out.device)
            self.fc1_adjusted = True
        
        # å…¨è¿æ¥å±‚
        out = self.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.relu(self.fc2(out))
        out = self.dropout(out)
        out = self.fc3(out)  # (B, 1)
        
        return out

class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç å±‚ - ä¸ºTransformeræä¾›åºåˆ—ä½ç½®ä¿¡æ¯
    """
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        x: (Batch, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return x

class TimeBlock(nn.Module):
    """
    æ—¶åºå·ç§¯å— (TCN)
    """
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(TimeBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))
        self.conv2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))
        self.conv3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))

    def forward(self, X):
        # è¾“å…¥ X: (Batch, Channels, Nodes, Time_steps)
        # GLU (Gated Linear Unit)
        # X shape: (B, C_in, N, T)
        return (self.conv1(X) + self.conv3(X)) * torch.sigmoid(self.conv2(X))

class STGCNBlock(nn.Module):
    """
    æ—¶ç©ºå·ç§¯å— (Spatio-Temporal GCN Block)
    """
    def __init__(self, in_channels, spatial_channels, out_channels, num_nodes, Kt):
        super(STGCNBlock, self).__init__()
        # æ—¶åºå·ç§¯ (TCN)
        self.tcn = TimeBlock(in_channels, spatial_channels, Kt)
        # ç©ºé—´å·ç§¯ (GCN)
        self.gcn = nn.Conv2d(spatial_channels, out_channels, (1, 1))
        # æ‰¹é‡å½’ä¸€åŒ– - åº”è¯¥å¯¹ out_channels è¿›è¡Œå½’ä¸€åŒ–
        self.bn = nn.BatchNorm2d(out_channels)
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚(å¦‚æœç»´åº¦ä¸åŒ¹é…)
        self.residual_conv = nn.Conv2d(in_channels, out_channels, (1, 1)) if in_channels != out_channels else None
        
    def forward(self, X, A_hat):
        # X: (B, C_in, N, T)
        # A_hat: (N, N)

        # 1. TCN
        X_tcn = self.tcn(X) # (B, spatial_channels, N, T_out)
        
        # 2. GCN
        # (B, spatial_channels, N, T_out) -> (B, T_out, N, spatial_channels)
        X_gcn_input = X_tcn.permute(0, 3, 2, 1) 
        
        # (B, T_out, N, spatial_channels) x (N, N) -> (B, T_out, N, spatial_channels)
        X_gcn = torch.einsum('btni,nm->btmi', X_gcn_input, A_hat)
        
        # (B, T_out, N, spatial_channels) -> (B, spatial_channels, N, T_out)
        X_gcn = X_gcn.permute(0, 3, 2, 1)

        # 3. 1x1 å·ç§¯
        X_gcn = self.gcn(X_gcn) # (B, out_channels, N, T_out)
        
        # 4. æ‰¹å½’ä¸€åŒ–
        X_gcn = self.bn(X_gcn)
        
        # 5. æ®‹å·®è¿æ¥(éœ€è¦å¤„ç†æ—¶é—´ç»´åº¦å’Œé€šé“ç»´åº¦çš„å˜åŒ–)
        if X.shape[-1] > X_gcn.shape[-1]:  # æ—¶é—´ç»´åº¦å‡å°‘äº†
            # æˆªå– X çš„æœ€åå‡ ä¸ªæ—¶é—´æ­¥ä»¥åŒ¹é…
            X_res = X[:, :, :, -(X_gcn.shape[-1]):]
        else:
            X_res = X
            
        # å¦‚æœé€šé“æ•°ä¸åŒ¹é…,ä½¿ç”¨ 1x1 å·ç§¯æŠ•å½±
        if self.residual_conv is not None:
            X_res = self.residual_conv(X_res)
        
        # 6. æ¿€æ´»
        X_out = F.relu(X_gcn + X_res)
        
        return X_out


class STGCN(nn.Module):
    """
    STGCN å®Œæ•´æ¨¡å‹
    è¾“å…¥ç»´åº¦: (Batch, Features_in, Num_Nodes, Seq_Len)
    è¾“å‡ºç»´åº¦: (Batch, Features_out, Num_Nodes, Pred_Len)
    """
    def __init__(self, num_nodes, num_features, seq_len, pred_len, hidden_dim=64, Kt=3):
        super(STGCN, self).__init__()
        self.num_nodes = num_nodes
        self.pred_len = pred_len
        
        # ä½¿ç”¨hidden_dimå‚æ•°åŒ–æ¨¡å‹å®¹é‡
        # STGCN Block 1
        self.st_block1 = STGCNBlock(num_features, hidden_dim, hidden_dim, num_nodes, Kt)
        
        # STGCN Block 2
        self.st_block2 = STGCNBlock(hidden_dim, hidden_dim, hidden_dim, num_nodes, Kt)
        
        # Dropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(0.2)
        
        # æœ€åä¸€ä¸ªæ—¶åºå·ç§¯ (æ‰©å±•åˆ°2å€hidden_dim)
        self.last_tcn = TimeBlock(hidden_dim, hidden_dim * 2, Kt)
        
        # è®¡ç®—ç»è¿‡æ‰€æœ‰å±‚åçš„æ—¶é—´ç»´åº¦
        # æ¯ä¸ª TimeBlock ä½¿ç”¨ padding=(kernel_size-1)//2, æ‰€ä»¥ä¸æ”¹å˜æ—¶é—´ç»´åº¦
        # ä½†ç”±äºæˆ‘ä»¬åœ¨ forward ä¸­åšäº†æ®‹å·®è¿æ¥çš„æˆªå–,å®é™…ä¼šå‡å°‘
        # å®é™…ä¸Š TimeBlock ä½¿ç”¨ same padding,æ—¶é—´ç»´åº¦åº”è¯¥ä¿æŒä¸å˜
        # è®©æˆ‘ä»¬ä½¿ç”¨è‡ªé€‚åº”æ± åŒ–æ¥å¤„ç†
        
        # è¾“å‡ºå±‚:å°†ç‰¹å¾æ˜ å°„åˆ°é¢„æµ‹é•¿åº¦
        self.output_conv = nn.Conv2d(hidden_dim * 2, hidden_dim * 2, (1, 1))
        self.temporal_conv = nn.Conv2d(hidden_dim * 2, pred_len, (1, 1))
        # æœ€ç»ˆé€šé“å‹ç¼©å±‚: hidden_dim*2 -> 1
        self.final_conv = nn.Conv2d(hidden_dim * 2, 1, (1, 1))
    
    def forward(self, X, A_hat):
        # X: (B, C_in, N, T_in)
        
        # Block 1
        X = self.st_block1(X, A_hat) # (B, hidden_dim, N, T)
        X = self.dropout(X)  # æ·»åŠ dropout
        
        # Block 2
        X = self.st_block2(X, A_hat) # (B, hidden_dim, N, T)
        X = self.dropout(X)  # æ·»åŠ dropout
        
        # Last TCN
        X = self.last_tcn(X) # (B, hidden_dim*2, N, T)
        
        # Output layers
        X = F.relu(self.output_conv(X)) # (B, hidden_dim*2, N, T)
        
        # ä½¿ç”¨è‡ªé€‚åº”å¹³å‡æ± åŒ–å°†æ—¶é—´ç»´åº¦è°ƒæ•´ä¸ºé¢„æµ‹é•¿åº¦
        X = F.adaptive_avg_pool2d(X, (X.shape[2], self.pred_len)) # (B, 128, N, pred_len)
        
        # å°†é€šé“æ•°è½¬æ¢ä¸º1(åªé¢„æµ‹ä¸€ä¸ªç‰¹å¾)
        X = self.final_conv(X) # (B, 1, N, pred_len)
        
        # ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œè®©æ¨¡å‹è‡ªç”±å­¦ä¹ è¾“å‡ºèŒƒå›´
        # åœ¨è®­ç»ƒæ—¶ä¼šé€šè¿‡clampè£å‰ªåˆ°[0,1]
        
        return X # (B, 1, N, pred_len)


# ----------------------------------------------------------------------
# 3. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
# ----------------------------------------------------------------------

def train_epoch(model, train_loader, optimizer, loss_fn, device, A_hat):
    model.train()
    total_loss = 0
    A_hat_tensor = torch.tensor(A_hat, dtype=torch.float32).to(device)
    
    for x_batch, y_batch in train_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        
        # x_batch: (B, F_in, N, T_in)
        # y_batch: (B, F_out, N, T_out)
        y_pred = model(x_batch, A_hat_tensor) # (B, 1, N, pred_len)
        
        # ç¡®ä¿ y_pred å’Œ y_batch ç»´åº¦åŒ¹é…
        # y_batch: (B, F_out, N, T_out)
        # y_pred: (B, 1, N, pred_len)
        loss = loss_fn(y_pred, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
    return total_loss / len(train_loader)

def evaluate(model, val_loader, loss_fn, device, A_hat):
    model.eval()
    total_loss = 0
    A_hat_tensor = torch.tensor(A_hat, dtype=torch.float32).to(device)
    
    with torch.no_grad():
        for x_batch, y_batch in val_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            
            # x_batch: (B, F_in, N, T_in)
            # y_batch: (B, F_out, N, T_out)
            y_pred = model(x_batch, A_hat_tensor) # (B, 1, N, pred_len)
            
            loss = loss_fn(y_pred, y_batch)
            total_loss += loss.item()
            
    return total_loss / len(val_loader)

# ----------------------------------------------------------------------
# 4. Streamlit äº¤äº’å¼ GUI ç•Œé¢
# ----------------------------------------------------------------------

st.set_page_config(page_title="çŸ¿å‹é¢„æµ‹ AI è®­ç»ƒå¹³å°", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– çŸ¿å‹é¢„æµ‹ AI è®­ç»ƒå¹³å°")
st.caption("ç®€å•æ˜“ç”¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå·¥å…· - ä¸€é”®æ™ºèƒ½è®­ç»ƒï¼Œç›®æ ‡RÂ²â‰¥0.8")

# --- ä¾§è¾¹æ :å¿«æ·å¸®åŠ© ---
st.sidebar.header("ğŸ“– å¿«é€Ÿå…¥é—¨")
st.sidebar.info("""
**æ–°æ‰‹è®­ç»ƒä¸‰æ­¥èµ°ï¼š**

1ï¸âƒ£ ç‚¹å‡»"åŠ è½½æ•°æ®"æŒ‰é’®

2ï¸âƒ£ é€‰æ‹©"Transformer"æ¨¡å‹

3ï¸âƒ£ å¯ç”¨"ä¸€é”®æ™ºèƒ½æ¨¡å¼"

4ï¸âƒ£ ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"

âœ¨ ç³»ç»Ÿä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼Œç›®æ ‡RÂ²â‰¥0.8
""")

st.sidebar.markdown("---")
st.sidebar.header("âš™ï¸ é«˜çº§è®¾ç½®")
SEQ_LEN = st.sidebar.slider("å†å²æ—¶é—´æ­¥", 5, 24, 12, help="ç”¨äºé¢„æµ‹çš„å†å²æ•°æ®é•¿åº¦")
PRED_LEN = st.sidebar.slider("é¢„æµ‹æ—¶é—´æ­¥", 1, 12, 1, help="éœ€è¦é¢„æµ‹çš„æœªæ¥æ­¥æ•°")
BATCH_SIZE = st.sidebar.slider("æ‰¹é‡å¤§å°", 8, 128, 32, help="æ¯æ‰¹å¤„ç†çš„æ ·æœ¬æ•°")
EPOCHS = st.sidebar.slider("è®­ç»ƒè½®æ•°", 10, 200, 50, help="å®Œæ•´éå†æ•°æ®é›†çš„æ¬¡æ•°")
LR = st.sidebar.number_input("å­¦ä¹ ç‡", 0.0001, 0.1, 0.001, format="%.4f", help="æ¨¡å‹å‚æ•°æ›´æ–°æ­¥é•¿")
st.sidebar.caption("ğŸ’¡ å»ºè®®ä¿æŒé»˜è®¤å€¼")

# GPU æ£€æµ‹å’Œè®¾å¤‡é€‰æ‹©
gpu_available = torch.cuda.is_available()
gpu_usable = False

if gpu_available:
    # æµ‹è¯• GPU æ˜¯å¦çœŸçš„å¯ç”¨
    try:
        test_tensor = torch.rand(10, 10).cuda()
        _ = test_tensor * 2
        torch.cuda.synchronize()
        gpu_usable = True
        del test_tensor
        torch.cuda.empty_cache()
    except Exception as e:
        gpu_usable = False
        st.sidebar.warning(f"âš ï¸ GPU æ£€æµ‹åˆ°ä½†ä¸å¯ç”¨: {str(e)[:50]}...")

if gpu_usable:
    DEVICE = st.sidebar.selectbox("è®¾å¤‡", ["cuda", "cpu"], index=0)
    st.sidebar.success("âœ… GPU å¯ç”¨ä¸”æ­£å¸¸å·¥ä½œ")
else:
    DEVICE = "cpu"
    st.sidebar.info("â„¹ï¸ ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒ")
    if gpu_available:
        st.sidebar.warning("""
        âš ï¸ **GPU å…¼å®¹æ€§é—®é¢˜**
        
        æ‚¨çš„ RTX 5070 Ti (Blackwell æ¶æ„) éœ€è¦æ›´æ–°çš„ PyTorch ç‰ˆæœ¬ã€‚
        
        å½“å‰ PyTorch 2.2.2 ä¸æ”¯æŒè¯¥ GPUã€‚
        
        å»ºè®®:
        1. ä½¿ç”¨ CPU è®­ç»ƒ(å½“å‰é€‰é¡¹)
        2. ç­‰å¾… PyTorch 2.6+ ç‰ˆæœ¬
        3. æˆ–å°è¯• PyTorch nightly ç‰ˆæœ¬
        """)

st.sidebar.header("æ•°æ®ä¸Šä¼ ")

# æ•°æ®æºé€‰æ‹©
data_source = st.sidebar.radio(
    "é€‰æ‹©æ•°æ®æ¥æº",
    ["ä¸Šä¼ CSVæ–‡ä»¶", "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†"],
    help="CSV: åŸå§‹çŸ¿å‹æ—¶é—´åºåˆ— | é¢„å¤„ç†: å·²æå–ç‰¹å¾çš„è®­ç»ƒæ•°æ®"
)

# æ ¹æ®æ•°æ®æºæ˜¾ç¤ºä¸åŒçš„ä¸Šä¼ é€‰é¡¹
if data_source == "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†":
    st.sidebar.subheader("ğŸ“¦ åŠ è½½é¢„å¤„ç†æ•°æ®")
    
    # æ£€æŸ¥é»˜è®¤æ•°æ®é›†
    default_npz_path = os.path.join(os.path.dirname(__file__), 'processed_data', 'sequence_dataset.npz')
    use_default_dataset = False
    
    if os.path.exists(default_npz_path):
        use_default_dataset = st.sidebar.checkbox(
            "ä½¿ç”¨å·²ç”Ÿæˆçš„æ•°æ®é›†",
            value=True,
            help=f"è·¯å¾„: {default_npz_path}"
        )
    
    if use_default_dataset:
        npz_file = default_npz_path
        st.sidebar.success("âœ“ ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†")
        st.sidebar.info(
            """
            **æ•°æ®é›†ä¿¡æ¯:**
            - 195,836 ä¸ªè®­ç»ƒæ ·æœ¬
            - 125 ä¸ªæ”¯æ¶
            - 17 ç»´ç‰¹å¾
            - åºåˆ—é•¿åº¦: 5 â†’ 1
            """
        )
    else:
        npz_file = st.sidebar.file_uploader(
            "ä¸Šä¼ é¢„å¤„ç†æ•°æ®æ–‡ä»¶ (.npz)",
            type=["npz"],
            help="ä½¿ç”¨ preprocess/prepare_training_data.py ç”Ÿæˆçš„æ•°æ®"
        )
    
    # åŠ è½½åæ ‡æ–‡ä»¶
    coord_file_path = os.path.join(os.path.dirname(__file__), 'processed_data', 'support_coordinates.csv')
    if os.path.exists(coord_file_path):
        coord_file = coord_file_path
    else:
        coord_file = None
    
else:  # CSV æ–‡ä»¶ä¸Šä¼ æ¨¡å¼
    npz_file = None
    
    # æ­¥éª¤1: ä¸Šä¼ çŸ¿å‹æ•°æ®
    st.sidebar.subheader("æ­¥éª¤1: çŸ¿å‹æ•°æ®")
    data_file = st.sidebar.file_uploader("ä¸Šä¼ çŸ¿å‹æ•°æ®æ–‡ä»¶ (.csv)", type=["csv"])
    st.sidebar.info(
        """
        **CSV æ ¼å¼è¦æ±‚:**
        - æ¯è¡Œ = ä¸€ä¸ªæ—¶é—´ç‚¹
        - æ¯åˆ— = ä¸€ä¸ªæ”¯æ¶(åˆ—åè¦ä¸åæ ‡æ–‡ä»¶å¯¹åº”)
        
        ç¤ºä¾‹:
        ```
        æ—¶é—´, ZJ001, ZJ002, ZJ003, ...
        2023-01-01, 100.5, 98.3, 102.1, ...
        ```
        """
    )

    # æ­¥éª¤2: ä¸Šä¼ æ”¯æ¶åæ ‡
    st.sidebar.subheader("æ­¥éª¤2: æ”¯æ¶åæ ‡ (é‡è¦!)")
    coord_file = st.sidebar.file_uploader(
        "ä¸Šä¼ æ”¯æ¶åæ ‡æ–‡ä»¶ (.csv/.xlsx)", 
        type=["csv", "xlsx", "xls"],
        help="åæ ‡æ–‡ä»¶åº”åŒ…å«: æ”¯æ¶ID, Xåæ ‡, Yåæ ‡"
    )
    st.sidebar.info(
        """
        **åæ ‡æ–‡ä»¶æ ¼å¼:**
        ```
        æ”¯æ¶ID, Xåæ ‡, Yåæ ‡
        ZJ001, 1000.5, 2000.3
        ZJ002, 1001.2, 2000.5
        ZJ003, 1002.0, 2001.1
        ```
        
        âš ï¸ **æ”¯æ¶IDå¿…é¡»ä¸çŸ¿å‹æ•°æ®çš„åˆ—åå¯¹åº”**
        """
    )

# æ­¥éª¤3: åœ°è´¨ç‰¹å¾èåˆï¼ˆå¼ºçƒˆæ¨èï¼‰â­â­â­
st.sidebar.markdown("---")
st.sidebar.subheader("æ­¥éª¤3: ğŸŒ åœ°è´¨ç‰¹å¾ (å¼ºçƒˆæ¨è)")
st.sidebar.warning("""
âš ï¸ **é‡è¦æç¤º**

çŸ¿å‹å—åœ°è´¨æ¡ä»¶å½±å“æ˜¾è‘—ï¼

çº¯çŸ¿å‹å†å²æ•°æ®é¢„æµ‹ç¼ºå°‘å…³é”®å› ç´ ï¼š
- ç…¤å±‚åšåº¦å˜åŒ–
- æ–­å±‚åˆ†å¸ƒ
- é¡¶åº•æ¿å²©æ€§
- åœ°è´¨æ„é€ 

**å»ºè®®å¯ç”¨åœ°è´¨ç‰¹å¾èåˆï¼Œæå‡é¢„æµ‹å‡†ç¡®æ€§ï¼**
""")

use_geological = st.sidebar.checkbox("ğŸ”§ èåˆåœ°è´¨ç‰¹å¾æ•°æ®", value=True, help="å¼ºçƒˆæ¨èï¼åœ°è´¨æ¡ä»¶æ˜¯çŸ¿å‹çš„ä¸»è¦å½±å“å› ç´ ")

geo_file = None
if use_geological:
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é»˜è®¤åœ°è´¨ç‰¹å¾æ–‡ä»¶
    default_geo_path = os.path.join(os.path.dirname(__file__), 'geology_features_extracted.csv')
    use_default_geo = False
    
    if os.path.exists(default_geo_path):
        use_default_geo = st.sidebar.checkbox(
            "âœ… ä½¿ç”¨æå–çš„é’»å­”åœ°è´¨ç‰¹å¾", 
            value=True,
            help=f"å·²æ£€æµ‹åˆ° geology_features_extracted.csv"
        )
    
    if use_default_geo:
        geo_file = default_geo_path
        st.sidebar.success("âœ… å°†èåˆé’»å­”åœ°è´¨ç‰¹å¾")
        st.sidebar.info(
            """
            **åŒ…å«çš„åœ°è´¨ç‰¹å¾:**
            - æ€»åšåº¦ (m)
            - ç…¤å±‚åšåº¦/æ•°é‡ (m/ä¸ª)
            - é¡¶æ¿ç…¤å±‚åŸ‹æ·± (m)
            - å¹³å‡å¼¹æ€§æ¨¡é‡ (GPa)
            - å¹³å‡å®¹é‡ (kN/mÂ³)
            - æœ€å¤§æŠ—æ‹‰å¼ºåº¦ (MPa)
            - ç ‚å²©/æ³¥å²©å æ¯”
            
            ğŸ’¡ è¿™äº›ç‰¹å¾å¯¹çŸ¿å‹é¢„æµ‹è‡³å…³é‡è¦ï¼
            """
        )
    else:
        st.sidebar.warning("âš ï¸ æœªæ‰¾åˆ°é»˜è®¤åœ°è´¨æ–‡ä»¶")
        geo_file = st.sidebar.file_uploader(
            "ä¸Šä¼ åœ°è´¨ç‰¹å¾æ–‡ä»¶ (.csv/.xlsx)",
            type=["csv", "xlsx", "xls"],
            help="åœ°è´¨æ–‡ä»¶åº”åŒ…å«: Xåæ ‡, Yåæ ‡, åœ°è´¨ç‰¹å¾"
        )
        if geo_file:
            st.sidebar.success("âœ… å·²ä¸Šä¼ åœ°è´¨æ–‡ä»¶")
        st.sidebar.info(
            """
            **åœ°è´¨æ–‡ä»¶æ ¼å¼ç¤ºä¾‹:**
            ```
            Xåæ ‡, Yåæ ‡, æ–­å±‚è·ç¦», ç…¤å±‚åšåº¦, ...
            1000.5, 2000.3, 50.2, 3.5, ...
            1001.0, 2000.5, 48.5, 3.6, ...
            ```
            
            ç³»ç»Ÿä¼šæ ¹æ®è·ç¦»å°†åœ°è´¨ç‰¹å¾æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
            """
        )
else:
    st.sidebar.error("""
    âŒ **æœªå¯ç”¨åœ°è´¨ç‰¹å¾**
    
    è­¦å‘Šï¼šçº¯çŸ¿å‹å†å²æ•°æ®é¢„æµ‹æ•ˆæœæœ‰é™ï¼
    
    åœ°è´¨æ¡ä»¶æ˜¯çŸ¿å‹çš„æ ¹æœ¬åŸå› ï¼š
    - ç…¤å±‚åšåº¦ â†’ ç›´æ¥å½±å“çŸ¿å‹å¤§å°
    - æ–­å±‚åˆ†å¸ƒ â†’ åº”åŠ›é›†ä¸­åŒºåŸŸ
    - å²©å±‚å¼ºåº¦ â†’ é¡¶æ¿ç¨³å®šæ€§
    
    å¼ºçƒˆå»ºè®®å¯ç”¨åœ°è´¨ç‰¹å¾èåˆï¼
    """)

# é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼
st.sidebar.header("é‚»æ¥çŸ©é˜µè®¾ç½®")
adj_method = st.sidebar.selectbox(
    "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼",
    ["chain", "grid", "distance", "knn", "full", "upload"],
    format_func=lambda x: {
        "chain": "é“¾å¼ç»“æ„ (ç›¸é‚»æ”¯æ¶è¿æ¥)",
        "grid": "ç½‘æ ¼ç»“æ„ (2Dæ’åˆ—)",
        "distance": "è·ç¦»é˜ˆå€¼ (éœ€è¦åæ ‡)",
        "knn": "Kè¿‘é‚» (éœ€è¦åæ ‡)",
        "full": "å…¨è¿æ¥ (æ‰€æœ‰èŠ‚ç‚¹äº’è¿)",
        "upload": "ä¸Šä¼ è‡ªå®šä¹‰é‚»æ¥çŸ©é˜µ"
    }[x]
)

# æ ¹æ®é€‰æ‹©çš„æ–¹æ³•æ˜¾ç¤ºé¢å¤–å‚æ•°
adj_params = {}
if adj_method == "grid":
    adj_params['rows'] = st.sidebar.number_input("ç½‘æ ¼è¡Œæ•°", min_value=1, value=10)
elif adj_method == "knn":
    adj_params['k'] = st.sidebar.number_input("Kå€¼(è¿‘é‚»æ•°é‡)", min_value=1, value=3)
    st.sidebar.warning("Kè¿‘é‚»æ–¹æ³•éœ€è¦æ”¯æ¶åæ ‡ä¿¡æ¯,æš‚æ—¶ä½¿ç”¨éšæœºåæ ‡")

adj_file = None
if adj_method == "upload":
    adj_file = st.sidebar.file_uploader("ä¸Šä¼ é‚»æ¥çŸ©é˜µæ–‡ä»¶ (.npyæˆ–.csv)", type=["npy", "csv"])

# --- ä¸»ç•Œé¢ ---
if data_source == "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†" and npz_file:
    st.header("1. åŠ è½½é¢„å¤„ç†æ•°æ®é›†")
    
    try:
        # åŠ è½½é¢„å¤„ç†çš„åºåˆ—æ•°æ®
        X, y_final, support_ids, feature_names = load_processed_sequence_data(npz_file)
        
        st.write(f"**æ•°æ®å½¢çŠ¶:**")
        st.write(f"- æ ·æœ¬æ•°é‡: {X.shape[0]:,}")
        st.write(f"- åºåˆ—é•¿åº¦: {X.shape[1]} (å†å²æ­¥æ•°)")
        st.write(f"- ç‰¹å¾ç»´åº¦: {X.shape[2]}")
        st.write(f"- æ ‡ç­¾å½¢çŠ¶: {y_final.shape}")
        
        NUM_SAMPLES = X.shape[0]
        SEQ_LEN = X.shape[1]
        NUM_FEATURES = X.shape[2]
        PRED_LEN = y_final.shape[1]
        
        # è·å–æ”¯æ¶ä¿¡æ¯
        unique_supports = np.unique(support_ids)
        NUM_NODES = len(unique_supports)
        
        st.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼åŒ…å« {NUM_NODES} ä¸ªæ”¯æ¶ï¼Œ{NUM_SAMPLES:,} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        # â­ åœ°è´¨ç‰¹å¾èåˆæ£€æŸ¥
        if use_geological and geo_file:
            st.success("âœ… å·²å¯ç”¨åœ°è´¨ç‰¹å¾èåˆ - çŸ¿å‹é¢„æµ‹å°†è€ƒè™‘åœ°è´¨æ¡ä»¶å½±å“")
        else:
            st.warning("""
            âš ï¸ **æœªå¯ç”¨åœ°è´¨ç‰¹å¾èåˆ**
            
            **é‡è¦æç¤ºï¼š** å½“å‰ä»…ä½¿ç”¨çŸ¿å‹å†å²æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œç¼ºå°‘åœ°è´¨å› ç´ å½±å“ï¼
            
            **çŸ¿å‹çš„ä¸»è¦å½±å“å› ç´ ï¼š**
            1. ğŸª¨ **åœ°è´¨æ¡ä»¶**ï¼ˆå æ¯”60-70%ï¼‰
               - ç…¤å±‚åšåº¦ã€å€¾è§’
               - æ–­å±‚åˆ†å¸ƒã€è¤¶çš±
               - é¡¶åº•æ¿å²©æ€§ã€å¼ºåº¦
            
            2. ğŸ“Š **å†å²çŸ¿å‹æ•°æ®**ï¼ˆå æ¯”30-40%ï¼‰
               - æ—¶é—´åºåˆ—æ¨¡å¼
               - ç›¸é‚»æ”¯æ¶å…³è”
            
            **å»ºè®®æ“ä½œï¼š**
            1. åœ¨å·¦ä¾§è¾¹æ æ‰¾åˆ°"æ­¥éª¤3: ğŸŒ åœ°è´¨ç‰¹å¾"
            2. å‹¾é€‰"ğŸ”§ èåˆåœ°è´¨ç‰¹å¾æ•°æ®"
            3. ä½¿ç”¨æå–çš„é’»å­”åœ°è´¨ç‰¹å¾æ–‡ä»¶
            
            âœ… **å¯ç”¨åœ°è´¨ç‰¹å¾åï¼Œé¢„æœŸRÂ²å¯æå‡15-25%ï¼**
            """)
        
        # â­ æ•°æ®æ ¼å¼è¯´æ˜ï¼ˆå›ºå®šä¸ºå•æ ·æœ¬åºåˆ—æ ¼å¼ï¼‰
        st.header("1.5 æ•°æ®æ ¼å¼è¯´æ˜")
        
        st.info("""
        ğŸ“Š **å½“å‰ä½¿ç”¨ï¼šå•æ ·æœ¬åºåˆ—æ ¼å¼**
        
        âœ… **ä¼˜ç‚¹ï¼š**
        - æ•°æ®é‡å……è¶³ï¼š195,836ä¸ªè®­ç»ƒæ ·æœ¬
        - é€‚åˆLSTM/Transformerç­‰åºåˆ—æ¨¡å‹
        - è®­ç»ƒç¨³å®šï¼Œæ”¶æ•›å¿«
        
        âš ï¸ **ä¸ºä»€ä¹ˆä¸ä½¿ç”¨"å®Œæ•´æ—¶ç©ºæ•°æ®æ ¼å¼"ï¼Ÿ**
        
        ç»æ£€æµ‹ï¼Œå½“å‰æ•°æ®é›†**ä¸é€‚åˆ**å®Œæ•´æ—¶ç©ºæ•°æ®æ ¼å¼ï¼ŒåŸå› ï¼š
        1. ä¸åŒæ”¯æ¶çš„æ—¶é—´ç‚¹ä¸å¯¹é½
        2. æ‰¾åˆ°0ä¸ªå®Œæ•´æ—¶é—´æ­¥ï¼ˆæ‰€æœ‰125ä¸ªæ”¯æ¶åŒæ—¶æœ‰æ•°æ®çš„æ—¶åˆ»ï¼‰
        3. æ•°æ®é‡‡é›†æ–¹å¼å¯¼è‡´æ—¶é—´æˆ³ä¸åŒæ­¥
        
        ğŸ’¡ **æ¨èæ–¹æ¡ˆï¼š**
        ä½¿ç”¨ **Transformer + ä¸€é”®æ™ºèƒ½æ¨¡å¼** å¯ä»¥è¾¾åˆ° RÂ² â‰¥ 0.80
        - Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å­¦ä¹ åºåˆ—å†…çš„æ—¶ç©ºå…³ç³»
        - ä¸ä¾èµ–å®Œæ•´çš„ç©ºé—´æ‹“æ‰‘ç»“æ„
        - å·²éªŒè¯åœ¨å•æ ·æœ¬æ ¼å¼ä¸‹æ•ˆæœæœ€ä½³
        
        âš ï¸ **STGCNæ¨¡å‹ä¸å¯ç”¨**ï¼šéœ€è¦å®Œæ•´æ—¶ç©ºæ•°æ®ï¼Œè¯·é€‰æ‹©Transformer
        """)
        
        # å›ºå®šä½¿ç”¨å•æ ·æœ¬åºåˆ—æ ¼å¼
        use_spatial_reconstruction = False
        
        # æ˜¾ç¤ºç‰¹å¾åˆ—è¡¨
        with st.expander("ğŸ“‹ æŸ¥çœ‹ç‰¹å¾åˆ—è¡¨"):
            st.write(f"å…± {len(feature_names)} ä¸ªç‰¹å¾:")
            for i, fname in enumerate(feature_names, 1):
                st.write(f"{i}. {fname}")
        
        # åŠ è½½åæ ‡
        coords_array = None
        if coord_file:
            if isinstance(coord_file, str):
                coord_df = pd.read_csv(coord_file)
            else:
                coord_df = load_coordinate_file(coord_file)
            
            coords_array = coord_df[['x', 'y']].values
            st.write(f"**æ”¯æ¶åæ ‡:** {coords_array.shape}")
        else:
            # ä½¿ç”¨é»˜è®¤çº¿æ€§åæ ‡
            coords_array = np.column_stack([np.arange(NUM_NODES), np.zeros(NUM_NODES)])
            st.info("ä½¿ç”¨é»˜è®¤çº¿æ€§åæ ‡")
        
        # æ•°æ®åˆ†å‰²
        st.header("2. æ•°æ®åˆ†å‰²")
        
        st.info("""
        ğŸ’¡ **æ•°æ®é‡å……è¶³**ï¼šå½“å‰æœ‰ 195,836 ä¸ªæ ·æœ¬ï¼Œæ•°æ®é‡å……è¶³é€‚åˆæ·±åº¦å­¦ä¹ ã€‚
        
        å»ºè®®æ¯”ä¾‹ï¼šè®­ç»ƒ 70% / éªŒè¯ 15% / æµ‹è¯• 15%
        """)
        
        train_ratio = st.slider("è®­ç»ƒé›†æ¯”ä¾‹", 0.5, 0.9, 0.7, 0.05)
        val_ratio = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.05, 0.3, 0.15, 0.05)
        
        train_end = int(NUM_SAMPLES * train_ratio)
        val_end = int(NUM_SAMPLES * (train_ratio + val_ratio))
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("è®­ç»ƒé›†", f"{train_end:,}", f"{train_ratio*100:.0f}%")
        with col2:
            st.metric("éªŒè¯é›†", f"{val_end-train_end:,}", f"{val_ratio*100:.0f}%")
        with col3:
            st.metric("æµ‹è¯•é›†", f"{NUM_SAMPLES-val_end:,}", f"{(1-train_ratio-val_ratio)*100:.0f}%")
        
        # è½¬æ¢ä¸ºå›¾æ•°æ®æ ¼å¼
        # ç”±äºé¢„å¤„ç†æ•°æ®å·²ç»æ˜¯ (num_samples, seq_len, num_features)
        # æˆ‘ä»¬éœ€è¦é‡å¡‘ä¸º (num_samples_per_support, num_supports, seq_len, num_features)
        
        st.header("3. å›¾ç»“æ„æ„å»º")
        
        # ç”Ÿæˆé‚»æ¥çŸ©é˜µ
        adj_method = st.selectbox(
            "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼",
            ["distance", "knn", "chain", "full"],
            help="distance: åŸºäºåæ ‡è·ç¦» | knn: Kè¿‘é‚» | chain: é“¾å¼ | full: å…¨è¿æ¥"
        )
        
        adj_params = {}
        if adj_method == "knn":
            adj_params['k'] = st.number_input("Kå€¼(è¿‘é‚»æ•°é‡)", min_value=1, value=5, max_value=20)
        elif adj_method == "distance":
            adj_params['threshold'] = st.number_input("è·ç¦»é˜ˆå€¼(ç±³)", min_value=0.1, value=5.0, step=0.5)
        
        adj_mx = generate_adjacency_matrix(
            NUM_NODES,
            method=adj_method,
            coords=coords_array,
            **adj_params
        )
        
        # æ˜¾ç¤ºå›¾ä¿¡æ¯
        num_edges = np.sum(adj_mx > 0)
        avg_degree = num_edges / NUM_NODES
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("èŠ‚ç‚¹æ•°", NUM_NODES)
            st.metric("å¹³å‡åº¦æ•°", f"{avg_degree:.1f}")
        with col2:
            st.metric("è¾¹æ•°", int(num_edges))
            st.metric("å›¾å¯†åº¦", f"{num_edges/(NUM_NODES*(NUM_NODES-1))*100:.2f}%")
        
        # å¯è§†åŒ–é‚»æ¥çŸ©é˜µ
        with st.expander("ğŸ” æŸ¥çœ‹é‚»æ¥çŸ©é˜µ"):
            fig, ax = plt.subplots(figsize=(8, 8))
            im = ax.imshow(adj_mx, cmap='Blues', aspect='auto')
            ax.set_title("é‚»æ¥çŸ©é˜µ")
            ax.set_xlabel("æ”¯æ¶ ID")
            ax.set_ylabel("æ”¯æ¶ ID")
            plt.colorbar(im, ax=ax)
            st.pyplot(fig)
        
        # æ¨¡å‹è®­ç»ƒéƒ¨åˆ†
        st.header("4. ğŸš€ å¼€å§‹è®­ç»ƒ")
        
        # â­â­â­ å…³é”®é…ç½®ï¼šç‰¹å¾å·¥ç¨‹æ§åˆ¶
        st.markdown("---")
        st.markdown("### ğŸ”§ ç‰¹å¾é…ç½®ï¼ˆé‡è¦ï¼ï¼‰")
        
        # å¼ºåˆ¶æç¤ºæ¡†
        st.error("""
        âš ï¸âš ï¸âš ï¸ **é‡è¦è­¦å‘Šï¼šç‰¹å¾å·¥ç¨‹å·²çŸ¥ä¼šå¯¼è‡´NaNï¼**
        
        å¦‚æœä½ çœ‹åˆ°"264ä¸ªç‰¹å¾"æˆ–è®­ç»ƒå‡ºç°NaNï¼Œè¯´æ˜ç‰¹å¾å·¥ç¨‹è¢«è¯¯å¯ç”¨äº†ï¼
        
        âœ… **è¯·ç¡®ä¿ä¸‹é¢çš„é€‰é¡¹ä¸ºï¼šç¦ç”¨ç‰¹å¾å·¥ç¨‹**
        """)
        
        # é»˜è®¤ç¦ç”¨ç‰¹å¾å·¥ç¨‹
        use_feature_engineering = st.checkbox(
            "ï¿½ å¯ç”¨ç‰¹å¾å·¥ç¨‹ï¼ˆâš ï¸ä¸æ¨èï¼Œå·²çŸ¥å¯¼è‡´NaNï¼‰",
            value=False,
            help="âŒ æ­¤åŠŸèƒ½ä¼šæ·»åŠ 200+ç‰¹å¾ä½†å¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼Œé™¤éä½ çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆï¼Œå¦åˆ™ä¸è¦å‹¾é€‰ï¼"
        )
        
        if use_feature_engineering:
            st.error("""
            âŒ **ä½ å¯ç”¨äº†ç‰¹å¾å·¥ç¨‹ï¼è¿™ä¼šå¯¼è‡´264ä¸ªç‰¹å¾å’ŒNaNï¼**
            
            å»ºè®®ï¼š
            1. ç«‹å³å–æ¶ˆå‹¾é€‰"å¯ç”¨ç‰¹å¾å·¥ç¨‹"
            2. ä½¿ç”¨åŸºç¡€25ä¸ªç‰¹å¾ï¼ˆ17çŸ¿å‹+8åœ°è´¨ï¼‰
            3. è®­ç»ƒæˆåŠŸåå†è€ƒè™‘æ˜¯å¦éœ€è¦æ›´å¤šç‰¹å¾
            """)
        else:
            st.success("""
            âœ… **ç‰¹å¾å·¥ç¨‹å·²ç¦ç”¨ï¼ˆæ¨èé…ç½®ï¼‰**
            - ä½¿ç”¨25ä¸ªåŸºç¡€ç‰¹å¾ï¼ˆ17çŸ¿å‹ + 8åœ°è´¨ï¼‰
            - æ•°å€¼ç¨³å®šï¼Œä¸ä¼šå‡ºç°NaN
            - å·²è¶³å¤Ÿè¾¾åˆ°RÂ²â‰¥0.70çš„ç›®æ ‡
            """)
        
        # ç´§æ€¥æ¨¡å¼ï¼ˆå¤‡ç”¨é€‰é¡¹ï¼‰
        emergency_mode = st.checkbox(
            "ï¿½ é¢å¤–ä¿å®ˆæ¨¡å¼ï¼ˆå¦‚æœåŸºç¡€é…ç½®ä»å¤±è´¥ï¼‰",
            value=False,
            help="è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ã€å‡å°‘æ‰¹æ¬¡å¤§å°ç­‰ï¼Œä»…åœ¨åŸºç¡€é…ç½®æ— æ³•è®­ç»ƒæ—¶å¯ç”¨"
        )
        
        if emergency_mode:
            st.warning("""
            âš ï¸ **é¢å¤–ä¿å®ˆæ¨¡å¼å·²å¯ç”¨**
            - å¼ºåˆ¶ä½¿ç”¨æœ€ä¿å®ˆçš„è®­ç»ƒå‚æ•°
            - å­¦ä¹ ç‡é™ä½10å€
            - æ‰¹æ¬¡å¤§å°å‡åŠ
            """)
            use_feature_engineering = False  # å¼ºåˆ¶ç¦ç”¨
        
        # æ¨¡å‹é€‰æ‹©
        st.markdown("### ğŸ“Š é€‰æ‹©AIæ¨¡å‹")
        
        # â­ æ ¹æ®ç´§æ€¥æ¨¡å¼è°ƒæ•´æ¨è
        if emergency_mode:
            model_options = [
                "LSTM (åŸºç¡€ç‰ˆ) - ğŸš¨ ç´§æ€¥æ¨¡å¼æ¨è", 
                "AttentionLSTM (æ³¨æ„åŠ›æœºåˆ¶) - ä¸­ç­‰å¤æ‚åº¦", 
                "Transformer (æœ€å¼ºè¡¨è¾¾åŠ›) - é«˜çº§é€‰é¡¹"
            ]
            default_model_idx = 0  # ç´§æ€¥æ¨¡å¼é»˜è®¤LSTM
            help_text = """
            ğŸš¨ ç´§æ€¥æ¨¡å¼å»ºè®®ï¼š
            LSTM: æœ€ç®€å•ç¨³å®šï¼Œä¼˜å…ˆéªŒè¯è®­ç»ƒæµç¨‹ âœ…
            AttentionLSTM: ä¸­ç­‰å¤æ‚åº¦ï¼Œè®­ç»ƒæˆåŠŸåå¯å°è¯•
            Transformer: æœ€å¤æ‚ï¼Œç¡®è®¤åŸºç¡€è®­ç»ƒæ­£å¸¸åå†ç”¨
            """
        else:
            model_options = [
                "LSTM (åŸºç¡€ç‰ˆ) - å¿«é€ŸéªŒè¯", 
                "AttentionLSTM (æ³¨æ„åŠ›æœºåˆ¶) - ä¸­ç­‰æ•ˆæœ", 
                "Transformer (æœ€å¼ºè¡¨è¾¾åŠ›)ğŸš€ - å¼ºçƒˆæ¨è"
            ]
            default_model_idx = 2  # æ­£å¸¸æ¨¡å¼é»˜è®¤Transformer
            help_text = """
            LSTM: ç®€å•å¿«é€Ÿï¼Œé€‚åˆå¿«é€ŸéªŒè¯ (é¢„æœŸRÂ²â‰ˆ0.35-0.45)
            AttentionLSTM: æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸­ç­‰æ•ˆæœ (é¢„æœŸRÂ²â‰ˆ0.40-0.55)
            Transformer: è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ€å¼ºè¡¨è¾¾èƒ½åŠ› (é¢„æœŸRÂ²â‰ˆ0.65-0.85)ğŸ”¥
            
            âš ï¸ STGCNå·²ç¦ç”¨ï¼šå½“å‰æ•°æ®ä¸æ”¯æŒå®Œæ•´æ—¶ç©ºæ ¼å¼ï¼ˆ0ä¸ªå®Œæ•´æ—¶é—´æ­¥ï¼‰
            """
        
        model_type = st.radio(
            "é€‰æ‹©æ¨¡å‹ ğŸ‘‡",
            model_options,
            index=default_model_idx,
            help=help_text
        )
        
        # â­ STGCNè¯´æ˜ï¼ˆå·²ç¦ç”¨ï¼‰
        with st.expander("âŒ ä¸ºä»€ä¹ˆSTGCNä¸å¯ç”¨ï¼Ÿ"):
            st.warning("""
            **STGCNå›¾ç¥ç»ç½‘ç»œå·²ç¦ç”¨ï¼ŒåŸå› ï¼š**
            
            1. **æ•°æ®ä¸å…¼å®¹**ï¼šSTGCNéœ€è¦å®Œæ•´çš„æ—¶ç©ºæ•°æ®çŸ©é˜µ
               - è¦æ±‚ï¼šæ‰€æœ‰125ä¸ªæ”¯æ¶åœ¨æ¯ä¸ªæ—¶é—´ç‚¹éƒ½æœ‰æ•°æ®
               - ç°çŠ¶ï¼šæ£€æµ‹åˆ°**0ä¸ªå®Œæ•´æ—¶é—´æ­¥**
            
            2. **æ•°æ®é‡‡é›†é—®é¢˜**ï¼š
               - ä¸åŒæ”¯æ¶çš„æ—¶é—´æˆ³ä¸åŒæ­¥
               - å­˜åœ¨å¤§é‡ç¼ºå¤±å€¼
               - æ— æ³•æ„å»ºæœ‰æ•ˆçš„ç©ºé—´æ‹“æ‰‘å›¾
            
            3. **æ¨èæ›¿ä»£æ–¹æ¡ˆ**ï¼š
               âœ… **Transformeræ¨¡å‹** - åŒæ ·å¼ºå¤§ï¼Œä¸”é€‚é…å½“å‰æ•°æ®
               - ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ åºåˆ—ä¾èµ–
               - ä¸ä¾èµ–å®Œæ•´çš„ç©ºé—´ç»“æ„
               - å·²éªŒè¯å¯è¾¾åˆ°RÂ²â‰¥0.80
            
            ğŸ’¡ **å¦‚æœå°†æ¥æ•°æ®å¯¹é½è‰¯å¥½ï¼Œå¯ä»¥é‡æ–°å¯ç”¨STGCN**
            """)
        
        # Transformeræ¨èè¯´æ˜
        if "Transformer" in model_type:
            st.info("""
            ğŸš€ **æœ€å¼ºé…ç½®ï¼š** Transformer + å¢å¼ºç‰¹å¾å·¥ç¨‹
            
            - Self-Attentionæœºåˆ¶æ•æ‰é•¿è·ç¦»æ—¶åºä¾èµ–
            - é€‚ç”¨äºå•æ ·æœ¬æ ¼å¼ï¼ˆä¿ç•™å…¨éƒ¨æ ·æœ¬ï¼‰
            - é¢„æœŸRÂ²: 0.65-0.80
            """)

        
        st.info(f"""
        **{model_type}**
        
        {'- åŸºç¡€LSTMæ¨¡å‹ï¼Œç›´æ¥åºåˆ—é¢„æµ‹' if 'LSTM (åŸºç¡€ç‰ˆ)' in model_type else ''}
        {'- â­ å¸¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œè‡ªåŠ¨å­¦ä¹ é‡è¦æ—¶é—´æ­¥' if 'AttentionLSTM' in model_type else ''}
        {'- ğŸš€ Self-Attentionæœºåˆ¶ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Œæœ€å¼ºè¡¨è¾¾èƒ½åŠ›' if 'Transformer' in model_type else ''}
        {'- å›¾å·ç§¯ç½‘ç»œï¼Œå­¦ä¹ ç©ºé—´-æ—¶é—´è”åˆæ¨¡å¼' if 'STGCN' in model_type else ''}
        """)
        
        # â­ æ™ºèƒ½ä¼˜åŒ–é¢æ¿ï¼ˆæ–°å¢ï¼‰
        st.markdown("### ğŸš€ æ™ºèƒ½ä¼˜åŒ–è®¾ç½®")
        
        # ä¸€é”®æ™ºèƒ½æ¨¡å¼
        smart_mode = st.checkbox(
            "ğŸ¤– ä¸€é”®æ™ºèƒ½æ¨¡å¼ï¼ˆæ¨èæ–°æ‰‹ï¼‰",
            value=True,
            help="è‡ªåŠ¨å¯ç”¨æ‰€æœ‰æ¨èä¼˜åŒ–ï¼Œç›®æ ‡RÂ²â‰¥0.8"
        )
        
        if smart_mode:
            if emergency_mode:
                st.success("âœ… ç´§æ€¥æ¨¡å¼ä¸‹æ™ºèƒ½ä¼˜åŒ–å·²ç®€åŒ–ï¼šä»…å¯ç”¨æ¢¯åº¦ç´¯ç§¯")
                use_combined_loss = False  # ç´§æ€¥æ¨¡å¼ä¸‹ç¦ç”¨ç»„åˆæŸå¤±
                use_gradient_accumulation = True
                use_amp = False  # ç¦ç”¨æ··åˆç²¾åº¦
                show_advanced_plots = True
                use_ensemble = False
                use_kfold = False
            else:
                st.success("âœ… å·²å¯ç”¨ï¼šç»„åˆæŸå¤± + æ¢¯åº¦ç´¯ç§¯ + æ··åˆç²¾åº¦ + é«˜çº§å¯è§†åŒ–")
                use_combined_loss = True
                use_gradient_accumulation = True
                use_amp = True
                show_advanced_plots = True
                use_ensemble = False  # é›†æˆå­¦ä¹ è€—æ—¶è¾ƒé•¿ï¼Œé»˜è®¤å…³é—­
                use_kfold = False  # KæŠ˜éªŒè¯è€—æ—¶è¾ƒé•¿ï¼Œé»˜è®¤å…³é—­
        else:
            # æ‰‹åŠ¨æ§åˆ¶
            st.info("ğŸ’¡ æ‰‹åŠ¨æ¨¡å¼ï¼šå¯ä»¥å•ç‹¬æ§åˆ¶æ¯é¡¹ä¼˜åŒ–")
            col_opt1, col_opt2 = st.columns(2)
            with col_opt1:
                use_combined_loss = st.checkbox("ç»„åˆæŸå¤±å‡½æ•°", value=True, help="Huber+MAE+MAPEï¼Œæå‡5-10%")
                use_gradient_accumulation = st.checkbox("æ¢¯åº¦ç´¯ç§¯", value=True, help="æ¨¡æ‹Ÿ4å€æ‰¹æ¬¡å¤§å°")
                use_amp = st.checkbox("æ··åˆç²¾åº¦è®­ç»ƒ", value=False, help="ä»…GPUæ”¯æŒï¼Œå‡å°‘æ˜¾å­˜50%")
            with col_opt2:
                show_advanced_plots = st.checkbox("é«˜çº§è¯Šæ–­å›¾è¡¨", value=True, help="æ®‹å·®å›¾+Q-Qå›¾ç­‰")
                use_ensemble = st.checkbox("é›†æˆå­¦ä¹ (3æ¨¡å‹)", value=False, help="æå‡3-5%ï¼Œä½†è®­ç»ƒæ—¶é—´Ã—3")
                use_kfold = st.checkbox("KæŠ˜äº¤å‰éªŒè¯", value=False, help="æ›´å‡†ç¡®è¯„ä¼°ï¼Œä½†è€—æ—¶Ã—5")
        
        # è®­ç»ƒå‚æ•°
        st.markdown("### âš™ï¸ åŸºç¡€å‚æ•°")
        col1, col2 = st.columns(2)
        with col1:
            epochs = st.number_input("è®­ç»ƒè½®æ•°", min_value=1, value=100, max_value=500)
            # â­ ç´§æ€¥æ¨¡å¼ä¸‹ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°
            default_batch = 64 if emergency_mode else 128
            batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=16, value=default_batch, max_value=512, step=16)
        with col2:
            # â­ ç´§æ€¥æ¨¡å¼ä¸‹ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
            default_lr = 0.00005 if emergency_mode else 0.0001
            learning_rate = st.number_input("å­¦ä¹ ç‡", min_value=0.00001, value=default_lr, max_value=0.1, format="%.5f", step=0.00001)
            # â­ ç´§æ€¥æ¨¡å¼ä¸‹ä½¿ç”¨æ›´å°çš„éšè—å±‚ç»´åº¦
            default_hidden = 64 if emergency_mode else 128
            hidden_dim = st.number_input("éšè—å±‚ç»´åº¦", min_value=16, value=default_hidden, max_value=256, step=16)
        
        # â­ STGCNå›¾ç»“æ„é€‰æ‹©
        # â­ STGCNå›¾ç»“æ„é€‰æ‹©
        adj_method = 'chain'  # é»˜è®¤å€¼
        adj_threshold = 10
        adj_sigma = 5
        adj_rows = 5
        adj_k = 8
        
        if 'STGCN' in model_type:
            st.markdown("### ğŸ”— å›¾ç»“æ„é…ç½®")
            adj_method = st.selectbox(
                "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹æ³•",
                ["adaptive", "grid", "chain", "knn"],
                index=0,
                help="""
                adaptive: è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾ï¼ˆæ¨èï¼ŒRÂ²æå‡10-20%ï¼‰
                grid: ç½‘æ ¼ç»“æ„ï¼ˆé€‚åˆè§„åˆ™æ’åˆ—ï¼‰
                chain: é“¾å¼ç»“æ„ï¼ˆç®€å•åœºæ™¯ï¼‰
                knn: Kè¿‘é‚»å›¾ï¼ˆçµæ´»è¿æ¥ï¼‰
                """
            )
            
            if adj_method == 'adaptive':
                col_a, col_b = st.columns(2)
                with col_a:
                    adj_threshold = st.slider("è·ç¦»é˜ˆå€¼", 5, 20, 10, help="è¶…è¿‡æ­¤è·ç¦»çš„æ”¯æ¶ä¸è¿æ¥")
                with col_b:
                    adj_sigma = st.slider("é«˜æ–¯æ ¸å‚æ•°", 2, 10, 5, help="æ§åˆ¶æƒé‡è¡°å‡é€Ÿåº¦")
            elif adj_method == 'grid':
                adj_rows = st.number_input("ç½‘æ ¼è¡Œæ•°", 1, 20, 5)
            elif adj_method == 'knn':
                adj_k = st.number_input("Kè¿‘é‚»æ•°", 1, 20, 8)
        
        # ä¼˜åŒ–å»ºè®®
        with st.expander("ğŸ’¡ è®­ç»ƒä¼˜åŒ–å»ºè®® - ç›®æ ‡RÂ²â‰¥0.8"):
            st.markdown("""
            **â­ æ¨èé…ç½®ï¼ˆå†²å‡»RÂ²â‰¥0.8ï¼‰ï¼š**
            
            1. **æ•°æ®æ ¼å¼** â†’ å®Œæ•´æ—¶ç©ºæ•°æ®ï¼ˆå¿…é€‰ï¼‰
            2. **ç‰¹å¾å·¥ç¨‹** â†’ å¯ç”¨ï¼ˆå¿…é€‰ï¼Œæ–°å¢10+ç‰¹å¾ï¼‰
            3. **æ¨¡å‹é€‰æ‹©** â†’ Transformer æˆ– STGCN + adaptiveå›¾
            4. **è®­ç»ƒè½®æ•°** â†’ 100-150è½®
            5. **æ‰¹æ¬¡å¤§å°** â†’ 128ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œæ•ˆæœï¼‰
            6. **å­¦ä¹ ç‡** â†’ 0.001ï¼ˆå·²å«warmupï¼‰
            
            **å¦‚æœæ•ˆæœä¸å¥½ï¼Œå¯ä»¥å°è¯•ï¼š**
            
            1. **Transformeræ¨¡å‹** â†’ d_model=128, nhead=8ï¼ˆæ¨èï¼‰
            2. **STGCN + adaptiveå›¾** â†’ threshold=10, sigma=5
            3. **å¢åŠ è®­ç»ƒè½®æ•°** â†’ æ”¹ä¸º 150-200 è½®
            4. **è°ƒæ•´å­¦ä¹ ç‡** â†’ å°è¯• 0.0005-0.002 ä¹‹é—´
            
            **å½“å‰ä¼˜åŒ–ï¼š**
            - âœ… å¢å¼ºç‰¹å¾å·¥ç¨‹ï¼ˆç»Ÿè®¡ã€å·®åˆ†ã€æ»‘åŠ¨çª—å£ã€äº¤å‰ç‰¹å¾ï¼‰
            - âœ… è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾ï¼ˆé«˜æ–¯æ ¸æƒé‡ï¼‰
            - âœ… Transformeræ¶æ„ï¼ˆSelf-Attentionæœºåˆ¶ï¼‰
            - âœ… å­¦ä¹ ç‡warmup + ä½™å¼¦é€€ç«
            - âœ… Huber Lossï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
            
            **ç†æƒ³æŒ‡æ ‡ï¼š**
            - MAE < 10 MPa
            - RMSE < 15 MPa  
            - RÂ² â‰¥ 0.8
            """)
        
        if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary"):
            try:
                st.success("å¼€å§‹è®­ç»ƒSTGCNæ¨¡å‹...")
                
                # 1. æ•°æ®åˆ‡åˆ†
                st.write("### æ­¥éª¤1: æ•°æ®åˆ‡åˆ†")
                
                # â­ æ ¹æ®æ•°æ®æ ¼å¼è®¡ç®—å®é™…æ ·æœ¬æ•°
                actual_num_samples = len(X)
                
                # æ£€æŸ¥æ ·æœ¬æ•°æ˜¯å¦è¶³å¤Ÿ
                if actual_num_samples < 100:
                    st.error(f"""
                    âŒ **æ•°æ®é‡ä¸è¶³ï¼**
                    
                    å½“å‰æ ·æœ¬æ•°: {actual_num_samples}
                    æœ€å°‘éœ€è¦: 100æ ·æœ¬
                    
                    **å¯èƒ½åŸå› ï¼š**
                    1. å®Œæ•´æ—¶ç©ºæ•°æ®é‡æ„åæ ·æœ¬æ•°å¤§å¹…å‡å°‘ï¼ˆåŸ195,836 â†’ {actual_num_samples}ï¼‰
                    2. æ•°æ®ä¸­ç¼ºå¤±å€¼è¿‡å¤šï¼Œå¯¼è‡´å®Œæ•´æ—¶é—´æ­¥è¾ƒå°‘
                    
                    **è§£å†³æ–¹æ¡ˆï¼š**
                    1. â­ åˆ‡æ¢åˆ°"å•æ ·æœ¬åºåˆ—æ ¼å¼"ï¼ˆä¸é‡æ„ï¼Œä¿ç•™å…¨éƒ¨æ ·æœ¬ï¼‰
                    2. æ£€æŸ¥åŸå§‹CSVæ•°æ®è´¨é‡
                    3. è°ƒæ•´æ—¶åºçª—å£å‚æ•°ï¼ˆå‡å°seq_lenï¼‰
                    """)
                    st.stop()
                
                # é‡æ–°è®¡ç®—åˆ‡åˆ†ç‚¹ï¼ˆåŸºäºå®é™…æ ·æœ¬æ•°ï¼‰
                train_end_actual = int(actual_num_samples * train_ratio)
                val_end_actual = int(actual_num_samples * (train_ratio + val_ratio))
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›æ ·æœ¬
                if train_end_actual < 10:
                    st.error(f"è®­ç»ƒé›†æ ·æœ¬æ•°å¤ªå°‘({train_end_actual})ï¼Œè¯·å¢åŠ train_ratioæˆ–åˆ‡æ¢æ•°æ®æ ¼å¼")
                    st.stop()
                
                if val_end_actual - train_end_actual < 5:
                    st.error(f"éªŒè¯é›†æ ·æœ¬æ•°å¤ªå°‘({val_end_actual - train_end_actual})ï¼Œè¯·å¢åŠ val_ratio")
                    st.stop()
                
                st.info(f"""
                ğŸ“Š **å®é™…æ•°æ®åˆ‡åˆ†ï¼ˆåŸºäº {actual_num_samples} ä¸ªæ ·æœ¬ï¼‰ï¼š**
                - è®­ç»ƒé›†: {train_end_actual} æ ·æœ¬ ({train_ratio*100:.0f}%)
                - éªŒè¯é›†: {val_end_actual - train_end_actual} æ ·æœ¬ ({val_ratio*100:.0f}%)
                - æµ‹è¯•é›†: {actual_num_samples - val_end_actual} æ ·æœ¬ ({(1-train_ratio-val_ratio)*100:.0f}%)
                """)
                
                # æ ¹æ®æ•°æ®æ ¼å¼ä¸åŒï¼Œé‡‡ç”¨ä¸åŒçš„åˆ‡åˆ†æ–¹å¼
                if use_spatial_reconstruction:
                    # æ—¶ç©ºæ•°æ®æ ¼å¼ï¼š(num_timesteps, num_supports, seq_len, num_features)
                    # ç›®æ ‡ï¼š(num_timesteps, num_supports)
                    st.info("ä½¿ç”¨å®Œæ•´æ—¶ç©ºæ•°æ®æ ¼å¼")
                    
                    X_train = X[:train_end_actual]  # (T_train, N, seq_len, F)
                    y_train = y_final[:train_end_actual]  # (T_train, N)
                    train_support_ids = None  # ä¸å†éœ€è¦
                    
                    X_val = X[train_end_actual:val_end_actual]
                    y_val = y_final[train_end_actual:val_end_actual]
                    val_support_ids = None
                    
                    X_test = X[val_end_actual:]
                    y_test = y_final[val_end_actual:]
                    test_support_ids = None
                    
                else:
                    # å•æ”¯æ¶åºåˆ—æ ¼å¼ï¼š(num_samples, seq_len, num_features)
                    st.info("ä½¿ç”¨å•æ”¯æ¶åºåˆ—æ ¼å¼")
                    
                    X_train = X[:train_end_actual]
                    y_train = y_final[:train_end_actual]
                    train_support_ids = support_ids[:train_end_actual]
                    
                    X_val = X[train_end_actual:val_end_actual]
                    y_val = y_final[train_end_actual:val_end_actual]
                    val_support_ids = support_ids[train_end_actual:val_end_actual]
                    
                    X_test = X[val_end_actual:]
                    y_test = y_final[val_end_actual:]
                    test_support_ids = support_ids[val_end_actual:]
                
                st.write(f"âœ“ è®­ç»ƒé›†: {len(X_train):,} {'æ—¶é—´æ­¥' if use_spatial_reconstruction else 'æ ·æœ¬'}")
                st.write(f"âœ“ éªŒè¯é›†: {len(X_val):,} {'æ—¶é—´æ­¥' if use_spatial_reconstruction else 'æ ·æœ¬'}")
                st.write(f"âœ“ æµ‹è¯•é›†: {len(X_test):,} {'æ—¶é—´æ­¥' if use_spatial_reconstruction else 'æ ·æœ¬'}")
                
                # â­â­â­ ç´§æ€¥æ¨¡å¼ï¼šæ•°æ®æŠ½æ ·ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
                if emergency_mode and len(X_train) > 10000:
                    st.write("### æ­¥éª¤1.1: ğŸš¨ ç´§æ€¥æ¨¡å¼æ•°æ®æŠ½æ ·")
                    st.warning(f"""
                    âš ï¸ è®­ç»ƒé›†æœ‰ {len(X_train):,} ä¸ªæ ·æœ¬ï¼Œä¸ºå¿«é€ŸéªŒè¯è®­ç»ƒæµç¨‹ï¼Œå°†æŠ½æ ·10,000ä¸ª
                    - æŠ½æ ·åå¯å¿«é€Ÿå®Œæˆè®­ç»ƒï¼ˆçº¦1-2åˆ†é’Ÿï¼‰
                    - éªŒè¯è®­ç»ƒæµç¨‹æ­£å¸¸åï¼Œå¯å…³é—­ç´§æ€¥æ¨¡å¼ä½¿ç”¨å…¨éƒ¨æ•°æ®
                    """)
                    
                    # éšæœºæŠ½æ ·
                    import random
                    train_indices = random.sample(range(len(X_train)), min(10000, len(X_train)))
                    X_train = X_train[train_indices]
                    y_train = y_train[train_indices]
                    if train_support_ids is not None:
                        train_support_ids = train_support_ids[train_indices]
                    
                    # éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹Ÿé€‚å½“å‡å°‘
                    if len(X_val) > 2000:
                        val_indices = random.sample(range(len(X_val)), 2000)
                        X_val = X_val[val_indices]
                        y_val = y_val[val_indices]
                        if val_support_ids is not None:
                            val_support_ids = val_support_ids[val_indices]
                    
                    if len(X_test) > 2000:
                        test_indices = random.sample(range(len(X_test)), 2000)
                        X_test = X_test[test_indices]
                        y_test = y_test[test_indices]
                        if test_support_ids is not None:
                            test_support_ids = test_support_ids[test_indices]
                    
                    st.success(f"""
                    âœ… æŠ½æ ·å®Œæˆï¼
                    - è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬
                    - éªŒè¯é›†: {len(X_val):,} æ ·æœ¬
                    - æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬
                    """)
                
                # â­ åœ°è´¨ç‰¹å¾èåˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if use_geological and geo_file and coords_array is not None:
                    st.write("### æ­¥éª¤1.4: åœ°è´¨ç‰¹å¾èåˆ ğŸŒ")
                    st.info("æ­£åœ¨èåˆåœ°è´¨ç‰¹å¾æ•°æ®...")
                    
                    try:
                        # åŠ è½½åœ°è´¨ç‰¹å¾
                        geo_features, geo_feature_names = load_geological_features(geo_file, coords_array)
                        
                        st.write(f"**åœ°è´¨ç‰¹å¾å½¢çŠ¶:** {geo_features.shape}")
                        st.write(f"**åœ°è´¨ç‰¹å¾æ•°é‡:** {len(geo_feature_names)}")
                        
                        # â­ å…³é”®ä¿®å¤ï¼šå½’ä¸€åŒ–åœ°è´¨ç‰¹å¾ï¼Œé¿å…æ•°å€¼èŒƒå›´å·®å¼‚å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
                        from sklearn.preprocessing import StandardScaler
                        geo_scaler = StandardScaler()
                        geo_features_normalized = geo_scaler.fit_transform(geo_features)
                        
                        st.info(f"""
                        ğŸ”§ åœ°è´¨ç‰¹å¾å½’ä¸€åŒ–:
                        - åŸå§‹èŒƒå›´: [{geo_features.min():.2f}, {geo_features.max():.2f}]
                        - å½’ä¸€åŒ–å: [{geo_features_normalized.min():.2f}, {geo_features_normalized.max():.2f}]
                        - æ–¹æ³•: StandardScaler (é›¶å‡å€¼å•ä½æ–¹å·®)
                        """)
                        
                        # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å¯¹åº”æ”¯æ¶çš„åœ°è´¨ç‰¹å¾
                        def add_geo_features_to_samples(X_data, sample_support_ids):
                            """ä¸ºæ ·æœ¬æ·»åŠ åœ°è´¨ç‰¹å¾ï¼ˆå·²å½’ä¸€åŒ–ï¼‰"""
                            B, T, F = X_data.shape
                            G = geo_features_normalized.shape[1]  # åœ°è´¨ç‰¹å¾æ•°é‡
                            
                            # åˆ›å»ºæ–°æ•°æ® (B, T, F+G)
                            X_with_geo = np.zeros((B, T, F + G))
                            X_with_geo[:, :, :F] = X_data  # åŸå§‹ç‰¹å¾
                            
                            # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å¯¹åº”æ”¯æ¶çš„åœ°è´¨ç‰¹å¾ï¼ˆåœ¨æ¯ä¸ªæ—¶é—´æ­¥é‡å¤ï¼‰
                            for i, sup_id in enumerate(sample_support_ids):
                                if sup_id < len(geo_features_normalized):
                                    # åœ°è´¨ç‰¹å¾åœ¨æ‰€æœ‰æ—¶é—´æ­¥ä¿æŒä¸å˜
                                    X_with_geo[i, :, F:] = geo_features_normalized[sup_id]
                            
                            return X_with_geo
                        
                        # èåˆåˆ°è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
                        X_train = add_geo_features_to_samples(X_train, train_support_ids)
                        X_val = add_geo_features_to_samples(X_val, val_support_ids)
                        X_test = add_geo_features_to_samples(X_test, test_support_ids)
                        
                        # æ›´æ–°ç‰¹å¾åç§°
                        feature_names = feature_names + geo_feature_names
                        
                        st.success(f"""
                        âœ… åœ°è´¨ç‰¹å¾èåˆå®Œæˆï¼
                        - åœ°è´¨ç‰¹å¾æ•°: {len(geo_feature_names)}
                        - æ–°å¢ç‰¹å¾: {', '.join(geo_feature_names[:5])}{'...' if len(geo_feature_names) > 5 else ''}
                        - æ€»ç‰¹å¾æ•°: {X_train.shape[-1]}
                        - é¢„æœŸRÂ²æå‡: +15-25% ğŸ¯
                        """)
                        
                    except Exception as e:
                        st.error(f"åœ°è´¨ç‰¹å¾èåˆå¤±è´¥: {str(e)}")
                        st.warning("å°†ç»§ç»­ä½¿ç”¨ä¸å«åœ°è´¨ç‰¹å¾çš„æ•°æ®è®­ç»ƒ")
                
                # â­ ç‰¹å¾å·¥ç¨‹ï¼ˆâš ï¸å±é™©åŠŸèƒ½ï¼Œé»˜è®¤ç¦ç”¨ï¼‰
                if use_feature_engineering:
                    st.error("âš ï¸âš ï¸âš ï¸ è­¦å‘Šï¼šä½ å¯ç”¨äº†ç‰¹å¾å·¥ç¨‹ï¼è¿™ä¼šå¯¼è‡´NaNï¼")
                    st.write("### æ­¥éª¤1.5: ç‰¹å¾å·¥ç¨‹ ğŸ”§ï¼ˆä¸æ¨èï¼‰")
                    st.info("æ­£åœ¨ç”Ÿæˆå·¥ç¨‹ç‰¹å¾...")
                    
                    original_feature_count = X_train.shape[-1]
                    
                    X_train, new_feature_names = add_engineered_features(X_train, feature_names)
                    X_val, _ = add_engineered_features(X_val, feature_names)
                    X_test, _ = add_engineered_features(X_test, feature_names)
                    
                    enhanced_feature_count = X_train.shape[-1]
                    added_features = enhanced_feature_count - original_feature_count
                    
                    st.error(f"""
                    âš ï¸ ç‰¹å¾å·¥ç¨‹å·²æ‰§è¡Œï¼ˆé«˜é£é™©ï¼‰
                    - åŸå§‹ç‰¹å¾æ•°: {original_feature_count}
                    - æ–°å¢ç‰¹å¾æ•°: {added_features}
                    - æ€»ç‰¹å¾æ•°: {enhanced_feature_count}
                    - âš ï¸ è­¦å‘Šï¼š{enhanced_feature_count}ä¸ªç‰¹å¾å¾ˆå¯èƒ½å¯¼è‡´è®­ç»ƒå¤±è´¥ï¼
                    - å»ºè®®ï¼šå–æ¶ˆå‹¾é€‰"å¯ç”¨ç‰¹å¾å·¥ç¨‹"ï¼Œä½¿ç”¨{original_feature_count}ä¸ªåŸºç¡€ç‰¹å¾
                    """)
                    
                    # æ›´æ–°feature_names
                    feature_names = new_feature_names
                    
                    # â­â­â­ å…³é”®ä¿®å¤ï¼šç‰¹å¾å·¥ç¨‹åé‡æ–°å½’ä¸€åŒ–æ‰€æœ‰ç‰¹å¾
                    st.write("### æ­¥éª¤1.6: é‡æ–°å½’ä¸€åŒ–å¢å¼ºç‰¹å¾ ğŸ”§")
                    st.info("ç‰¹å¾å·¥ç¨‹äº§ç”Ÿçš„æ–°ç‰¹å¾éœ€è¦é‡æ–°å½’ä¸€åŒ–...")
                    
                    # ç¬¬ä¸€æ­¥ï¼šä¸¥æ ¼æ¸…ç†å¼‚å¸¸å€¼
                    def deep_clean_data(X, name):
                        """æ·±åº¦æ¸…ç†æ•°æ®ä¸­çš„NaN/Infå’Œæç«¯å€¼"""
                        # æ£€æµ‹å¼‚å¸¸å€¼
                        nan_mask = np.isnan(X)
                        inf_mask = np.isinf(X)
                        
                        if nan_mask.any():
                            nan_count = nan_mask.sum()
                            st.warning(f"âš ï¸ {name} å‘ç° {nan_count} ä¸ªNaNï¼Œæ›¿æ¢ä¸º0")
                            X[nan_mask] = 0
                        
                        if inf_mask.any():
                            inf_count = inf_mask.sum()
                            st.warning(f"âš ï¸ {name} å‘ç° {inf_count} ä¸ªInfï¼Œæ›¿æ¢ä¸ºä¸­ä½æ•°")
                            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ä¸­ä½æ•°ï¼ˆå¿½ç•¥Infï¼‰
                            for feat_idx in range(X.shape[-1]):
                                feat_data = X[:, :, feat_idx]
                                feat_inf_mask = np.isinf(feat_data)
                                if feat_inf_mask.any():
                                    valid_data = feat_data[~feat_inf_mask]
                                    if len(valid_data) > 0:
                                        median_val = np.median(valid_data)
                                    else:
                                        median_val = 0
                                    feat_data[feat_inf_mask] = median_val
                                    X[:, :, feat_idx] = feat_data
                        
                        # è£å‰ªæç«¯å€¼ï¼ˆè¶…è¿‡99.9%åˆ†ä½æ•°çš„è§†ä¸ºå¼‚å¸¸ï¼‰
                        X_flat = X.reshape(-1)
                        q999 = np.percentile(X_flat, 99.9)
                        q001 = np.percentile(X_flat, 0.1)
                        
                        # æ‰©å±•è£å‰ªèŒƒå›´ä»¥ä¿æŒæ›´å¤šä¿¡æ¯
                        clip_max = max(q999, 1e3)
                        clip_min = min(q001, -1e3)
                        
                        extreme_mask = (X > clip_max) | (X < clip_min)
                        if extreme_mask.any():
                            extreme_count = extreme_mask.sum()
                            st.warning(f"âš ï¸ {name} å‘ç° {extreme_count} ä¸ªæç«¯å€¼ï¼Œè£å‰ªåˆ° [{clip_min:.2f}, {clip_max:.2f}]")
                            X = np.clip(X, clip_min, clip_max)
                        
                        return X
                    
                    # ä¿å­˜åŸå§‹æ•°æ®èŒƒå›´
                    before_min = X_train.min()
                    before_max = X_train.max()
                    
                    # æ·±åº¦æ¸…ç†
                    X_train = deep_clean_data(X_train, "è®­ç»ƒé›†")
                    X_val = deep_clean_data(X_val, "éªŒè¯é›†")
                    X_test = deep_clean_data(X_test, "æµ‹è¯•é›†")
                    
                    # é‡å¡‘æ•°æ®ä»¥é€‚åº”StandardScaler
                    samples_train, seq_len, features = X_train.shape
                    samples_val = X_val.shape[0]
                    samples_test = X_test.shape[0]
                    
                    X_train_flat = X_train.reshape(-1, features)
                    X_val_flat = X_val.reshape(-1, features)
                    X_test_flat = X_test.reshape(-1, features)
                    
                    # ä½¿ç”¨RobustScalerä»£æ›¿StandardScalerï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
                    from sklearn.preprocessing import RobustScaler
                    feature_scaler = RobustScaler()
                    
                    X_train_flat = feature_scaler.fit_transform(X_train_flat)
                    X_val_flat = feature_scaler.transform(X_val_flat)
                    X_test_flat = feature_scaler.transform(X_test_flat)
                    
                    # å†æ¬¡è£å‰ªåˆ°å®‰å…¨èŒƒå›´ï¼ˆRobustScaleråä»å¯èƒ½æœ‰æå€¼ï¼‰
                    X_train_flat = np.clip(X_train_flat, -10, 10)
                    X_val_flat = np.clip(X_val_flat, -10, 10)
                    X_test_flat = np.clip(X_test_flat, -10, 10)
                    
                    # æ¢å¤å½¢çŠ¶
                    X_train = X_train_flat.reshape(samples_train, seq_len, features)
                    X_val = X_val_flat.reshape(samples_val, seq_len, features)
                    X_test = X_test_flat.reshape(samples_test, seq_len, features)
                    
                    # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ²¡æœ‰NaN/Inf
                    assert not np.isnan(X_train).any(), "è®­ç»ƒé›†ä»å«NaNï¼"
                    assert not np.isinf(X_train).any(), "è®­ç»ƒé›†ä»å«Infï¼"
                    assert not np.isnan(X_val).any(), "éªŒè¯é›†ä»å«NaNï¼"
                    assert not np.isinf(X_val).any(), "éªŒè¯é›†ä»å«Infï¼"
                    
                    # æ˜¾ç¤ºå½’ä¸€åŒ–ä¿¡æ¯
                    after_min = X_train.min()
                    after_max = X_train.max()
                    after_mean = X_train.mean()
                    after_std = X_train.std()
                    
                    st.success(f"""
                    âœ… ç‰¹å¾æ·±åº¦æ¸…ç†+å½’ä¸€åŒ–å®Œæˆï¼
                    - å½’ä¸€åŒ–å‰èŒƒå›´: [{before_min:.4f}, {before_max:.4f}]
                    - å½’ä¸€åŒ–åèŒƒå›´: [{after_min:.4f}, {after_max:.4f}]
                    - å‡å€¼: {after_mean:.4f}, æ ‡å‡†å·®: {after_std:.4f}
                    - ä½¿ç”¨RobustScalerï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰
                    - å·²è£å‰ªåˆ°[-10, 10]å®‰å…¨èŒƒå›´ ğŸ›¡ï¸
                    """)
                
                # â­â­â­ è®­ç»ƒå‰æœ€ç»ˆæ£€æŸ¥
                st.write("### æ­¥éª¤1.9: è®­ç»ƒå‰æœ€ç»ˆæ£€æŸ¥ âœ…")
                
                final_feature_count = X_train.shape[-1]
                
                if final_feature_count > 30:
                    st.error(f"""
                    âŒâŒâŒ **ä¸¥é‡è­¦å‘Šï¼šæ£€æµ‹åˆ°{final_feature_count}ä¸ªç‰¹å¾ï¼**
                    
                    è¿™è¯´æ˜ç‰¹å¾å·¥ç¨‹è¢«å¯ç”¨äº†ï¼è¿™ä¼šå¯¼è‡´è®­ç»ƒå¤±è´¥ï¼
                    
                    **ç«‹å³æ“ä½œï¼š**
                    1. åœæ­¢å½“å‰æ“ä½œ
                    2. å–æ¶ˆå‹¾é€‰"å¯ç”¨ç‰¹å¾å·¥ç¨‹"
                    3. é‡æ–°åŠ è½½æ•°æ®ï¼ˆåº”è¯¥åªæœ‰25ä¸ªç‰¹å¾ï¼‰
                    4. å†æ¬¡å¼€å§‹è®­ç»ƒ
                    """)
                    st.stop()  # å¼ºåˆ¶åœæ­¢
                elif final_feature_count == 25:
                    st.success(f"""
                    âœ… **ç‰¹å¾æ•°é‡æ­£ç¡®ï¼š{final_feature_count}ä¸ªç‰¹å¾**
                    - 17ä¸ªçŸ¿å‹ç‰¹å¾
                    - 8ä¸ªåœ°è´¨ç‰¹å¾  
                    - ç‰¹å¾å·¥ç¨‹å·²ç¦ç”¨ï¼ˆæ­£ç¡®é…ç½®ï¼‰
                    - å¯ä»¥å®‰å…¨å¼€å§‹è®­ç»ƒï¼
                    """)
                else:
                    st.warning(f"""
                    âš ï¸ æ£€æµ‹åˆ°{final_feature_count}ä¸ªç‰¹å¾ï¼ˆé¢„æœŸ25ä¸ªï¼‰
                    - å¦‚æœ<25ï¼šå¯èƒ½ç¼ºå°‘åœ°è´¨ç‰¹å¾
                    - å¦‚æœ>25ï¼šå¯èƒ½å¯ç”¨äº†éƒ¨åˆ†ç‰¹å¾å·¥ç¨‹
                    - å»ºè®®é‡æ–°æ£€æŸ¥é…ç½®
                    """)
                
                # è·å–é‚»æ¥çŸ©é˜µ
                A_hat = adj_mx
                
                # 2. æ•°æ®å‡†å¤‡
                st.write("### æ­¥éª¤2: å‡†å¤‡GPUè®¡ç®—")
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                st.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
                
                # â­â­â­ å…³é”®ä¿®å¤ï¼šç®€å•å¥åº·æ£€æŸ¥ï¼ˆåªæ£€æµ‹NaN/Infï¼‰
                st.write("### æ­¥éª¤2.1: æ•°æ®å¥åº·æ£€æŸ¥ ğŸ”")
                
                def simple_data_health_check(X_data, y_data, name):
                    """ç®€å•ä½†æœ‰æ•ˆçš„æ•°æ®å¥åº·æ£€æŸ¥ - åªæ£€æŸ¥NaN/Inf"""
                    n_samples = len(X_data)
                    
                    # 1. åŸºç¡€æ£€æŸ¥NaNå’ŒInf
                    X_nan_count = np.isnan(X_data).sum()
                    X_inf_count = np.isinf(X_data).sum()
                    y_nan_count = np.isnan(y_data).sum()
                    y_inf_count = np.isinf(y_data).sum()
                    
                    total_issues = X_nan_count + X_inf_count + y_nan_count + y_inf_count
                    
                    if total_issues > 0:
                        st.error(f"""
                        âŒ **{name} åŒ…å«å¼‚å¸¸å€¼ï¼**
                        - Xä¸­NaN: {X_nan_count}, Inf: {X_inf_count}
                        - yä¸­NaN: {y_nan_count}, Inf: {y_inf_count}
                        - æ•°æ®å½¢çŠ¶: X={X_data.shape}, y={y_data.shape}
                        """)
                        
                        # å°è¯•æ¸…ç†
                        st.warning("å°è¯•æ¸…ç†NaN/Inf...")
                        
                        # æ‰¾å‡ºæœ‰é—®é¢˜çš„æ ·æœ¬ç´¢å¼•
                        X_has_issue = np.isnan(X_data).any(axis=(1,2)) | np.isinf(X_data).any(axis=(1,2))
                        y_has_issue = np.isnan(y_data) | np.isinf(y_data)
                        combined_issue = X_has_issue | y_has_issue
                        
                        # ç§»é™¤æœ‰é—®é¢˜çš„æ ·æœ¬
                        X_clean = X_data[~combined_issue]
                        y_clean = y_data[~combined_issue]
                        
                        removed_count = combined_issue.sum()
                        st.warning(f"ç§»é™¤äº† {removed_count} ä¸ªæœ‰NaN/Infçš„æ ·æœ¬")
                        
                        if len(X_clean) < 100:
                            st.error(f"æ¸…ç†åæ ·æœ¬å¤ªå°‘({len(X_clean)})ï¼Œæ— æ³•è®­ç»ƒï¼")
                            return None, None
                        
                        return X_clean, y_clean
                    
                    # 2. æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
                    X_min = np.min(X_data)
                    X_max = np.max(X_data)
                    X_mean = np.mean(X_data)
                    X_std = np.std(X_data)
                    
                    y_min = np.min(y_data)
                    y_max = np.max(y_data)
                    y_mean = np.mean(y_data)
                    
                    st.success(f"""
                    âœ… **{name} å¥åº·æ£€æŸ¥é€šè¿‡**
                    
                    **X (ç‰¹å¾):**
                    - å½¢çŠ¶: {X_data.shape}
                    - èŒƒå›´: [{X_min:.2f}, {X_max:.2f}]
                    - å‡å€¼: {X_mean:.2f}, æ ‡å‡†å·®: {X_std:.2f}
                    
                    **y (ç›®æ ‡):**
                    - å½¢çŠ¶: {y_data.shape}  
                    - èŒƒå›´: [{y_min:.2f}, {y_max:.2f}] MPa
                    - å‡å€¼: {y_mean:.2f} MPa
                    
                    âœ“ æ— NaNæˆ–Inf
                    âœ“ æ•°æ®å®Œæ•´
                    """)
                    
                    return X_data, y_data
                
                # ç®€å•å¥åº·æ£€æŸ¥ï¼ˆåªæ£€æŸ¥NaN/Infï¼‰
                X_train, y_train = simple_data_health_check(X_train, y_train, "è®­ç»ƒé›†")
                if X_train is None:
                    st.error("è®­ç»ƒé›†æœ‰ä¸¥é‡é—®é¢˜ï¼Œæ— æ³•ç»§ç»­ï¼")
                    st.stop()
                
                X_val, y_val = simple_data_health_check(X_val, y_val, "éªŒè¯é›†")
                if X_val is None:
                    st.error("éªŒè¯é›†æœ‰ä¸¥é‡é—®é¢˜ï¼Œæ— æ³•ç»§ç»­ï¼")
                    st.stop()
                
                X_test, y_test = simple_data_health_check(X_test, y_test, "æµ‹è¯•é›†")
                if X_test is None:
                    st.error("æµ‹è¯•é›†æœ‰ä¸¥é‡é—®é¢˜ï¼Œæ— æ³•ç»§ç»­ï¼")
                    st.stop()
                
                # â­ æ ¹æ®æ•°æ®æ ¼å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
                if use_spatial_reconstruction:
                    # å®Œæ•´æ—¶ç©ºæ•°æ®ï¼š(T, N, seq_len, F)
                    st.write("### æ­¥éª¤3: æ•°æ®å½’ä¸€åŒ–ï¼ˆæ—¶ç©ºæ ¼å¼ï¼‰")
                    st.info("ä½¿ç”¨å®Œæ•´æ—¶ç©ºæ•°æ®ï¼Œæ— éœ€é‡ç»„å›¾ç»“æ„")
                    
                    # â­ æ•°æ®éªŒè¯
                    if y_train.size == 0:
                        st.error("""
                        âŒ **è®­ç»ƒæ•°æ®ä¸ºç©ºï¼**
                        
                        è¿™é€šå¸¸å‘ç”Ÿåœ¨å®Œæ•´æ—¶ç©ºæ•°æ®é‡æ„æ—¶ï¼Œå¯èƒ½åŸå› ï¼š
                        1. æ•°æ®åˆ‡åˆ†åè®­ç»ƒé›†ä¸ºç©º
                        2. æ—¶ç©ºé‡æ„å¤±è´¥
                        
                        **è§£å†³æ–¹æ¡ˆï¼šè¯·åˆ‡æ¢åˆ°"å•æ ·æœ¬åºåˆ—æ ¼å¼"**
                        """)
                        st.stop()
                    
                    # ä¸ºäº†å½’ä¸€åŒ–ï¼Œéœ€è¦flatten
                    # y_train: (T, N) â†’ flatten to (T*N,)
                    y_train_flat = y_train.reshape(-1)
                    y_val_flat = y_val.reshape(-1)
                    
                    # â­ å†æ¬¡æ£€æŸ¥flattenåæ˜¯å¦ä¸ºç©º
                    if y_train_flat.size == 0:
                        st.error("è®­ç»ƒé›†ç›®æ ‡å€¼ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®")
                        st.stop()
                    
                    # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
                    y_mean = y_train_flat.mean()
                    y_std = y_train_flat.std()
                    y_min = y_train_flat.min()
                    y_max = y_train_flat.max()
                    y_range = y_max - y_min
                    if y_range < 1e-6:
                        y_range = 1.0
                    
                    # MinMaxå½’ä¸€åŒ–
                    y_train_normalized = (y_train - y_min) / y_range  # (T, N)
                    y_val_normalized = (y_val - y_min) / y_range
                    
                    # ç‰¹å¾å½’ä¸€åŒ–
                    X_train_normalized = X_train.copy()
                    X_val_normalized = X_val.copy()
                    
                    seq_len = X_train.shape[2]
                    num_features = X_train.shape[3]
                    
                    for feat_idx in range(num_features):
                        # å¯¹æ‰€æœ‰æ—¶é—´æ­¥å’Œæ‰€æœ‰æ”¯æ¶çš„è¯¥ç‰¹å¾å½’ä¸€åŒ–
                        feat_data = X_train[:, :, :, feat_idx]  # (T, N, seq_len)
                        feat_min = feat_data.min()
                        feat_max = feat_data.max()
                        feat_range = feat_max - feat_min
                        if feat_range < 1e-6:
                            feat_range = 1.0
                        
                        X_train_normalized[:, :, :, feat_idx] = (X_train[:, :, :, feat_idx] - feat_min) / feat_range
                        X_val_normalized[:, :, :, feat_idx] = (X_val[:, :, :, feat_idx] - feat_min) / feat_range
                    
                    support_to_idx = None
                    num_nodes = X_train.shape[1]  # N
                    
                else:
                    # å•æ”¯æ¶åºåˆ—æ•°æ®ï¼šåŸæœ‰é€»è¾‘
                    st.write("### æ­¥éª¤3: æ•°æ®å½’ä¸€åŒ–ï¼ˆå•æ”¯æ¶æ ¼å¼ï¼‰")
                    
                    # ä¸ºæ¯ä¸ªæ ·æœ¬æ‰¾åˆ°å¯¹åº”çš„supportç´¢å¼•
                    unique_supports_list = np.unique(support_ids)
                    support_to_idx = {sup_id: idx for idx, sup_id in enumerate(unique_supports_list)}
                    num_nodes = len(unique_supports_list)
                    
                    st.write(f"å›¾èŠ‚ç‚¹æ•°: {num_nodes}")
                    
                    # åŸæœ‰çš„å½’ä¸€åŒ–é€»è¾‘
                    y_mean = y_train.mean()
                    y_std = y_train.std()
                    y_min = y_train.min()
                    y_max = y_train.max()
                    y_range = y_max - y_min
                    if y_range < 1e-6:
                        y_range = 1.0
                    
                    y_train_normalized = (y_train - y_min) / y_range
                    y_val_normalized = (y_val - y_min) / y_range
                    
                    X_train_normalized = X_train.copy()
                    X_val_normalized = X_val.copy()
                    
                    seq_len = X_train.shape[1]
                    num_features = X_train.shape[2]
                    
                    for feat_idx in range(num_features):
                        feat_min = X_train[:, :, feat_idx].min()
                        feat_max = X_train[:, :, feat_idx].max()
                        feat_range = feat_max - feat_min
                        if feat_range < 1e-6:
                            feat_range = 1.0
                        
                        X_train_normalized[:, :, feat_idx] = (X_train[:, :, feat_idx] - feat_min) / feat_range
                        X_val_normalized[:, :, feat_idx] = (X_val[:, :, feat_idx] - feat_min) / feat_range
                
                # æ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
                st.info(f"""
                **ğŸ“Š ç›®æ ‡å˜é‡ç»Ÿè®¡åˆ†æï¼š**
                - å‡å€¼: {y_mean:.2f} MPa
                - æ ‡å‡†å·®: {y_std:.2f} MPa
                - èŒƒå›´: [{y_min:.2f}, {y_max:.2f}] MPa
                - å˜å¼‚ç³»æ•°(CV): {(y_std/y_mean)*100:.1f}%
                - æ ·æœ¬æ•°: {len(y_train_normalized.flatten()):,}
                
                **ğŸ’¡ å¯é¢„æµ‹æ€§åˆ†æï¼š**
                - CV < 30%: æ•°æ®å˜åŒ–è¾ƒå°ï¼Œè¾ƒéš¾é¢„æµ‹
                - CV 30-50%: ä¸­ç­‰å˜åŒ–ï¼Œé€‚åˆé¢„æµ‹
                - CV > 50%: å˜åŒ–å¤§ï¼Œæ¨¡å¼æ˜æ˜¾
                
                å½“å‰ CV={(y_std/y_mean)*100:.1f}% ({'åä½ï¼Œé¢„æµ‹éš¾åº¦å¤§' if (y_std/y_mean) < 0.3 else 'ä¸­ç­‰' if (y_std/y_mean) < 0.5 else 'è¾ƒé«˜ï¼Œæœ‰åˆ©äºé¢„æµ‹'})
                """)
                
                # æ ¹æ®æ¨¡å‹ç±»å‹å‡†å¤‡æ•°æ®
                st.write("### æ­¥éª¤4: å‡†å¤‡æ¨¡å‹è¾“å…¥æ•°æ®")
                
                if use_spatial_reconstruction:
                    # æ—¶ç©ºæ•°æ®æ ¼å¼å·²ç»æ˜¯å®Œæ•´çš„
                    # (T, N, seq_len, F) â†’ STGCNéœ€è¦ (T, F, N, seq_len)
                    if "LSTM" in model_type or "Transformer" in model_type:
                        # LSTM/Transformer: éœ€è¦flattenç©ºé—´ç»´åº¦ï¼Œæˆ–é€‰æ‹©ç‰¹å®šæ”¯æ¶
                        # è¿™é‡Œæˆ‘ä»¬flattenæ‰€æœ‰æ”¯æ¶ï¼Œå°†å…¶è§†ä¸ºç‹¬ç«‹æ ·æœ¬
                        T, N, seq_len, F = X_train_normalized.shape
                        X_train_flat = X_train_normalized.reshape(T * N, seq_len, F)
                        y_train_flat = y_train_normalized.reshape(T * N, 1)
                        
                        X_val_flat = X_val_normalized.reshape(-1, seq_len, F)
                        y_val_flat = y_val_normalized.reshape(-1, 1)
                        
                        train_X_tensor = torch.FloatTensor(X_train_flat)
                        train_y_tensor = torch.FloatTensor(y_train_flat)
                        val_X_tensor = torch.FloatTensor(X_val_flat)
                        val_y_tensor = torch.FloatTensor(y_val_flat)
                        
                        model_type_short = "LSTM/Transformer" if "Transformer" in model_type else "LSTM"
                        st.write(f"{model_type_short}æ¨¡å¼ - è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                        st.write(f"{model_type_short}æ¨¡å¼ - éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                        
                    else:
                        # STGCN: è½¬æ¢ç»´åº¦ (T, N, seq_len, F) â†’ (T, F, N, seq_len)
                        X_train_stgcn = np.transpose(X_train_normalized, (0, 3, 1, 2))
                        X_val_stgcn = np.transpose(X_val_normalized, (0, 3, 1, 2))
                        
                        # y: (T, N) â†’ (T, 1, N, 1)
                        y_train_stgcn = y_train_normalized[:, np.newaxis, :, np.newaxis]
                        y_val_stgcn = y_val_normalized[:, np.newaxis, :, np.newaxis]
                        
                        train_X_tensor = torch.FloatTensor(X_train_stgcn)
                        train_y_tensor = torch.FloatTensor(y_train_stgcn)
                        val_X_tensor = torch.FloatTensor(X_val_stgcn)
                        val_y_tensor = torch.FloatTensor(y_val_stgcn)
                        A_hat_tensor = torch.FloatTensor(A_hat).to(device)
                        
                        st.write(f"STGCNæ¨¡å¼ - è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                        st.write(f"STGCNæ¨¡å¼ - éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                
                else:
                    # å•æ”¯æ¶åºåˆ—æ ¼å¼ï¼šåŸæœ‰é€»è¾‘
                    if "STGCN" in model_type:
                        # âš ï¸ å•æ ·æœ¬æ ¼å¼ä¸æ”¯æŒSTGCN
                        st.error("""
                        âŒ **å•æ ·æœ¬åºåˆ—æ ¼å¼ä¸æ”¯æŒSTGCNæ¨¡å‹ï¼**
                        
                        **åŸå› ï¼š**
                        - STGCNéœ€è¦å®Œæ•´çš„ç©ºé—´æ‹“æ‰‘ç»“æ„ï¼ˆæ‰€æœ‰125ä¸ªæ”¯æ¶åŒæ—¶å­˜åœ¨ï¼‰
                        - å•æ ·æœ¬æ ¼å¼æ¯ä¸ªæ ·æœ¬åªåŒ…å«1ä¸ªæ”¯æ¶çš„æ•°æ®
                        - å¼ºè¡Œè½¬æ¢ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºï¼ˆéœ€è¦111GB+ï¼‰
                        
                        **è§£å†³æ–¹æ¡ˆï¼ˆ3é€‰1ï¼‰ï¼š**
                        
                        1ï¸âƒ£ **æ¨èï¼šåˆ‡æ¢åˆ°Transformeræ¨¡å‹** â­â­â­
                           - ä¿æŒ"å•æ ·æœ¬åºåˆ—æ ¼å¼"
                           - é€‰æ‹©"Transformer (æœ€å¼ºè¡¨è¾¾åŠ›)ğŸš€"
                           - é¢„æœŸRÂ²: 0.65-0.80
                        
                        2ï¸âƒ£ **åˆ‡æ¢åˆ°AttentionLSTMæ¨¡å‹** â­â­
                           - ä¿æŒ"å•æ ·æœ¬åºåˆ—æ ¼å¼"
                           - é€‰æ‹©"AttentionLSTM (æ³¨æ„åŠ›å¢å¼º)â­"
                           - é¢„æœŸRÂ²: 0.45-0.55
                        
                        3ï¸âƒ£ **åˆ‡æ¢åˆ°å®Œæ•´æ—¶ç©ºæ•°æ®æ ¼å¼**
                           - é€‰æ‹©"å®Œæ•´æ—¶ç©ºæ•°æ®ï¼ˆæ¨èï¼Œé¢„æœŸRÂ²>0.5ï¼‰"
                           - ç„¶åå¯ä»¥ä½¿ç”¨STGCN
                           - âš ï¸ ä½†æ ·æœ¬æ•°ä¼šå¤§å¹…å‡å°‘ï¼ˆå¯èƒ½<100ï¼‰
                        
                        **å½“å‰æœ€ä¼˜é€‰æ‹©ï¼šæ–¹æ¡ˆ1ï¼ˆå•æ ·æœ¬+Transformerï¼‰**
                        """)
                        st.stop()
                    
                    else:
                        # LSTM/AttentionLSTM/Transformer: ç›´æ¥ä½¿ç”¨åºåˆ—æ•°æ®
                        train_X_tensor = torch.FloatTensor(X_train_normalized)
                        train_y_tensor = torch.FloatTensor(y_train_normalized).view(-1, 1)
                        val_X_tensor = torch.FloatTensor(X_val_normalized)
                        val_y_tensor = torch.FloatTensor(y_val_normalized).view(-1, 1)
                        
                        st.write(f"è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                        st.write(f"éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                        st.write(f"y å½’ä¸€åŒ–èŒƒå›´: [{train_y_tensor.min():.4f}, {train_y_tensor.max():.4f}]")
                
                # åˆå§‹åŒ–æ¨¡å‹
                if use_spatial_reconstruction and "STGCN" in model_type:
                    seq_len = X_train_normalized.shape[2]
                    num_features = X_train_normalized.shape[3]
                else:
                    seq_len = X_train_normalized.shape[1 if not use_spatial_reconstruction else 2]
                    num_features = X_train_normalized.shape[2 if not use_spatial_reconstruction else 3]
                
                pred_len = 1
                
                if "LSTM (åŸºç¡€ç‰ˆ)" in model_type:
                    model = SimpleLSTM(
                        num_features=num_features,
                        hidden_dim=hidden_dim * 2,  # LSTM ç”¨æ›´å¤§çš„éšè—å±‚
                        num_layers=2
                    ).to(device)
                elif "AttentionLSTM" in model_type:
                    model = AttentionLSTM(
                        num_features=num_features,
                        hidden_dim=hidden_dim * 2,  # æ³¨æ„åŠ›LSTMç”¨æ›´å¤§çš„éšè—å±‚
                        num_layers=2
                    ).to(device)
                    st.success("âœ¨ ä½¿ç”¨æ³¨æ„åŠ›å¢å¼ºLSTMï¼Œé¢„æœŸæå‡5-15%")
                elif "Transformer" in model_type:
                    # â­ Transformeræ¨¡å‹ - æœ€å¼ºè¡¨è¾¾èƒ½åŠ›
                    model = TransformerPredictor(
                        num_features=num_features,
                        d_model=hidden_dim,  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„hidden_dim
                        nhead=8,
                        num_encoder_layers=3,
                        dim_feedforward=hidden_dim * 4
                    ).to(device)
                    st.success("ğŸš€ ä½¿ç”¨Transformeræ¨¡å‹ï¼Œæœ€å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œé¢„æœŸRÂ²â‰¥0.8")
                else:
                    # STGCNæ¨¡å‹
                    # è·å–å›¾ç»“æ„å‚æ•°
                    adj_params = {}
                    if adj_method == 'adaptive':
                        adj_params = {'threshold': adj_threshold, 'sigma': adj_sigma}
                    elif adj_method == 'grid':
                        adj_params = {'rows': adj_rows}
                    elif adj_method == 'knn':
                        adj_params = {'k': adj_k}
                    
                    model = STGCN(
                        num_nodes=num_nodes,
                        num_features=num_features,
                        seq_len=seq_len,
                        pred_len=pred_len,
                        hidden_dim=hidden_dim,  # ä¼ å…¥hidden_dimå‚æ•°
                        Kt=3
                    ).to(device)
                
                st.write(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
                
                # æ˜¾ç¤ºæ•°æ®å’Œæ¨¡å‹ä¿¡æ¯
                model_name = "SimpleLSTM" if "LSTM (åŸºç¡€ç‰ˆ)" in model_type else \
                             "AttentionLSTM" if "AttentionLSTM" in model_type else \
                             "Transformer" if "Transformer" in model_type else "STGCN"
                st.info(f"""
                **{model_name} æ¨¡å‹é…ç½®ï¼š**
                - è¾“å…¥ç»´åº¦: {train_X_tensor.shape}
                - è¾“å‡ºç»´åº¦: {train_y_tensor.shape}
                - ç‰¹å¾æ•°: {num_features}
                - åºåˆ—é•¿åº¦: {seq_len}
                - éšè—å±‚: {hidden_dim * 2 if 'LSTM' in model_type else hidden_dim}
                {'- Transformerå±‚æ•°: 3, æ³¨æ„åŠ›å¤´æ•°: 8' if 'Transformer' in model_type else ''}
                {'- å›¾ç»“æ„: ' + adj_method if 'STGCN' in model_type else ''}
                """)
                
                # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
                # â­ æ ¹æ®ä¼˜åŒ–è®¾ç½®é€‰æ‹©æŸå¤±å‡½æ•°
                if use_combined_loss:
                    # ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°
                    st.info("âœ… ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°ï¼ˆHuber + MAE + MAPEï¼‰")
                    criterion = lambda pred, target: combined_loss(pred, target, alpha=0.5, beta=0.3, gamma=0.2)
                else:
                    # ä½¿ç”¨æ ‡å‡†Huber Loss
                    criterion = nn.SmoothL1Loss()
                    st.info("âœ… ä½¿ç”¨Huber Lossï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰")
                
                # æ·»åŠ L2æ­£åˆ™åŒ–(weight_decay)æ¥é˜²æ­¢è¿‡æ‹Ÿåˆ
                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
                
                # â­ æ··åˆç²¾åº¦è®­ç»ƒé…ç½®
                if use_amp and device.type == 'cuda':
                    scaler = torch.cuda.amp.GradScaler()
                    st.success("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨ï¼ˆè®­ç»ƒé€Ÿåº¦æå‡2-3å€ï¼‰")
                else:
                    scaler = None
                    if use_amp and device.type == 'cpu':
                        st.warning("âš ï¸ CPUæ¨¡å¼ä¸æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
                
                # â­ å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨
                def get_lr_scheduler(optimizer, warmup_epochs=10):
                    """
                    å­¦ä¹ ç‡é¢„çƒ­+ä½™å¼¦é€€ç«
                    â­ å¢åŠ warmupæ—¶é•¿ä»¥é€‚åº”264ä¸ªç‰¹å¾
                    """
                    from torch.optim.lr_scheduler import LambdaLR
                    import math
                    
                    def lr_lambda(epoch):
                        if epoch < warmup_epochs:
                            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿ï¼ˆä»0.1å€å¼€å§‹ï¼‰
                            return 0.1 + 0.9 * (epoch + 1) / warmup_epochs
                        else:
                            # ä½™å¼¦é€€ç«
                            progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)
                            return 0.5 * (1 + math.cos(math.pi * progress))
                    
                    return LambdaLR(optimizer, lr_lambda)
                
                scheduler = get_lr_scheduler(optimizer, warmup_epochs=10)
                st.info("âœ… ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­(10è½®)+ä½™å¼¦é€€ç«ç­–ç•¥ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
                
                # â­ æ¢¯åº¦ç´¯ç§¯é…ç½®
                if use_gradient_accumulation:
                    accumulation_steps = 4  # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = batch_size * accumulation_steps
                    st.info(f"âœ… ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œæœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {batch_size * accumulation_steps}")
                else:
                    accumulation_steps = 1
                
                # æ—©åœå‚æ•° - æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´patience
                early_stop_patience = 30 if "STGCN" in model_type else 25
                early_stop_counter = 0
                
                # è®­ç»ƒå¾ªç¯
                progress_bar = st.progress(0)
                status_text = st.empty()
                metrics_placeholder = st.empty()
                
                # å­˜å‚¨æŸå¤±å†å²
                train_losses = []
                val_losses = []
                best_val_loss = float('inf')
                
                # åˆ›å»ºDataLoader (è®­ç»ƒå’ŒéªŒè¯éƒ½ä½¿ç”¨æ‰¹å¤„ç†)
                from torch.utils.data import TensorDataset, DataLoader
                train_dataset = TensorDataset(train_X_tensor, train_y_tensor)
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                
                val_dataset = TensorDataset(val_X_tensor, val_y_tensor)
                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                
                start_time = time.time()
                
                for epoch in range(epochs):
                    # è®­ç»ƒé˜¶æ®µ
                    model.train()
                    epoch_train_loss = 0
                    batch_count = 0
                    
                    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):
                        # å°†æ•°æ®ç§»åˆ°GPU
                        batch_X = batch_X.to(device)
                        batch_y = batch_y.to(device)
                        
                        # â­â­â­ å…³é”®æ£€æŸ¥ï¼šè¾“å…¥æ•°æ®éªŒè¯
                        if torch.isnan(batch_X).any() or torch.isinf(batch_X).any():
                            st.error(f"""
                            âŒ æ£€æµ‹åˆ°è¾“å…¥æ•°æ®å¼‚å¸¸ï¼
                            - Epoch: {epoch+1}, Batch: {batch_idx+1}
                            - NaNæ•°é‡: {torch.isnan(batch_X).sum().item()}
                            - Infæ•°é‡: {torch.isinf(batch_X).sum().item()}
                            - è¿™ä¸åº”è¯¥å‘ç”Ÿï¼æ•°æ®é¢„å¤„ç†æœ‰bugï¼
                            """)
                            st.stop()
                        
                        # â­ æ··åˆç²¾åº¦è®­ç»ƒ
                        if scaler is not None:
                            with torch.cuda.amp.autocast():
                                # å‰å‘ä¼ æ’­
                                if "STGCN" in model_type:
                                    outputs = model(batch_X, A_hat_tensor)
                                    loss = criterion(outputs, batch_y)
                                else:
                                    outputs = model(batch_X)
                                    loss = criterion(outputs, batch_y)
                            
                            # â­ æ¢¯åº¦ç´¯ç§¯ï¼šå½’ä¸€åŒ–æŸå¤±
                            loss = loss / accumulation_steps
                            
                            # åå‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
                            scaler.scale(loss).backward()
                            
                            # â­ æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
                            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
                                # â­ æ›´ä¿å®ˆçš„æ¢¯åº¦è£å‰ªï¼ˆé™ä½åˆ°0.5ï¼‰
                                scaler.unscale_(optimizer)
                                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                                
                                # æ›´æ–°
                                scaler.step(optimizer)
                                scaler.update()
                                optimizer.zero_grad()
                        else:
                            # æ ‡å‡†è®­ç»ƒ
                            if "STGCN" in model_type:
                                outputs = model(batch_X, A_hat_tensor)
                                loss = criterion(outputs, batch_y)
                            else:
                                outputs = model(batch_X)
                                loss = criterion(outputs, batch_y)
                            
                            # â­ æ¢¯åº¦ç´¯ç§¯ï¼šå½’ä¸€åŒ–æŸå¤±
                            loss = loss / accumulation_steps
                            
                            # åå‘ä¼ æ’­
                            loss.backward()
                            
                            # â­ æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
                            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
                                # â­ æ›´ä¿å®ˆçš„æ¢¯åº¦è£å‰ªï¼ˆé™ä½åˆ°0.5ï¼‰
                                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                                
                                optimizer.step()
                                optimizer.zero_grad()
                        
                        # è®°å½•æœªå½’ä¸€åŒ–çš„æŸå¤±ç”¨äºæ˜¾ç¤º
                        current_loss = loss.item() * accumulation_steps
                        
                        # â­ å…³é”®ä¿®å¤ï¼šæ£€æµ‹NaNï¼Œç«‹å³åœæ­¢è®­ç»ƒ
                        if np.isnan(current_loss) or np.isinf(current_loss):
                            st.error(f"""
                            âŒ **è®­ç»ƒå¤±è´¥ï¼šæ£€æµ‹åˆ°NaN/InfæŸå¤±ï¼**
                            
                            **é—®é¢˜ï¼š** åœ¨epoch {epoch+1}, batch {batch_idx+1}å‡ºç°æ•°å€¼å¼‚å¸¸
                            - å½“å‰æŸå¤±: {current_loss}
                            
                            **å¯èƒ½åŸå› ï¼š**
                            1. å­¦ä¹ ç‡è¿‡å¤§ (å½“å‰: {optimizer.param_groups[0]['lr']:.6f})
                            2. ç‰¹å¾æ•°å€¼èŒƒå›´è¿‡å¤§ï¼ˆ264ä¸ªç‰¹å¾å¯èƒ½å­˜åœ¨æœªå½’ä¸€åŒ–çš„ï¼‰
                            3. æ¢¯åº¦çˆ†ç‚¸
                            
                            **å»ºè®®ï¼š**
                            1. é™ä½å­¦ä¹ ç‡ï¼ˆ0.001 â†’ 0.0001ï¼‰
                            2. æ£€æŸ¥åœ°è´¨ç‰¹å¾æ˜¯å¦æ­£ç¡®å½’ä¸€åŒ–
                            3. ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°
                            """)
                            st.stop()
                        
                        epoch_train_loss += current_loss
                        batch_count += 1
                    
                    avg_train_loss = epoch_train_loss / batch_count
                    
                    # å†æ¬¡æ£€æŸ¥å¹³å‡æŸå¤±
                    if np.isnan(avg_train_loss) or np.isinf(avg_train_loss):
                        st.error(f"âŒ è®­ç»ƒæŸå¤±å¼‚å¸¸: {avg_train_loss}")
                        st.stop()
                    
                    train_losses.append(avg_train_loss)
                    
                    # éªŒè¯é˜¶æ®µ (ä½¿ç”¨æ‰¹å¤„ç†é¿å…æ˜¾å­˜æº¢å‡º)
                    model.eval()
                    val_loss_sum = 0
                    all_preds = []
                    all_targets = []
                    val_batch_count = 0
                    
                    with torch.no_grad():
                        for val_batch_X, val_batch_y in val_loader:
                            val_batch_X = val_batch_X.to(device)
                            val_batch_y = val_batch_y.to(device)
                            
                            if "STGCN" in model_type:
                                val_batch_outputs = model(val_batch_X, A_hat_tensor)
                            else:
                                # LSTM/AttentionLSTM/Transformer
                                val_batch_outputs = model(val_batch_X)
                            
                            # ç´¯ç§¯æŸå¤±ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰- ä¸clampï¼Œä½¿ç”¨åŸå§‹è¾“å‡º
                            batch_loss = criterion(val_batch_outputs, val_batch_y).item()
                            val_loss_sum += batch_loss * len(val_batch_X)
                            
                            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºåç»­è®¡ç®—
                            all_preds.append(val_batch_outputs.cpu())
                            all_targets.append(val_batch_y.cpu())
                            
                            val_batch_count += len(val_batch_X)
                            
                            # æ¸…ç†æ˜¾å­˜
                            del val_batch_X, val_batch_y, val_batch_outputs
                            torch.cuda.empty_cache()
                        
                        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
                        all_preds = torch.cat(all_preds, dim=0)  # (N, ...)
                        all_targets = torch.cat(all_targets, dim=0)  # (N, ...)
                        
                        # å±•å¹³ä¸ºä¸€ç»´å‘é‡ç”¨äºè®¡ç®—æŒ‡æ ‡
                        if "STGCN" in model_type:
                            # STGCN: éœ€è¦æå–éé›¶å€¼
                            # all_preds: (B, 1, N, 1)
                            # all_targets: (B, 1, N, 1)
                            # å‹ç¼©åˆ° (B, N) - æ³¨æ„é¡ºåºå’Œç»´åº¦
                            all_preds_2d = all_preds.squeeze(3).squeeze(1)  # (B, 1, N, 1) -> (B, 1, N) -> (B, N)
                            all_targets_2d = all_targets.squeeze(3).squeeze(1)  # (B, 1, N, 1) -> (B, 1, N) -> (B, N)
                            
                            # åˆ›å»ºmaskæ‰¾å‡ºéé›¶èŠ‚ç‚¹
                            mask = all_targets_2d != 0  # (B, N)
                            
                            # æå–éé›¶å€¼å¹¶å±•å¹³
                            all_preds_flat = all_preds_2d[mask]  # (num_nonzero,)
                            all_targets_flat = all_targets_2d[mask]  # (num_nonzero,)
                            
                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€ä¸ªepochæ˜¾ç¤ºï¼‰
                            if epoch == 0 or (epoch + 1) % 20 == 0:
                                num_nonzero = mask.sum().item()
                                st.write(f"ğŸ“Š STGCNè°ƒè¯•: æå–äº† {num_nonzero} ä¸ªéé›¶èŠ‚ç‚¹é¢„æµ‹å€¼")
                                st.write(f"ğŸ“Š åŸå§‹è¾“å‡ºèŒƒå›´: [{all_preds_flat.min():.4f}, {all_preds_flat.max():.4f}]")
                        else:
                            # LSTM/AttentionLSTM/Transformer
                            all_preds_flat = all_preds.squeeze()  # (N,)
                            all_targets_flat = all_targets.squeeze()  # (N,)
                        
                        # è£å‰ªå½’ä¸€åŒ–é¢„æµ‹å€¼åˆ°[0,1]èŒƒå›´ï¼Œä»…ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡
                        # æ³¨æ„ï¼šè¿™ä¸ä¼šå½±å“è®­ç»ƒï¼Œåªå½±å“æ˜¾ç¤ºçš„æŒ‡æ ‡
                        all_preds_flat_clamped = torch.clamp(all_preds_flat, 0.0, 1.0)
                        
                        # æ·»åŠ å½’ä¸€åŒ–ç©ºé—´çš„è°ƒè¯•ä¿¡æ¯ - å‰5ä¸ªepochæ¯æ¬¡éƒ½æ˜¾ç¤º
                        if epoch < 5 or epoch == 0 or (epoch + 1) % 20 == 0:
                            st.write(f"ğŸ“Š å½’ä¸€åŒ–ç©ºé—´(clampå) - é¢„æµ‹å€¼èŒƒå›´: [{all_preds_flat_clamped.min():.4f}, {all_preds_flat_clamped.max():.4f}]")
                            st.write(f"ğŸ“Š å½’ä¸€åŒ–ç©ºé—´ - çœŸå®å€¼èŒƒå›´: [{all_targets_flat.min():.4f}, {all_targets_flat.max():.4f}]")
                            st.write(f"ğŸ“Š å½’ä¸€åŒ–ç©ºé—´ - MSE: {torch.mean((all_preds_flat_clamped - all_targets_flat)**2).item():.6f}")
                        
                        # åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦ (MinMax åå˜æ¢)
                        all_preds_original = all_preds_flat_clamped * y_range + y_min
                        all_targets_original = all_targets_flat * y_range + y_min
                        
                        # è®¡ç®—åŸå§‹å°ºåº¦çš„æŒ‡æ ‡
                        mae = torch.mean(torch.abs(all_preds_original - all_targets_original)).item()
                        rmse = torch.sqrt(torch.mean((all_preds_original - all_targets_original)**2)).item()
                        
                        # RÂ² (åœ¨åŸå§‹å°ºåº¦è®¡ç®—) - ä½¿ç”¨æ›´ç¨³å¥çš„æ–¹å¼
                        y_mean_original = torch.mean(all_targets_original)
                        ss_tot = torch.sum((all_targets_original - y_mean_original)**2).item()
                        ss_res = torch.sum((all_targets_original - all_preds_original)**2).item()
                        
                        # æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯ - å‰5ä¸ªepochæ¯æ¬¡éƒ½æ˜¾ç¤º
                        if epoch < 5 or epoch == 0 or (epoch + 1) % 20 == 0:
                            st.write(f"ğŸ“Š åŸå§‹å°ºåº¦ - é¢„æµ‹å€¼èŒƒå›´: [{all_preds_original.min():.2f}, {all_preds_original.max():.2f}] MPa")
                            st.write(f"ğŸ“Š åŸå§‹å°ºåº¦ - çœŸå®å€¼èŒƒå›´: [{all_targets_original.min():.2f}, {all_targets_original.max():.2f}] MPa")
                            st.write(f"ğŸ“Š çœŸå®å€¼å‡å€¼: {y_mean_original:.2f} MPa")
                            st.write(f"ğŸ“Š ss_tot={ss_tot:.2f}, ss_res={ss_res:.2f}, æ¯”ä¾‹={ss_res/ss_tot:.2f}")
                            st.write(f"ğŸ“Š åŸå§‹RÂ²å€¼(æœªè£å‰ª): {1 - ss_res/ss_tot:.4f}")
                        
                        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œåˆç†æ€§çº¦æŸ
                        if ss_tot < 1e-6:
                            # ç›®æ ‡æ–¹å·®å¤ªå°ï¼ŒRÂ²æ— æ„ä¹‰
                            r2 = 0.0
                        else:
                            r2_raw = 1 - ss_res / ss_tot
                            # å°†RÂ²é™åˆ¶åœ¨åˆç†èŒƒå›´ [-1, 1]ï¼Œé¿å…æ•°å€¼å¼‚å¸¸
                            if r2_raw < -1.0:
                                r2 = -1.0  # é¢„æµ‹éå¸¸å·®ï¼Œä½†ä¸è‡³äºå´©æºƒ
                            elif r2_raw > 1.0:
                                r2 = 1.0  # ä¸å¯èƒ½è¶…è¿‡1
                            else:
                                r2 = r2_raw
                        
                        # è®¡ç®—å¹³å‡æŸå¤±
                        val_loss = val_loss_sum / val_batch_count
                        val_losses.append(val_loss)
                        
                        # æ·»åŠ é¢„æµ‹å€¼èŒƒå›´ç›‘æ§ï¼ˆåŸå§‹å°ºåº¦ï¼‰
                        pred_min = all_preds_original.min().item()
                        pred_max = all_preds_original.max().item()
                        target_min = all_targets_original.min().item()
                        target_max = all_targets_original.max().item()
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        # â­ ä¿å­˜å®Œæ•´checkpointï¼ŒåŒ…å«æ¨¡å‹ç»“æ„ä¿¡æ¯
                        checkpoint = {
                            'model_state_dict': model.state_dict(),
                            'model_type': model_type,
                            'input_dim': train_X_tensor.shape[-1],  # å½“å‰ç‰¹å¾æ•°
                            'hidden_dim': hidden_dim,
                            'epoch': epoch,
                            'best_val_loss': best_val_loss,
                            'feature_names': feature_names
                        }
                        torch.save(checkpoint, 'best_stgcn_model.pth')
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                    
                    # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ¯ä¸ªepochè°ƒç”¨ï¼Œä¸éœ€è¦ä¼ å…¥val_lossï¼‰
                    scheduler.step()
                    current_lr = optimizer.param_groups[0]['lr']
                    
                    # æ—©åœæ£€æŸ¥
                    if early_stop_counter >= early_stop_patience:
                        st.warning(f"âš ï¸ éªŒè¯æŸå¤±è¿ç»­ {early_stop_patience} è½®æœªæ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                        break
                    
                    # æ›´æ–°è¿›åº¦
                    progress = (epoch + 1) / epochs
                    progress_bar.progress(progress)
                    
                    elapsed = time.time() - start_time
                    eta = elapsed / (epoch + 1) * (epochs - epoch - 1)
                    
                    status_text.text(
                        f"Epoch {epoch+1}/{epochs} | "
                        f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | "
                        f"éªŒè¯æŸå¤±: {val_loss:.4f} | "
                        f"RÂ²: {r2:.4f} | "
                        f"å­¦ä¹ ç‡: {current_lr:.6f} | "
                        f"å·²ç”¨æ—¶: {elapsed:.1f}s | ETA: {eta:.1f}s"
                    )
                    
                    # æ¯10ä¸ªepochæ›´æ–°ä¸€æ¬¡æŒ‡æ ‡
                    if (epoch + 1) % 10 == 0 or epoch == 0:
                        metrics_placeholder.markdown(f"""
                        ### å½“å‰æŒ‡æ ‡
                        - **è®­ç»ƒæŸå¤±**: {avg_train_loss:.6f}
                        - **éªŒè¯æŸå¤±**: {val_loss:.6f}
                        - **MAE**: {mae:.4f} MPa
                        - **RMSE**: {rmse:.4f} MPa
                        - **RÂ²**: {r2:.4f}
                        - **é¢„æµ‹èŒƒå›´**: [{pred_min:.2f}, {pred_max:.2f}] MPa
                        - **çœŸå®èŒƒå›´**: [{target_min:.2f}, {target_max:.2f}] MPa
                        - **ss_tot**: {ss_tot:.2f}, **ss_res**: {ss_res:.2f}
                        """)
                
                # è®­ç»ƒå®Œæˆ
                st.success("âœ… è®­ç»ƒå®Œæˆï¼")
                st.balloons()
                
                # ç»˜åˆ¶æŸå¤±æ›²çº¿
                st.subheader("ğŸ“ˆ è®­ç»ƒå†å²")
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
                ax.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss (MSE)')
                ax.set_title('è®­ç»ƒè¿‡ç¨‹')
                ax.legend()
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                
                # æœ€ç»ˆè¯„ä¼°
                st.subheader("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æœ€ä½³éªŒè¯æŸå¤±", f"{best_val_loss:.6f}")
                with col2:
                    st.metric("MAE", f"{mae:.4f} MPa")
                with col3:
                    st.metric("RMSE", f"{rmse:.4f} MPa")
                with col4:
                    st.metric("RÂ²", f"{r2:.4f}")
                
                st.info("æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: best_stgcn_model.pth")
                
                # é¢„æµ‹ç¤ºä¾‹ (ä½¿ç”¨å°æ‰¹æ¬¡é¿å…æ˜¾å­˜æº¢å‡º)
                st.subheader("ğŸ”® é¢„æµ‹ç¤ºä¾‹")
                
                # â­ æ™ºèƒ½åŠ è½½æœ€ä½³æ¨¡å‹
                try:
                    checkpoint = torch.load('best_stgcn_model.pth')
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼çš„checkpoint
                    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                        # æ–°æ ¼å¼ï¼šæ£€æŸ¥å…¼å®¹æ€§
                        saved_input_dim = checkpoint.get('input_dim', 0)
                        current_input_dim = train_X_tensor.shape[-1]
                        
                        if saved_input_dim == current_input_dim:
                            model.load_state_dict(checkpoint['model_state_dict'])
                            st.success(f"âœ… æˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆepoch {checkpoint.get('epoch', '?')}ï¼‰")
                        else:
                            st.warning(f"""
                            âš ï¸ æ¨¡å‹ç‰¹å¾æ•°ä¸åŒ¹é…
                            - ä¿å­˜çš„æ¨¡å‹: {saved_input_dim}ä¸ªç‰¹å¾
                            - å½“å‰æ¨¡å‹: {current_input_dim}ä¸ªç‰¹å¾
                            
                            ä½¿ç”¨å½“å‰è®­ç»ƒå®Œæˆçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
                            """)
                    else:
                        # æ—§æ ¼å¼ï¼šç›´æ¥å°è¯•åŠ è½½
                        model.load_state_dict(checkpoint)
                        st.info("åŠ è½½äº†æ—§æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶")
                
                except FileNotFoundError:
                    st.info("æœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹")
                except RuntimeError as e:
                    st.warning(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆç»“æ„ä¸åŒ¹é…ï¼‰ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œé¢„æµ‹")
                    st.caption(f"é”™è¯¯è¯¦æƒ…: {str(e)[:150]}...")
                
                model.eval()
                
                # éšæœºé€‰æ‹©å‡ ä¸ªéªŒè¯æ ·æœ¬
                num_examples = min(5, len(val_X_tensor))
                indices = np.random.choice(len(val_X_tensor), num_examples, replace=False)
                
                with torch.no_grad():
                    example_X = val_X_tensor[indices].to(device)
                    example_y_true = val_y_tensor[indices].to(device)
                    
                    if "STGCN" in model_type:
                        example_y_pred = model(example_X, A_hat_tensor)
                        # è£å‰ªåˆ°[0,1]èŒƒå›´
                        example_y_pred = torch.clamp(example_y_pred, 0.0, 1.0)
                    else:
                        # LSTM/AttentionLSTM/Transformer
                        example_y_pred = model(example_X)  # (B, 1)
                        # è£å‰ªåˆ°[0,1]èŒƒå›´
                        example_y_pred = torch.clamp(example_y_pred, 0.0, 1.0)
                
                # åˆ›å»ºå¯¹æ¯”è¡¨
                comparison_data = []
                for i, idx in enumerate(indices):
                    sup_id = val_support_ids[idx]
                    
                    if "STGCN" in model_type:
                        # STGCN: ä»å›¾ç»“æ„ä¸­æå–
                        node_idx = support_to_idx[sup_id]
                        true_val_normalized = example_y_true[i, 0, node_idx, 0].cpu().item()
                        pred_val_normalized = example_y_pred[i, 0, node_idx, 0].cpu().item()
                    else:
                        # LSTM/AttentionLSTM/Transformer: ç›´æ¥è¾“å‡ºæ ‡é‡
                        true_val_normalized = example_y_true[i].cpu().item()
                        pred_val_normalized = example_y_pred[i].cpu().item()
                    
                    # åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦ (MinMax åå˜æ¢)
                    true_val = true_val_normalized * y_range + y_min
                    pred_val = pred_val_normalized * y_range + y_min
                    error = abs(pred_val - true_val)
                    
                    comparison_data.append({
                        'æ”¯æ¶ç¼–å·': sup_id,
                        'çœŸå®å€¼ (MPa)': f"{true_val:.2f}",
                        'é¢„æµ‹å€¼ (MPa)': f"{pred_val:.2f}",
                        'è¯¯å·® (MPa)': f"{error:.2f}",
                        'è¯¯å·®ç‡': f"{error/abs(true_val)*100:.1f}%" if abs(true_val) > 1e-6 else "N/A"
                    })
                
                st.table(pd.DataFrame(comparison_data))
                
                # â­ é«˜çº§è¯Šæ–­å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if show_advanced_plots:
                    st.markdown("---")
                    st.subheader("ğŸ“Š é«˜çº§è¯Šæ–­åˆ†æ")
                    try:
                        # è·å–å®Œæ•´éªŒè¯é›†é¢„æµ‹
                        model.eval()
                        with torch.no_grad():
                            if "STGCN" in model_type:
                                all_val_pred = model(val_X_tensor, A_hat_tensor)
                            else:
                                all_val_pred = model(val_X_tensor)
                        
                        # è½¬æ¢ä¸ºnumpy
                        if "STGCN" in model_type:
                            # STGCNè¾“å‡º: (B, T, N, 1)ï¼Œå–æœ€åæ—¶é—´æ­¥å’Œæ‰€æœ‰èŠ‚ç‚¹
                            val_pred_np = all_val_pred[:, -1, :, 0].cpu().numpy().flatten()
                            val_true_np = val_y_tensor[:, -1, :, 0].cpu().numpy().flatten()
                        else:
                            val_pred_np = all_val_pred.cpu().numpy().flatten()
                            val_true_np = val_y_tensor.cpu().numpy().flatten()
                        
                        # åå½’ä¸€åŒ–
                        val_pred_original = val_pred_np * y_range + y_min
                        val_true_original = val_true_np * y_range + y_min
                        
                        # è°ƒç”¨é«˜çº§è¯Šæ–­å‡½æ•°
                        plot_advanced_diagnostics(
                            y_true=val_true_original,
                            y_pred=val_pred_original,
                            train_losses=train_losses,
                            val_losses=val_losses
                        )
                    except Exception as plot_error:
                        st.warning(f"é«˜çº§å¯è§†åŒ–å‡ºé”™: {str(plot_error)}")
                
                # æ¸…ç†æ˜¾å­˜
                del example_X, example_y_true, example_y_pred
                torch.cuda.empty_cache()
                
            except Exception as e:
                st.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
                import traceback
                st.code(traceback.format_exc())
        
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        st.code(traceback.format_exc())

elif data_source == "ä¸Šä¼ CSVæ–‡ä»¶" and data_file:
    st.header("1. æ•°æ®åŠ è½½ä¸å¯¹é½")
    
    # è½½å…¥æ•°æ®
    try:
        # åŠ è½½çŸ¿å‹æ•°æ®
        data, column_names = load_csv_data(data_file)
        
        st.write(f"**çŸ¿å‹æ•°æ®å½¢çŠ¶:** {data.shape}")
        st.write(f"- æ—¶é—´æ­¥æ•°: {data.shape[0]}")
        st.write(f"- æ”¯æ¶æ•°é‡: {data.shape[1]}")
        st.write(f"- ç‰¹å¾æ•°: {data.shape[2]}")
        
        NUM_SAMPLES, NUM_NODES, NUM_FEATURES = data.shape
        
        # æ˜¾ç¤ºæ”¯æ¶åˆ—è¡¨
        with st.expander("ğŸ“‹ æŸ¥çœ‹æ”¯æ¶åˆ—è¡¨"):
            st.write(column_names)
        
        # åæ ‡å¯¹é½
        coords_array = None
        if coord_file:
            st.subheader("ğŸ—ºï¸ åæ ‡å¯¹é½")
            try:
                coord_df = load_coordinate_file(coord_file)
                st.write("**åæ ‡æ–‡ä»¶é¢„è§ˆ:**")
                st.dataframe(coord_df.head(10))
                
                # å¯¹é½åæ ‡
                coords_array, alignment_info = align_data_with_coordinates(column_names, coord_df)
                
                # æ˜¾ç¤ºå¯¹é½ç»“æœ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»æ”¯æ¶æ•°", alignment_info['total_supports'])
                with col2:
                    st.metric("æˆåŠŸåŒ¹é…", alignment_info['matched'], 
                             delta=f"{alignment_info['matched']/alignment_info['total_supports']*100:.1f}%")
                with col3:
                    st.metric("ç»´åº¦", "3D" if alignment_info['has_z'] else "2D")
                
                if alignment_info['missing']:
                    st.warning(f"âš ï¸ ä»¥ä¸‹ {len(alignment_info['missing'])} ä¸ªæ”¯æ¶æœªæ‰¾åˆ°åæ ‡: {', '.join(alignment_info['missing'][:5])}" + 
                              ("..." if len(alignment_info['missing']) > 5 else ""))
                else:
                    st.success("âœ… æ‰€æœ‰æ”¯æ¶åæ ‡å¯¹é½æˆåŠŸ!")
                
                # å¯è§†åŒ–æ”¯æ¶åˆ†å¸ƒ
                st.subheader("æ”¯æ¶ç©ºé—´åˆ†å¸ƒ")
                fig_scatter_data = pd.DataFrame({
                    'Xåæ ‡': coords_array[:, 0],
                    'Yåæ ‡': coords_array[:, 1],
                    'æ”¯æ¶': column_names
                })
                st.scatter_chart(fig_scatter_data, x='Xåæ ‡', y='Yåæ ‡', size=20)
                
            except Exception as e:
                st.error(f"åæ ‡å¯¹é½å¤±è´¥: {e}")
                st.info("å°†ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„åæ ‡")
        else:
            st.warning("âš ï¸ æœªä¸Šä¼ åæ ‡æ–‡ä»¶,å°†ä½¿ç”¨çº¿æ€§æ’åˆ—çš„é»˜è®¤åæ ‡")
            # ç”Ÿæˆé»˜è®¤åæ ‡(çº¿æ€§æ’åˆ—)
            coords_array = np.column_stack([np.arange(NUM_NODES), np.zeros(NUM_NODES)])
        
        # åœ°è´¨ç‰¹å¾èåˆ
        geo_features = None
        if use_geological and geo_file and coords_array is not None:
            st.subheader("ğŸŒ åœ°è´¨ç‰¹å¾èåˆ")
            try:
                geo_features, feature_names = load_geological_features(geo_file, coords_array)
                st.write(f"**åœ°è´¨ç‰¹å¾å½¢çŠ¶:** {geo_features.shape}")
                st.write(f"**ç‰¹å¾åç§°:** {feature_names}")
                
                # æ˜¾ç¤ºåœ°è´¨ç‰¹å¾ç»Ÿè®¡
                geo_df_display = pd.DataFrame(geo_features, columns=feature_names)
                st.write("**åœ°è´¨ç‰¹å¾ç»Ÿè®¡:**")
                st.dataframe(geo_df_display.describe())
                
                # å°†åœ°è´¨ç‰¹å¾æ·»åŠ åˆ°æ•°æ®ä¸­
                # å°†åœ°è´¨ç‰¹å¾æ‰©å±•åˆ°æ‰€æœ‰æ—¶é—´æ­¥
                geo_features_expanded = np.tile(geo_features[np.newaxis, :, :], (NUM_SAMPLES, 1, 1))
                # (num_samples, num_nodes, geo_features)
                
                # åˆå¹¶çŸ¿å‹æ•°æ®å’Œåœ°è´¨ç‰¹å¾
                data = np.concatenate([data, geo_features_expanded], axis=-1)
                NUM_FEATURES = data.shape[2]
                
                st.success(f"âœ… åœ°è´¨ç‰¹å¾å·²èåˆ! æ€»ç‰¹å¾æ•°: {NUM_FEATURES}")
                
            except Exception as e:
                st.error(f"åœ°è´¨ç‰¹å¾åŠ è½½å¤±è´¥: {e}")
                st.info("å°†ä»…ä½¿ç”¨çŸ¿å‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
        
        # ç”Ÿæˆæˆ–åŠ è½½é‚»æ¥çŸ©é˜µ
        st.header("2. å›¾ç»“æ„æ„å»º")
        if adj_method == "upload" and adj_file:
            # ä¸Šä¼ è‡ªå®šä¹‰é‚»æ¥çŸ©é˜µ
            if adj_file.name.endswith('.npy'):
                adj_mx = np.load(adj_file)
            else:  # CSV
                adj_mx = pd.read_csv(adj_file).values
            st.write(f"**é‚»æ¥çŸ©é˜µ (ä¸Šä¼ ) å½¢çŠ¶:** {adj_mx.shape}")
        else:
            # è‡ªåŠ¨ç”Ÿæˆé‚»æ¥çŸ©é˜µ
            if adj_method == "knn" and coords_array is not None:
                # ä½¿ç”¨çœŸå®åæ ‡ç”Ÿæˆ KNN
                adj_params['coords'] = coords_array
                st.info(f"âœ… ä½¿ç”¨çœŸå®æ”¯æ¶åæ ‡ç”Ÿæˆ K={adj_params.get('k', 3)} è¿‘é‚»å›¾")
            elif adj_method == "distance" and coords_array is not None:
                # ä½¿ç”¨è·ç¦»é˜ˆå€¼
                adj_params['coords'] = coords_array
                threshold = st.slider("è·ç¦»é˜ˆå€¼ (å•ä½:ç±³)", 1.0, 100.0, 10.0)
                adj_params['threshold'] = threshold
                st.info(f"âœ… ä½¿ç”¨çœŸå®åæ ‡ç”Ÿæˆè·ç¦»å›¾ (é˜ˆå€¼={threshold}ç±³)")
            elif adj_method in ["knn", "distance"] and coords_array is None:
                st.warning("âš ï¸ éœ€è¦åæ ‡æ–‡ä»¶æ‰èƒ½ä½¿ç”¨è·ç¦»ç›¸å…³æ–¹æ³•,å°†ä½¿ç”¨éšæœºåæ ‡")
                adj_params['coords'] = np.random.rand(NUM_NODES, 2)
            
            adj_mx = generate_adjacency_matrix(NUM_NODES, adj_method, **adj_params)
            st.write(f"**é‚»æ¥çŸ©é˜µ (è‡ªåŠ¨ç”Ÿæˆ):** {adj_mx.shape}")
            st.info(f"ä½¿ç”¨ **{adj_method}** æ–¹æ³•ç”Ÿæˆé‚»æ¥çŸ©é˜µ")
        
        # éªŒè¯é‚»æ¥çŸ©é˜µ
        if NUM_NODES != adj_mx.shape[0] or NUM_NODES != adj_mx.shape[1]:
            st.error(f"æ•°æ®ä¸é‚»æ¥çŸ©é˜µçš„èŠ‚ç‚¹æ•°ä¸åŒ¹é…! (æ•°æ®: {NUM_NODES}, é‚»æ¥çŸ©é˜µ: {adj_mx.shape[0]})")
        else:
            st.success("æ•°æ®æ–‡ä»¶å’Œé‚»æ¥çŸ©é˜µåŠ è½½/ç”ŸæˆæˆåŠŸ!")
            
            # æ˜¾ç¤ºé‚»æ¥çŸ©é˜µçš„è¿æ¥ç»Ÿè®¡
            num_edges = np.sum(adj_mx) / 2  # é™¤ä»¥2å› ä¸ºæ˜¯æ— å‘å›¾
            st.write(f"**å›¾ç»“æ„ç»Ÿè®¡:**")
            st.write(f"- æ€»è¾¹æ•°: {int(num_edges)}")
            st.write(f"- å¹³å‡åº¦æ•°: {np.sum(adj_mx, axis=1).mean():.2f}")
            
            # å¯è§†åŒ–é‚»æ¥çŸ©é˜µ
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("é‚»æ¥çŸ©é˜µå¯è§†åŒ–")
                st.image(adj_mx, use_container_width=True, clamp=True, caption="ç™½è‰²=è¿æ¥,é»‘è‰²=æ— è¿æ¥")
            
            with col2:
                st.subheader("èŠ‚ç‚¹ 0 çš„æ•°æ®é¢„è§ˆ")
                chart_data = pd.DataFrame(data[:min(500, len(data)), 0, 0], columns=['Node 0, Feature 0'])
                st.line_chart(chart_data)

            # --- è®­ç»ƒæ¨¡å— ---
            st.header("3. æ¨¡å‹è®­ç»ƒ")
            if st.button("å¼€å§‹è®­ç»ƒ"):
                
                with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®..."):
                    # 1. è®¡ç®— A_hat
                    A_hat = calculate_normalized_laplacian(adj_mx)
                    
                    # 2. ç”Ÿæˆ Dataloaders
                    train_loader, val_loader, test_loader, scaler = generate_dataloader(
                        data, BATCH_SIZE, SEQ_LEN, PRED_LEN
                    )
                    
                    # 3. åˆå§‹åŒ–æ¨¡å‹
                    device = torch.device(DEVICE)
                    model = STGCN(NUM_NODES, NUM_FEATURES, SEQ_LEN, PRED_LEN).to(device)
                    optimizer = optim.Adam(model.parameters(), lr=LR)
                    loss_fn = nn.MSELoss()
                    
                    st.success("æ¨¡å‹å’Œæ•°æ®åˆå§‹åŒ–å®Œæˆï¼å¼€å§‹è®­ç»ƒ...")
                
                # å‡†å¤‡å®æ—¶æ˜¾ç¤º
                status_placeholder = st.empty()
                loss_chart_placeholder = st.empty()
                loss_df = pd.DataFrame(columns=["Epoch", "Train Loss", "Val Loss"])

                start_time = time.time()
                
                for epoch in range(1, EPOCHS + 1):
                    # è®­ç»ƒ
                    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device, A_hat)
                    
                    # éªŒè¯
                    val_loss = evaluate(model, val_loader, loss_fn, device, A_hat)
                    
                    # æ›´æ–°çŠ¶æ€
                    elapsed = time.time() - start_time
                    status_text = f"""
                    **Epoch: {epoch}/{EPOCHS}**
                    - è®­ç»ƒæŸå¤± (Train Loss): {train_loss:.6f}
                    - éªŒè¯æŸå¤± (Val Loss): {val_loss:.6f}
                    - å·²ç”¨æ—¶é—´ (Elapsed): {elapsed:.2f}s
                    """
                    status_placeholder.markdown(status_text)
                    
                    # æ›´æ–°å›¾è¡¨
                    new_loss_row = pd.DataFrame({
                        "Epoch": [epoch],
                        "Train Loss": [train_loss],
                        "Val Loss": [val_loss]
                    })
                    loss_df = pd.concat([loss_df, new_loss_row], ignore_index=True)
                    loss_chart_placeholder.line_chart(loss_df.set_index("Epoch"))

                st.success("æ¨¡å‹è®­ç»ƒå®Œæˆ!")
                
                # --- ç»“æœå±•ç¤º ---
                st.header("4. è®­ç»ƒç»“æœ")
                st.subheader("æœ€ç»ˆæŸå¤±æ›²çº¿")
                st.line_chart(loss_df.set_index("Epoch"))
                
                st.subheader("æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç° (éšæœºæŠ½æ ·)")
                try:
                    # ä»æµ‹è¯•é›†è·å–ä¸€ä¸ªæ‰¹æ¬¡
                    x_test_batch, y_test_batch = next(iter(test_loader))
                    x_test_batch, y_test_batch = x_test_batch.to(device), y_test_batch.to(device)
                    A_hat_tensor = torch.tensor(A_hat).to(device)
                    
                    model.eval()
                    with torch.no_grad():
                        y_pred = model(x_test_batch, A_hat_tensor) # (B, T_out, N, F_out)
                        y_pred = y_pred.permute(0, 3, 2, 1) # (B, F_out, N, T_out)

                    # åå½’ä¸€åŒ–
                    y_pred_real = (y_pred.cpu().numpy() * scaler['std']) + scaler['mean']
                    y_test_real = (y_test_batch.cpu().numpy() * scaler['std']) + scaler['mean']
                    
                    # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬å’Œä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œæ¯”è¾ƒ (Batch 0, Node 0)
                    pred_series = y_pred_real[0, :, 0, 0] # é¢„æµ‹å€¼
                    true_series = y_test_real[0, :, 0, 0] # çœŸå®å€¼
                    
                    if PRED_LEN == 1:
                        st.write("é¢„æµ‹å€¼ (Test Sample 0, Node 0):", pred_series[0])
                        st.write("çœŸå®å€¼ (Test Sample 0, Node 0):", true_series[0])
                    else:
                        result_df = pd.DataFrame({
                            'Predicted': pred_series,
                            'True': true_series
                        }, index=[f't+{i+1}' for i in range(PRED_LEN)])
                        
                        st.dataframe(result_df)
                        st.line_chart(result_df)
                        
                except Exception as e:
                    st.error(f"åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ—¶å‡ºé”™: {e}")

    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        st.error("è¯·ç¡®ä¿æ‚¨ä¸Šä¼ äº†æ­£ç¡®æ ¼å¼çš„ CSV æ–‡ä»¶ã€‚")
        st.info("""
        **CSV æ–‡ä»¶æ ¼å¼æç¤º:**
        - æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ—¶é—´ç‚¹
        - æ¯åˆ—ä»£è¡¨ä¸€ä¸ªæ”¯æ¶(ç›‘æµ‹ç‚¹)
        - å¯ä»¥åŒ…å«æ—¶é—´åˆ—(ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶ç§»é™¤)
        """)
else:
    st.info("è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„ CSV çŸ¿å‹æ•°æ®æ–‡ä»¶ä»¥å¼€å§‹ã€‚")