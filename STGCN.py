import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import math
import time
import warnings
from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt

# å¿½ç•¥ CUDA å…¼å®¹æ€§è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, message='.*CUDA capability.*')
warnings.filterwarnings('ignore', category=UserWarning, message='.*NVIDIA.*')

# è®¾ç½® CUDA ç¯å¢ƒ
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

# è§£å†³ OpenMP å†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# ----------------------------------------------------------------------
# 1. å¸®åŠ©å‡½æ•° (Utils)
# ----------------------------------------------------------------------

def load_csv_data(csv_file, time_col=None):
    """
    ä» CSV æ–‡ä»¶åŠ è½½çŸ¿å‹æ•°æ®
    :param csv_file: CSV æ–‡ä»¶å¯¹è±¡
    :param time_col: æ—¶é—´åˆ—åç§°(å¦‚æœæœ‰)
    :return: numpy array (num_samples, num_nodes, num_features), column_names
    """
    df = pd.read_csv(csv_file)
    
    # å¦‚æœæœ‰æ—¶é—´åˆ—,åˆ é™¤å®ƒ
    if time_col and time_col in df.columns:
        df = df.drop(columns=[time_col])
    
    # å°è¯•è‡ªåŠ¨æ£€æµ‹æ—¶é—´åˆ—(å¸¸è§åç§°)
    time_cols = ['æ—¶é—´', 'time', 'Time', 'DATE', 'date', 'datetime', 'Datetime', 'æ—¥æœŸ']
    for col in time_cols:
        if col in df.columns:
            df = df.drop(columns=[col])
            break
    
    # ä¿å­˜åˆ—å(æ”¯æ¶åç§°)
    column_names = df.columns.tolist()
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    data = df.values  # (num_samples, num_nodes)
    
    # æ·»åŠ ç‰¹å¾ç»´åº¦ (å‡è®¾åªæœ‰ä¸€ä¸ªç‰¹å¾:å‹åŠ›å€¼)
    data = np.expand_dims(data, axis=-1)  # (num_samples, num_nodes, 1)
    
    return data, column_names

def load_processed_sequence_data(npz_file):
    """
    åŠ è½½é¢„å¤„ç†å¥½çš„åºåˆ—æ•°æ®é›†
    :param npz_file: .npz æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡
    :return: X, y_final, support_ids, feature_names
    """
    if isinstance(npz_file, str):
        data = np.load(npz_file, allow_pickle=True)
    else:
        data = np.load(npz_file, allow_pickle=True)
    
    X = data['X']  # (num_samples, seq_len, num_features)
    y_final = data['y_final']  # (num_samples, pred_len)
    support_ids = data['support_ids']  # (num_samples,)
    feature_names = data['feature_names'].tolist() if 'feature_names' in data else []
    
    return X, y_final, support_ids, feature_names

def load_coordinate_file(coord_file):
    """
    åŠ è½½æ”¯æ¶åæ ‡æ–‡ä»¶
    :param coord_file: åæ ‡æ–‡ä»¶å¯¹è±¡ (CSV æˆ– Excel)
    :return: DataFrame with columns [æ”¯æ¶ID/åç§°, Xåæ ‡, Yåæ ‡, (å¯é€‰)Zåæ ‡]
    """
    if coord_file.name.endswith('.csv'):
        df = pd.read_csv(coord_file)
    elif coord_file.name.endswith(('.xls', '.xlsx')):
        df = pd.read_excel(coord_file)
    else:
        raise ValueError("åæ ‡æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ,è¯·ä½¿ç”¨ CSV æˆ– Excel æ ¼å¼")
    
    return df

def align_data_with_coordinates(column_names, coord_df):
    """
    å¯¹é½çŸ¿å‹æ•°æ®å’Œåæ ‡æ•°æ®
    :param column_names: çŸ¿å‹æ•°æ®çš„åˆ—å(æ”¯æ¶åç§°)
    :param coord_df: åæ ‡æ•°æ®DataFrame
    :return: å¯¹é½åçš„åæ ‡æ•°ç»„ (num_nodes, 2 æˆ– 3), å¯¹é½ä¿¡æ¯
    """
    # å°è¯•æ‰¾åˆ°åæ ‡DataFrameä¸­çš„æ”¯æ¶IDåˆ—
    possible_id_cols = ['æ”¯æ¶ID', 'æ”¯æ¶ç¼–å·', 'æ”¯æ¶åç§°', 'ID', 'id', 'Name', 'name', 'ç¼–å·']
    id_col = None
    for col in possible_id_cols:
        if col in coord_df.columns:
            id_col = col
            break
    
    if id_col is None:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°,ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºIDåˆ—
        id_col = coord_df.columns[0]
    
    # å°è¯•æ‰¾åˆ°X,Yåæ ‡åˆ—
    possible_x_cols = ['X', 'x', 'Xåæ ‡', 'xåæ ‡', 'lon', 'longitude', 'ç»åº¦']
    possible_y_cols = ['Y', 'y', 'Yåæ ‡', 'yåæ ‡', 'lat', 'latitude', 'çº¬åº¦']
    possible_z_cols = ['Z', 'z', 'Zåæ ‡', 'zåæ ‡', 'elevation', 'é«˜ç¨‹']
    
    x_col = next((col for col in possible_x_cols if col in coord_df.columns), None)
    y_col = next((col for col in possible_y_cols if col in coord_df.columns), None)
    z_col = next((col for col in possible_z_cols if col in coord_df.columns), None)
    
    if x_col is None or y_col is None:
        raise ValueError("æ— æ³•è¯†åˆ«åæ ‡åˆ—,è¯·ç¡®ä¿åæ ‡æ–‡ä»¶åŒ…å« X å’Œ Y åˆ—")
    
    # åˆ›å»ºåæ ‡å­—å…¸
    coord_dict = {}
    for _, row in coord_df.iterrows():
        support_id = str(row[id_col]).strip()
        if z_col:
            coord_dict[support_id] = [row[x_col], row[y_col], row[z_col]]
        else:
            coord_dict[support_id] = [row[x_col], row[y_col]]
    
    # å¯¹é½åæ ‡
    aligned_coords = []
    missing_coords = []
    
    for col_name in column_names:
        col_name_str = str(col_name).strip()
        if col_name_str in coord_dict:
            aligned_coords.append(coord_dict[col_name_str])
        else:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…
            matched = False
            for key in coord_dict.keys():
                if col_name_str in key or key in col_name_str:
                    aligned_coords.append(coord_dict[key])
                    matched = True
                    break
            if not matched:
                missing_coords.append(col_name_str)
                # ä½¿ç”¨é»˜è®¤åæ ‡æˆ–è·³è¿‡
                if len(aligned_coords) > 0:
                    aligned_coords.append(aligned_coords[-1])  # ä½¿ç”¨ä¸Šä¸€ä¸ªåæ ‡
                else:
                    aligned_coords.append([0, 0] if z_col is None else [0, 0, 0])
    
    coords_array = np.array(aligned_coords)
    
    alignment_info = {
        'total_supports': len(column_names),
        'matched': len(column_names) - len(missing_coords),
        'missing': missing_coords,
        'has_z': z_col is not None
    }
    
    return coords_array, alignment_info

def load_geological_features(geo_file, coords_array, column_names=None):
    """
    åŠ è½½åœ°è´¨ç‰¹å¾æ•°æ®å¹¶æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
    :param geo_file: åœ°è´¨ç‰¹å¾æ–‡ä»¶ (CSV, Excel, æˆ–è·¯å¾„å­—ç¬¦ä¸²)
    :param coords_array: æ”¯æ¶åæ ‡æ•°ç»„ (num_nodes, 2 æˆ– 3)
    :param column_names: æ”¯æ¶åç§°åˆ—è¡¨(å¯é€‰,ç”¨äºç›´æ¥åŒ¹é…é’»å­”åç§°)
    :return: åœ°è´¨ç‰¹å¾çŸ©é˜µ (num_nodes, num_geo_features), ç‰¹å¾åˆ—å
    """
    # æ”¯æŒæ–‡ä»¶å¯¹è±¡æˆ–è·¯å¾„å­—ç¬¦ä¸²
    if isinstance(geo_file, str):
        if geo_file.endswith('.csv'):
            geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        elif geo_file.endswith(('.xls', '.xlsx')):
            geo_df = pd.read_excel(geo_file)
        else:
            raise ValueError("åœ°è´¨æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")
    else:
        if geo_file.name.endswith('.csv'):
            geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        elif geo_file.name.endswith(('.xls', '.xlsx')):
            geo_df = pd.read_excel(geo_file)
        else:
            raise ValueError("åœ°è´¨æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")
    
    from scipy.spatial import KDTree
    
    # æå–åœ°è´¨ç‚¹åæ ‡
    x_col = next((col for col in ['x', 'X', 'Xåæ ‡', 'åæ ‡x'] if col in geo_df.columns), None)
    y_col = next((col for col in ['y', 'Y', 'Yåæ ‡', 'åæ ‡y'] if col in geo_df.columns), None)
    
    if x_col is None or y_col is None:
        raise ValueError("åœ°è´¨æ–‡ä»¶å¿…é¡»åŒ…å« X å’Œ Y åæ ‡åˆ—")
    
    geo_coords = geo_df[[x_col, y_col]].values
    
    # æå–åœ°è´¨ç‰¹å¾åˆ—(æ’é™¤åæ ‡ã€é’»å­”åç­‰IDåˆ—)
    exclude_cols = [x_col, y_col, 'borehole', 'é’»å­”å', 'id', 'ID', 'name']
    feature_cols = [col for col in geo_df.columns if col not in exclude_cols]
    
    # å¦‚æœæ²¡æœ‰æ•°å€¼ç‰¹å¾,è¿”å›ç©ºæ•°ç»„
    if len(feature_cols) == 0:
        return np.zeros((len(coords_array), 1)), ['dummy_feature']
    
    geo_features = geo_df[feature_cols].values
    
    # å¡«å……ç¼ºå¤±å€¼
    geo_features = np.nan_to_num(geo_features, nan=0.0)
    
    # ä½¿ç”¨ KNN æ’å€¼æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
    tree = KDTree(geo_coords)
    distances, indices = tree.query(coords_array[:, :2], k=1)
    
    # ä¸ºæ¯ä¸ªæ”¯æ¶åˆ†é…æœ€è¿‘é’»å­”çš„åœ°è´¨ç‰¹å¾
    support_geo_features = geo_features[indices.flatten()]
    
    return support_geo_features, feature_cols

def generate_adjacency_matrix(num_nodes, method='chain', **kwargs):
    """
    ç”Ÿæˆé‚»æ¥çŸ©é˜µ
    :param num_nodes: èŠ‚ç‚¹æ•°é‡
    :param method: ç”Ÿæˆæ–¹æ³•
        - 'chain': é“¾å¼ç»“æ„(ç›¸é‚»æ”¯æ¶è¿æ¥)
        - 'grid': ç½‘æ ¼ç»“æ„(å¦‚æœæ”¯æ¶æ’åˆ—æˆç½‘æ ¼)
        - 'distance': åŸºäºè·ç¦»(éœ€è¦æä¾›åæ ‡)
        - 'full': å…¨è¿æ¥
        - 'knn': Kè¿‘é‚»
    :param kwargs: é¢å¤–å‚æ•°
    :return: é‚»æ¥çŸ©é˜µ (num_nodes, num_nodes)
    """
    adj_mx = np.zeros((num_nodes, num_nodes))
    
    if method == 'chain':
        # é“¾å¼ç»“æ„:æ¯ä¸ªèŠ‚ç‚¹ä¸ç›¸é‚»èŠ‚ç‚¹è¿æ¥
        for i in range(num_nodes - 1):
            adj_mx[i, i + 1] = 1
            adj_mx[i + 1, i] = 1
    
    elif method == 'grid':
        # ç½‘æ ¼ç»“æ„(éœ€è¦æŒ‡å®šè¡Œåˆ—æ•°)
        rows = kwargs.get('rows', int(np.sqrt(num_nodes)))
        cols = num_nodes // rows
        for i in range(num_nodes):
            row, col = i // cols, i % cols
            # è¿æ¥ä¸Šä¸‹å·¦å³é‚»å±…
            neighbors = []
            if row > 0: neighbors.append((row - 1) * cols + col)
            if row < rows - 1: neighbors.append((row + 1) * cols + col)
            if col > 0: neighbors.append(row * cols + col - 1)
            if col < cols - 1: neighbors.append(row * cols + col + 1)
            for j in neighbors:
                adj_mx[i, j] = 1
    
    elif method == 'distance':
        # åŸºäºè·ç¦»(éœ€è¦æä¾›åæ ‡)
        coords = kwargs.get('coords')
        threshold = kwargs.get('threshold', 1.0)
        if coords is not None:
            distances = squareform(pdist(coords))
            adj_mx = (distances <= threshold).astype(float)
            np.fill_diagonal(adj_mx, 0)
    
    elif method == 'full':
        # å…¨è¿æ¥
        adj_mx = np.ones((num_nodes, num_nodes))
        np.fill_diagonal(adj_mx, 0)
    
    elif method == 'knn':
        # Kè¿‘é‚»(éœ€è¦æä¾›åæ ‡)
        coords = kwargs.get('coords')
        k = kwargs.get('k', 3)
        if coords is not None:
            distances = squareform(pdist(coords))
            for i in range(num_nodes):
                # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»å±…
                neighbors = np.argsort(distances[i])[1:k+1]
                adj_mx[i, neighbors] = 1
                adj_mx[neighbors, i] = 1
    
    return adj_mx

def calculate_normalized_laplacian(adj_mx):
    """
    è®¡ç®—æ ‡å‡†åŒ–çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ A_hat = D^{-1/2} * (A + I) * D^{-1/2}
    
    :param adj_mx: é‚»æ¥çŸ©é˜µ (N, N)
    :return: A_hat (N, N)
    """
    adj_mx = adj_mx + np.eye(adj_mx.shape[0])
    d = np.array(adj_mx.sum(1)).flatten()
    d_inv_sqrt = np.power(d, -0.5)
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = np.diag(d_inv_sqrt)
    normalized_laplacian = d_mat_inv_sqrt.dot(adj_mx).dot(d_mat_inv_sqrt)
    return normalized_laplacian.astype(np.float32)

def generate_dataloader(data, batch_size, seq_len=12, pre_len=1, train_ratio=0.7, val_ratio=0.1):
    """
    ç”Ÿæˆ PyTorch DataLoaders
    :param data: (num_samples, num_nodes, num_features)
    :param batch_size: æ‰¹é‡å¤§å°
    :param seq_len: å†å²æ—¶é—´æ­¥
    :param pre_len: é¢„æµ‹æ—¶é—´æ­¥
    :return: train_loader, val_loader, test_loader, scaler
    """
    # å½’ä¸€åŒ– (è¿™é‡Œä½¿ç”¨ç®€å•çš„ Z-Score)
    mean = data.mean()
    std = data.std()
    scaler = {'mean': mean, 'std': std}
    data = (data - mean) / std

    x, y = [], []
    for i in range(len(data) - seq_len - pre_len + 1):
        x.append(data[i : i + seq_len])
        y.append(data[i + seq_len : i + seq_len + pre_len, :, 0:1]) # å‡è®¾åªé¢„æµ‹ç¬¬ä¸€ä¸ªç‰¹å¾

    x = np.array(x) # (N_samples, seq_len, num_nodes, num_features)
    y = np.array(y) # (N_samples, pre_len, num_nodes, 1)

    # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹ (Batch, Features, Nodes, Time)
    x = np.transpose(x, (0, 3, 2, 1)) 
    # (Batch, Time_out, Nodes, Features_out) -> (Batch, Features_out, Nodes, Time_out)
    y = np.transpose(y, (0, 3, 2, 1))

    # åˆ’åˆ†æ•°æ®é›†
    num_samples = x.shape[0]
    train_end = int(num_samples * train_ratio)
    val_end = int(num_samples * (train_ratio + val_ratio))

    train_x, train_y = x[:train_end], y[:train_end]
    val_x, val_y = x[train_end:val_end], y[val_end:]
    test_x, test_y = x[val_end:], y[val_end:]

    # åˆ›å»º TensorDataset å’Œ DataLoader
    def create_loader(x_data, y_data):
        tensor_x = torch.tensor(x_data, dtype=torch.float32)
        tensor_y = torch.tensor(y_data, dtype=torch.float32)
        dataset = torch.utils.data.TensorDataset(tensor_x, tensor_y)
        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    train_loader = create_loader(train_x, train_y)
    val_loader = create_loader(val_x, val_y)
    test_loader = create_loader(test_x, test_y)

    return train_loader, val_loader, test_loader, scaler

# ----------------------------------------------------------------------
# 2. STGCN æ¨¡å‹å®šä¹‰ (PyTorch)
# ----------------------------------------------------------------------

class SimpleLSTM(nn.Module):
    """
    ç®€å•çš„ LSTM æ¨¡å‹ï¼ˆä¸ä½¿ç”¨å›¾ç»“æ„ï¼Œç›´æ¥åºåˆ—é¢„æµ‹ï¼‰
    é€‚ç”¨äºç¨€ç–å›¾æ•°æ®
    """
    def __init__(self, num_features, hidden_dim=128, num_layers=2):
        super(SimpleLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTM å±‚
        self.lstm = nn.LSTM(
            num_features, 
            hidden_dim, 
            num_layers, 
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        
        self.dropout = nn.Dropout(0.2)
        self.relu = nn.ReLU()
        
    def forward(self, X):
        """
        X: (Batch, seq_len, num_features)
        è¾“å‡º: (Batch, 1) - é¢„æµ‹å€¼
        """
        # LSTM
        lstm_out, _ = self.lstm(X)  # (B, T, hidden)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_hidden = lstm_out[:, -1, :]  # (B, hidden)
        
        # å…¨è¿æ¥å±‚
        x = self.relu(self.fc1(last_hidden))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        out = self.fc3(x)  # (B, 1)
        
        return out

class TimeBlock(nn.Module):
    """
    æ—¶åºå·ç§¯å— (TCN)
    """
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(TimeBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))
        self.conv2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))
        self.conv3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))

    def forward(self, X):
        # è¾“å…¥ X: (Batch, Channels, Nodes, Time_steps)
        # GLU (Gated Linear Unit)
        # X shape: (B, C_in, N, T)
        return (self.conv1(X) + self.conv3(X)) * torch.sigmoid(self.conv2(X))

class STGCNBlock(nn.Module):
    """
    æ—¶ç©ºå·ç§¯å— (Spatio-Temporal GCN Block)
    """
    def __init__(self, in_channels, spatial_channels, out_channels, num_nodes, Kt):
        super(STGCNBlock, self).__init__()
        # æ—¶åºå·ç§¯ (TCN)
        self.tcn = TimeBlock(in_channels, spatial_channels, Kt)
        # ç©ºé—´å·ç§¯ (GCN)
        self.gcn = nn.Conv2d(spatial_channels, out_channels, (1, 1))
        # æ‰¹é‡å½’ä¸€åŒ– - åº”è¯¥å¯¹ out_channels è¿›è¡Œå½’ä¸€åŒ–
        self.bn = nn.BatchNorm2d(out_channels)
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚(å¦‚æœç»´åº¦ä¸åŒ¹é…)
        self.residual_conv = nn.Conv2d(in_channels, out_channels, (1, 1)) if in_channels != out_channels else None
        
    def forward(self, X, A_hat):
        # X: (B, C_in, N, T)
        # A_hat: (N, N)

        # 1. TCN
        X_tcn = self.tcn(X) # (B, spatial_channels, N, T_out)
        
        # 2. GCN
        # (B, spatial_channels, N, T_out) -> (B, T_out, N, spatial_channels)
        X_gcn_input = X_tcn.permute(0, 3, 2, 1) 
        
        # (B, T_out, N, spatial_channels) x (N, N) -> (B, T_out, N, spatial_channels)
        X_gcn = torch.einsum('btni,nm->btmi', X_gcn_input, A_hat)
        
        # (B, T_out, N, spatial_channels) -> (B, spatial_channels, N, T_out)
        X_gcn = X_gcn.permute(0, 3, 2, 1)

        # 3. 1x1 å·ç§¯
        X_gcn = self.gcn(X_gcn) # (B, out_channels, N, T_out)
        
        # 4. æ‰¹å½’ä¸€åŒ–
        X_gcn = self.bn(X_gcn)
        
        # 5. æ®‹å·®è¿æ¥(éœ€è¦å¤„ç†æ—¶é—´ç»´åº¦å’Œé€šé“ç»´åº¦çš„å˜åŒ–)
        if X.shape[-1] > X_gcn.shape[-1]:  # æ—¶é—´ç»´åº¦å‡å°‘äº†
            # æˆªå– X çš„æœ€åå‡ ä¸ªæ—¶é—´æ­¥ä»¥åŒ¹é…
            X_res = X[:, :, :, -(X_gcn.shape[-1]):]
        else:
            X_res = X
            
        # å¦‚æœé€šé“æ•°ä¸åŒ¹é…,ä½¿ç”¨ 1x1 å·ç§¯æŠ•å½±
        if self.residual_conv is not None:
            X_res = self.residual_conv(X_res)
        
        # 6. æ¿€æ´»
        X_out = F.relu(X_gcn + X_res)
        
        return X_out


class STGCN(nn.Module):
    """
    STGCN å®Œæ•´æ¨¡å‹
    è¾“å…¥ç»´åº¦: (Batch, Features_in, Num_Nodes, Seq_Len)
    è¾“å‡ºç»´åº¦: (Batch, Features_out, Num_Nodes, Pred_Len)
    """
    def __init__(self, num_nodes, num_features, seq_len, pred_len, Kt=3):
        super(STGCN, self).__init__()
        self.num_nodes = num_nodes
        self.pred_len = pred_len
        
        # STGCN Block 1
        self.st_block1 = STGCNBlock(num_features, 64, 64, num_nodes, Kt)
        
        # STGCN Block 2
        self.st_block2 = STGCNBlock(64, 64, 64, num_nodes, Kt)
        
        # æœ€åä¸€ä¸ªæ—¶åºå·ç§¯
        self.last_tcn = TimeBlock(64, 128, Kt)
        
        # è®¡ç®—ç»è¿‡æ‰€æœ‰å±‚åçš„æ—¶é—´ç»´åº¦
        # æ¯ä¸ª TimeBlock ä½¿ç”¨ padding=(kernel_size-1)//2, æ‰€ä»¥ä¸æ”¹å˜æ—¶é—´ç»´åº¦
        # ä½†ç”±äºæˆ‘ä»¬åœ¨ forward ä¸­åšäº†æ®‹å·®è¿æ¥çš„æˆªå–,å®é™…ä¼šå‡å°‘
        # å®é™…ä¸Š TimeBlock ä½¿ç”¨ same padding,æ—¶é—´ç»´åº¦åº”è¯¥ä¿æŒä¸å˜
        # è®©æˆ‘ä»¬ä½¿ç”¨è‡ªé€‚åº”æ± åŒ–æ¥å¤„ç†
        
        # è¾“å‡ºå±‚:å°†ç‰¹å¾æ˜ å°„åˆ°é¢„æµ‹é•¿åº¦
        self.output_conv = nn.Conv2d(128, 128, (1, 1))
        self.temporal_conv = nn.Conv2d(128, pred_len, (1, 1))
        
    def forward(self, X, A_hat):
        # X: (B, C_in, N, T_in)
        
        # Block 1
        X = self.st_block1(X, A_hat) # (B, 64, N, T)
        
        # Block 2
        X = self.st_block2(X, A_hat) # (B, 64, N, T)
        
        # Last TCN
        X = self.last_tcn(X) # (B, 128, N, T)
        
        # Output layers
        X = F.relu(self.output_conv(X)) # (B, 128, N, T)
        
        # ä½¿ç”¨è‡ªé€‚åº”å¹³å‡æ± åŒ–å°†æ—¶é—´ç»´åº¦è°ƒæ•´ä¸ºé¢„æµ‹é•¿åº¦
        X = F.adaptive_avg_pool2d(X, (X.shape[2], self.pred_len)) # (B, 128, N, pred_len)
        
        # å°†é€šé“æ•°è½¬æ¢ä¸º1(åªé¢„æµ‹ä¸€ä¸ªç‰¹å¾)
        # ä½¿ç”¨1x1å·ç§¯å°†128ä¸ªé€šé“å‹ç¼©ä¸º1ä¸ªé€šé“
        final_conv = nn.Conv2d(128, 1, (1, 1)).to(X.device)
        X = final_conv(X) # (B, 1, N, pred_len)
        
        return X # (B, 1, N, pred_len)


# ----------------------------------------------------------------------
# 3. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
# ----------------------------------------------------------------------

def train_epoch(model, train_loader, optimizer, loss_fn, device, A_hat):
    model.train()
    total_loss = 0
    A_hat_tensor = torch.tensor(A_hat, dtype=torch.float32).to(device)
    
    for x_batch, y_batch in train_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        
        # x_batch: (B, F_in, N, T_in)
        # y_batch: (B, F_out, N, T_out)
        y_pred = model(x_batch, A_hat_tensor) # (B, 1, N, pred_len)
        
        # ç¡®ä¿ y_pred å’Œ y_batch ç»´åº¦åŒ¹é…
        # y_batch: (B, F_out, N, T_out)
        # y_pred: (B, 1, N, pred_len)
        loss = loss_fn(y_pred, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
    return total_loss / len(train_loader)

def evaluate(model, val_loader, loss_fn, device, A_hat):
    model.eval()
    total_loss = 0
    A_hat_tensor = torch.tensor(A_hat, dtype=torch.float32).to(device)
    
    with torch.no_grad():
        for x_batch, y_batch in val_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            
            # x_batch: (B, F_in, N, T_in)
            # y_batch: (B, F_out, N, T_out)
            y_pred = model(x_batch, A_hat_tensor) # (B, 1, N, pred_len)
            
            loss = loss_fn(y_pred, y_batch)
            total_loss += loss.item()
            
    return total_loss / len(val_loader)

# ----------------------------------------------------------------------
# 4. Streamlit äº¤äº’å¼ GUI ç•Œé¢
# ----------------------------------------------------------------------

st.title("STGCN çŸ¿å‹é¢„æµ‹æ¨¡å‹ - è®­ç»ƒç•Œé¢")

# --- ä¾§è¾¹æ :å‚æ•°è®¾ç½® ---
st.sidebar.header("æ¨¡å‹å‚æ•°è®¾ç½®")
SEQ_LEN = st.sidebar.slider("å†å²æ—¶é—´æ­¥ (Seq_Len)", 5, 24, 12)
PRED_LEN = st.sidebar.slider("é¢„æµ‹æ—¶é—´æ­¥ (Pred_Len)", 1, 12, 1)
BATCH_SIZE = st.sidebar.slider("æ‰¹é‡å¤§å° (Batch_Size)", 8, 128, 32)
EPOCHS = st.sidebar.slider("è®­ç»ƒè½®æ•° (Epochs)", 10, 200, 50)
LR = st.sidebar.number_input("å­¦ä¹ ç‡ (Learning_Rate)", 0.0001, 0.1, 0.001, format="%.4f")

# GPU æ£€æµ‹å’Œè®¾å¤‡é€‰æ‹©
gpu_available = torch.cuda.is_available()
gpu_usable = False

if gpu_available:
    # æµ‹è¯• GPU æ˜¯å¦çœŸçš„å¯ç”¨
    try:
        test_tensor = torch.rand(10, 10).cuda()
        _ = test_tensor * 2
        torch.cuda.synchronize()
        gpu_usable = True
        del test_tensor
        torch.cuda.empty_cache()
    except Exception as e:
        gpu_usable = False
        st.sidebar.warning(f"âš ï¸ GPU æ£€æµ‹åˆ°ä½†ä¸å¯ç”¨: {str(e)[:50]}...")

if gpu_usable:
    DEVICE = st.sidebar.selectbox("è®¾å¤‡", ["cuda", "cpu"], index=0)
    st.sidebar.success("âœ… GPU å¯ç”¨ä¸”æ­£å¸¸å·¥ä½œ")
else:
    DEVICE = "cpu"
    st.sidebar.info("â„¹ï¸ ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒ")
    if gpu_available:
        st.sidebar.warning("""
        âš ï¸ **GPU å…¼å®¹æ€§é—®é¢˜**
        
        æ‚¨çš„ RTX 5070 Ti (Blackwell æ¶æ„) éœ€è¦æ›´æ–°çš„ PyTorch ç‰ˆæœ¬ã€‚
        
        å½“å‰ PyTorch 2.2.2 ä¸æ”¯æŒè¯¥ GPUã€‚
        
        å»ºè®®:
        1. ä½¿ç”¨ CPU è®­ç»ƒ(å½“å‰é€‰é¡¹)
        2. ç­‰å¾… PyTorch 2.6+ ç‰ˆæœ¬
        3. æˆ–å°è¯• PyTorch nightly ç‰ˆæœ¬
        """)

st.sidebar.header("æ•°æ®ä¸Šä¼ ")

# æ•°æ®æºé€‰æ‹©
data_source = st.sidebar.radio(
    "é€‰æ‹©æ•°æ®æ¥æº",
    ["ä¸Šä¼ CSVæ–‡ä»¶", "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†"],
    help="CSV: åŸå§‹çŸ¿å‹æ—¶é—´åºåˆ— | é¢„å¤„ç†: å·²æå–ç‰¹å¾çš„è®­ç»ƒæ•°æ®"
)

# æ ¹æ®æ•°æ®æºæ˜¾ç¤ºä¸åŒçš„ä¸Šä¼ é€‰é¡¹
if data_source == "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†":
    st.sidebar.subheader("ğŸ“¦ åŠ è½½é¢„å¤„ç†æ•°æ®")
    
    # æ£€æŸ¥é»˜è®¤æ•°æ®é›†
    default_npz_path = os.path.join(os.path.dirname(__file__), 'processed_data', 'sequence_dataset.npz')
    use_default_dataset = False
    
    if os.path.exists(default_npz_path):
        use_default_dataset = st.sidebar.checkbox(
            "ä½¿ç”¨å·²ç”Ÿæˆçš„æ•°æ®é›†",
            value=True,
            help=f"è·¯å¾„: {default_npz_path}"
        )
    
    if use_default_dataset:
        npz_file = default_npz_path
        st.sidebar.success("âœ“ ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†")
        st.sidebar.info(
            """
            **æ•°æ®é›†ä¿¡æ¯:**
            - 195,836 ä¸ªè®­ç»ƒæ ·æœ¬
            - 125 ä¸ªæ”¯æ¶
            - 17 ç»´ç‰¹å¾
            - åºåˆ—é•¿åº¦: 5 â†’ 1
            """
        )
    else:
        npz_file = st.sidebar.file_uploader(
            "ä¸Šä¼ é¢„å¤„ç†æ•°æ®æ–‡ä»¶ (.npz)",
            type=["npz"],
            help="ä½¿ç”¨ preprocess/prepare_training_data.py ç”Ÿæˆçš„æ•°æ®"
        )
    
    # åŠ è½½åæ ‡æ–‡ä»¶
    coord_file_path = os.path.join(os.path.dirname(__file__), 'processed_data', 'support_coordinates.csv')
    if os.path.exists(coord_file_path):
        coord_file = coord_file_path
    else:
        coord_file = None
    
else:  # CSV æ–‡ä»¶ä¸Šä¼ æ¨¡å¼
    npz_file = None
    
    # æ­¥éª¤1: ä¸Šä¼ çŸ¿å‹æ•°æ®
    st.sidebar.subheader("æ­¥éª¤1: çŸ¿å‹æ•°æ®")
    data_file = st.sidebar.file_uploader("ä¸Šä¼ çŸ¿å‹æ•°æ®æ–‡ä»¶ (.csv)", type=["csv"])
    st.sidebar.info(
        """
        **CSV æ ¼å¼è¦æ±‚:**
        - æ¯è¡Œ = ä¸€ä¸ªæ—¶é—´ç‚¹
        - æ¯åˆ— = ä¸€ä¸ªæ”¯æ¶(åˆ—åè¦ä¸åæ ‡æ–‡ä»¶å¯¹åº”)
        
        ç¤ºä¾‹:
        ```
        æ—¶é—´, ZJ001, ZJ002, ZJ003, ...
        2023-01-01, 100.5, 98.3, 102.1, ...
        ```
        """
    )

    # æ­¥éª¤2: ä¸Šä¼ æ”¯æ¶åæ ‡
    st.sidebar.subheader("æ­¥éª¤2: æ”¯æ¶åæ ‡ (é‡è¦!)")
    coord_file = st.sidebar.file_uploader(
        "ä¸Šä¼ æ”¯æ¶åæ ‡æ–‡ä»¶ (.csv/.xlsx)", 
        type=["csv", "xlsx", "xls"],
        help="åæ ‡æ–‡ä»¶åº”åŒ…å«: æ”¯æ¶ID, Xåæ ‡, Yåæ ‡"
    )
    st.sidebar.info(
        """
        **åæ ‡æ–‡ä»¶æ ¼å¼:**
        ```
        æ”¯æ¶ID, Xåæ ‡, Yåæ ‡
        ZJ001, 1000.5, 2000.3
        ZJ002, 1001.2, 2000.5
        ZJ003, 1002.0, 2001.1
        ```
        
        âš ï¸ **æ”¯æ¶IDå¿…é¡»ä¸çŸ¿å‹æ•°æ®çš„åˆ—åå¯¹åº”**
        """
    )

# æ­¥éª¤3: ä¸Šä¼ åœ°è´¨ç‰¹å¾(å¯é€‰) - åªåœ¨CSVæ¨¡å¼ä¸‹æ˜¾ç¤º
use_geological = False  # é»˜è®¤å€¼
geo_file = None
if data_source == "ä¸Šä¼ CSVæ–‡ä»¶":
    st.sidebar.subheader("æ­¥éª¤3: åœ°è´¨ç‰¹å¾ (å¯é€‰)")
    use_geological = st.sidebar.checkbox("èåˆåœ°è´¨ç‰¹å¾æ•°æ®", value=False)
    
if use_geological:
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é»˜è®¤åœ°è´¨ç‰¹å¾æ–‡ä»¶
    default_geo_path = os.path.join(os.path.dirname(__file__), 'geology_features_extracted.csv')
    use_default_geo = False
    
    if os.path.exists(default_geo_path):
        use_default_geo = st.sidebar.checkbox(
            "ä½¿ç”¨æå–çš„é’»å­”åœ°è´¨ç‰¹å¾", 
            value=True,
            help=f"å·²æ£€æµ‹åˆ° geology_features_extracted.csv"
        )
    
    if use_default_geo:
        geo_file = default_geo_path
        st.sidebar.success("âœ“ ä½¿ç”¨é’»å­”åœ°è´¨ç‰¹å¾æ•°æ®")
        st.sidebar.info(
            """
            **åŒ…å«çš„åœ°è´¨ç‰¹å¾:**
            - æ€»åšåº¦ (m)
            - ç…¤å±‚åšåº¦/æ•°é‡ (m/ä¸ª)
            - é¡¶æ¿ç…¤å±‚åŸ‹æ·± (m)
            - å¹³å‡å¼¹æ€§æ¨¡é‡ (GPa)
            - å¹³å‡å®¹é‡ (kN/mÂ³)
            - æœ€å¤§æŠ—æ‹‰å¼ºåº¦ (MPa)
            - ç ‚å²©/æ³¥å²©å æ¯”
            """
        )
    else:
        geo_file = st.sidebar.file_uploader(
            "ä¸Šä¼ åœ°è´¨ç‰¹å¾æ–‡ä»¶ (.csv/.xlsx)",
            type=["csv", "xlsx", "xls"],
            help="åœ°è´¨æ–‡ä»¶åº”åŒ…å«: Xåæ ‡, Yåæ ‡, åœ°è´¨ç‰¹å¾"
        )
        st.sidebar.info(
            """
            **åœ°è´¨æ–‡ä»¶æ ¼å¼:**
            ```
            Xåæ ‡, Yåæ ‡, æ–­å±‚è·ç¦», ç…¤å±‚åšåº¦, ...
            1000.5, 2000.3, 50.2, 3.5, ...
            1001.0, 2000.5, 48.5, 3.6, ...
            ```
            
            ç³»ç»Ÿä¼šæ ¹æ®è·ç¦»å°†åœ°è´¨ç‰¹å¾æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
            """
        )

# é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼
st.sidebar.header("é‚»æ¥çŸ©é˜µè®¾ç½®")
adj_method = st.sidebar.selectbox(
    "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼",
    ["chain", "grid", "distance", "knn", "full", "upload"],
    format_func=lambda x: {
        "chain": "é“¾å¼ç»“æ„ (ç›¸é‚»æ”¯æ¶è¿æ¥)",
        "grid": "ç½‘æ ¼ç»“æ„ (2Dæ’åˆ—)",
        "distance": "è·ç¦»é˜ˆå€¼ (éœ€è¦åæ ‡)",
        "knn": "Kè¿‘é‚» (éœ€è¦åæ ‡)",
        "full": "å…¨è¿æ¥ (æ‰€æœ‰èŠ‚ç‚¹äº’è¿)",
        "upload": "ä¸Šä¼ è‡ªå®šä¹‰é‚»æ¥çŸ©é˜µ"
    }[x]
)

# æ ¹æ®é€‰æ‹©çš„æ–¹æ³•æ˜¾ç¤ºé¢å¤–å‚æ•°
adj_params = {}
if adj_method == "grid":
    adj_params['rows'] = st.sidebar.number_input("ç½‘æ ¼è¡Œæ•°", min_value=1, value=10)
elif adj_method == "knn":
    adj_params['k'] = st.sidebar.number_input("Kå€¼(è¿‘é‚»æ•°é‡)", min_value=1, value=3)
    st.sidebar.warning("Kè¿‘é‚»æ–¹æ³•éœ€è¦æ”¯æ¶åæ ‡ä¿¡æ¯,æš‚æ—¶ä½¿ç”¨éšæœºåæ ‡")

adj_file = None
if adj_method == "upload":
    adj_file = st.sidebar.file_uploader("ä¸Šä¼ é‚»æ¥çŸ©é˜µæ–‡ä»¶ (.npyæˆ–.csv)", type=["npy", "csv"])

# --- ä¸»ç•Œé¢ ---
if data_source == "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†" and npz_file:
    st.header("1. åŠ è½½é¢„å¤„ç†æ•°æ®é›†")
    
    try:
        # åŠ è½½é¢„å¤„ç†çš„åºåˆ—æ•°æ®
        X, y_final, support_ids, feature_names = load_processed_sequence_data(npz_file)
        
        st.write(f"**æ•°æ®å½¢çŠ¶:**")
        st.write(f"- æ ·æœ¬æ•°é‡: {X.shape[0]:,}")
        st.write(f"- åºåˆ—é•¿åº¦: {X.shape[1]} (å†å²æ­¥æ•°)")
        st.write(f"- ç‰¹å¾ç»´åº¦: {X.shape[2]}")
        st.write(f"- æ ‡ç­¾å½¢çŠ¶: {y_final.shape}")
        
        NUM_SAMPLES = X.shape[0]
        SEQ_LEN = X.shape[1]
        NUM_FEATURES = X.shape[2]
        PRED_LEN = y_final.shape[1]
        
        # è·å–æ”¯æ¶ä¿¡æ¯
        unique_supports = np.unique(support_ids)
        NUM_NODES = len(unique_supports)
        
        st.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼åŒ…å« {NUM_NODES} ä¸ªæ”¯æ¶ï¼Œ{NUM_SAMPLES:,} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        # æ˜¾ç¤ºç‰¹å¾åˆ—è¡¨
        with st.expander("ğŸ“‹ æŸ¥çœ‹ç‰¹å¾åˆ—è¡¨"):
            st.write(f"å…± {len(feature_names)} ä¸ªç‰¹å¾:")
            for i, fname in enumerate(feature_names, 1):
                st.write(f"{i}. {fname}")
        
        # åŠ è½½åæ ‡
        coords_array = None
        if coord_file:
            if isinstance(coord_file, str):
                coord_df = pd.read_csv(coord_file)
            else:
                coord_df = load_coordinate_file(coord_file)
            
            coords_array = coord_df[['x', 'y']].values
            st.write(f"**æ”¯æ¶åæ ‡:** {coords_array.shape}")
        else:
            # ä½¿ç”¨é»˜è®¤çº¿æ€§åæ ‡
            coords_array = np.column_stack([np.arange(NUM_NODES), np.zeros(NUM_NODES)])
            st.info("ä½¿ç”¨é»˜è®¤çº¿æ€§åæ ‡")
        
        # æ•°æ®åˆ†å‰²
        st.header("2. æ•°æ®åˆ†å‰²")
        
        st.info("""
        ğŸ’¡ **æ•°æ®é‡å……è¶³**ï¼šå½“å‰æœ‰ 195,836 ä¸ªæ ·æœ¬ï¼Œæ•°æ®é‡å……è¶³é€‚åˆæ·±åº¦å­¦ä¹ ã€‚
        
        å»ºè®®æ¯”ä¾‹ï¼šè®­ç»ƒ 70% / éªŒè¯ 15% / æµ‹è¯• 15%
        """)
        
        train_ratio = st.slider("è®­ç»ƒé›†æ¯”ä¾‹", 0.5, 0.9, 0.7, 0.05)
        val_ratio = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.05, 0.3, 0.15, 0.05)
        
        train_end = int(NUM_SAMPLES * train_ratio)
        val_end = int(NUM_SAMPLES * (train_ratio + val_ratio))
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("è®­ç»ƒé›†", f"{train_end:,}", f"{train_ratio*100:.0f}%")
        with col2:
            st.metric("éªŒè¯é›†", f"{val_end-train_end:,}", f"{val_ratio*100:.0f}%")
        with col3:
            st.metric("æµ‹è¯•é›†", f"{NUM_SAMPLES-val_end:,}", f"{(1-train_ratio-val_ratio)*100:.0f}%")
        
        # è½¬æ¢ä¸ºå›¾æ•°æ®æ ¼å¼
        # ç”±äºé¢„å¤„ç†æ•°æ®å·²ç»æ˜¯ (num_samples, seq_len, num_features)
        # æˆ‘ä»¬éœ€è¦é‡å¡‘ä¸º (num_samples_per_support, num_supports, seq_len, num_features)
        
        st.header("3. å›¾ç»“æ„æ„å»º")
        
        # ç”Ÿæˆé‚»æ¥çŸ©é˜µ
        adj_method = st.selectbox(
            "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼",
            ["distance", "knn", "chain", "full"],
            help="distance: åŸºäºåæ ‡è·ç¦» | knn: Kè¿‘é‚» | chain: é“¾å¼ | full: å…¨è¿æ¥"
        )
        
        adj_params = {}
        if adj_method == "knn":
            adj_params['k'] = st.number_input("Kå€¼(è¿‘é‚»æ•°é‡)", min_value=1, value=5, max_value=20)
        elif adj_method == "distance":
            adj_params['threshold'] = st.number_input("è·ç¦»é˜ˆå€¼(ç±³)", min_value=0.1, value=5.0, step=0.5)
        
        adj_mx = generate_adjacency_matrix(
            NUM_NODES,
            method=adj_method,
            coords=coords_array,
            **adj_params
        )
        
        # æ˜¾ç¤ºå›¾ä¿¡æ¯
        num_edges = np.sum(adj_mx > 0)
        avg_degree = num_edges / NUM_NODES
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("èŠ‚ç‚¹æ•°", NUM_NODES)
            st.metric("å¹³å‡åº¦æ•°", f"{avg_degree:.1f}")
        with col2:
            st.metric("è¾¹æ•°", int(num_edges))
            st.metric("å›¾å¯†åº¦", f"{num_edges/(NUM_NODES*(NUM_NODES-1))*100:.2f}%")
        
        # å¯è§†åŒ–é‚»æ¥çŸ©é˜µ
        with st.expander("ğŸ” æŸ¥çœ‹é‚»æ¥çŸ©é˜µ"):
            fig, ax = plt.subplots(figsize=(8, 8))
            im = ax.imshow(adj_mx, cmap='Blues', aspect='auto')
            ax.set_title("é‚»æ¥çŸ©é˜µ")
            ax.set_xlabel("æ”¯æ¶ ID")
            ax.set_ylabel("æ”¯æ¶ ID")
            plt.colorbar(im, ax=ax)
            st.pyplot(fig)
        
        # æ¨¡å‹è®­ç»ƒéƒ¨åˆ†
        st.header("4. æ¨¡å‹è®­ç»ƒ")
        
        # æ¨¡å‹é€‰æ‹©
        model_type = st.radio(
            "é€‰æ‹©æ¨¡å‹ç±»å‹",
            ["LSTM (æ¨è)", "STGCN (å›¾ç¥ç»ç½‘ç»œ)"],
            help="LSTM é€‚ç”¨äºç¨€ç–æ•°æ®ï¼ŒSTGCN é€‚ç”¨äºå¯†é›†å›¾æ•°æ®"
        )
        
        st.info(f"""
        **{'âœ… LSTM æ¨¡å‹' if model_type.startswith('LSTM') else 'âš ï¸ STGCN æ¨¡å‹'}**
        
        {'- ä¸ä½¿ç”¨å›¾ç»“æ„ï¼Œç›´æ¥åºåˆ—é¢„æµ‹' if model_type.startswith('LSTM') else '- ä½¿ç”¨å›¾ç»“æ„è¿›è¡Œç©ºé—´-æ—¶é—´è”åˆå»ºæ¨¡'}
        {'- é€‚åˆç¨€ç–æ•°æ®ï¼ˆå½“å‰æ•°æ®æ¯æ ·æœ¬åªæœ‰1ä¸ªèŠ‚ç‚¹ï¼‰' if model_type.startswith('LSTM') else '- é€‚åˆå¯†é›†å›¾æ•°æ®ï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰å€¼ï¼‰'}
        {'- è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ•ˆæœç¨³å®š' if model_type.startswith('LSTM') else '- éœ€è¦å®Œæ•´çš„å›¾ç»“æ„ä¿¡æ¯'}
        """)
        
        # è®­ç»ƒå‚æ•°
        col1, col2 = st.columns(2)
        with col1:
            epochs = st.number_input("è®­ç»ƒè½®æ•°", min_value=1, value=50, max_value=500)
            batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=16, value=64, max_value=512, step=16)
        with col2:
            learning_rate = st.number_input("å­¦ä¹ ç‡", min_value=0.0001, value=0.001, max_value=0.1, format="%.4f", step=0.0001)
            hidden_dim = st.number_input("éšè—å±‚ç»´åº¦", min_value=16, value=64, max_value=256, step=16)
        
        # ä¼˜åŒ–å»ºè®®
        with st.expander("ğŸ’¡ è®­ç»ƒä¼˜åŒ–å»ºè®®"):
            st.markdown("""
            **å¦‚æœæ•ˆæœä¸å¥½ï¼Œå¯ä»¥å°è¯•ï¼š**
            
            1. **å¢åŠ è®­ç»ƒè½®æ•°** â†’ æ”¹ä¸º 100-200 è½®
            2. **è°ƒæ•´å­¦ä¹ ç‡** â†’ å°è¯• 0.0001-0.005 ä¹‹é—´
            3. **å¢å¤§æ‰¹æ¬¡** â†’ æ”¹ä¸º 128 æˆ– 256ï¼ˆå¦‚æœæ˜¾å­˜å¤Ÿï¼‰
            4. **æ›´æ¢å›¾ç»“æ„** â†’ è¯•è¯• KNN (K=3-10) è€Œä¸æ˜¯ distance
            5. **è°ƒæ•´è·ç¦»é˜ˆå€¼** â†’ å¦‚æœç”¨ distance æ–¹æ³•ï¼Œè¯•è¯• 3-15 ç±³
            
            **å½“å‰ä¼˜åŒ–ï¼š**
            - âœ… é€ç‰¹å¾å½’ä¸€åŒ–ï¼ˆé¿å…ä¸åŒå°ºåº¦ç‰¹å¾çš„å½±å“ï¼‰
            - âœ… æ‰¹å¤„ç†éªŒè¯ï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
            - âœ… åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´ï¼ˆå¯è€ƒè™‘æ·»åŠ å­¦ä¹ ç‡è¡°å‡ï¼‰
            
            **ç†æƒ³æŒ‡æ ‡ï¼š**
            - MAE < 10 MPa
            - RMSE < 15 MPa  
            - RÂ² > 0.5
            """)
        
        if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary"):
            try:
                st.success("å¼€å§‹è®­ç»ƒSTGCNæ¨¡å‹...")
                
                # 1. æ•°æ®åˆ‡åˆ†
                st.write("### æ­¥éª¤1: æ•°æ®åˆ‡åˆ†")
                X_train = X[:train_end]
                y_train = y_final[:train_end]
                train_support_ids = support_ids[:train_end]
                
                X_val = X[train_end:val_end]
                y_val = y_final[train_end:val_end]
                val_support_ids = support_ids[train_end:val_end]
                
                X_test = X[val_end:]
                y_test = y_final[val_end:]
                test_support_ids = support_ids[val_end:]
                
                st.write(f"âœ“ è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬")
                st.write(f"âœ“ éªŒè¯é›†: {len(X_val):,} æ ·æœ¬")
                st.write(f"âœ“ æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬")
                
                # è·å–é‚»æ¥çŸ©é˜µ
                A_hat = adj_mx
                
                # 2. æ•°æ®å‡†å¤‡
                st.write("### æ­¥éª¤2: å‡†å¤‡GPUè®¡ç®—")
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                st.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
                
                # è½¬æ¢æ•°æ®æ ¼å¼ - STGCNéœ€è¦ (Batch, Features, Nodes, SeqLen)
                # å½“å‰ X_train: (samples, seq_len, features)
                # éœ€è¦æŒ‰ support_id é‡ç»„ä¸ºå›¾ç»“æ„
                
                # ä¸ºæ¯ä¸ªæ ·æœ¬æ‰¾åˆ°å¯¹åº”çš„supportç´¢å¼•
                unique_supports_list = np.unique(support_ids)
                support_to_idx = {sup_id: idx for idx, sup_id in enumerate(unique_supports_list)}
                num_nodes = len(unique_supports_list)
                
                st.write(f"å›¾èŠ‚ç‚¹æ•°: {num_nodes}")
                
                # åˆ›å»ºè®­ç»ƒæ•°æ®æ‰¹æ¬¡
                def prepare_batch_data(X_data, y_data, support_data, num_nodes):
                    """å°†åºåˆ—æ•°æ®è½¬æ¢ä¸ºSTGCNæ‰€éœ€çš„å›¾ç»“æ„æ‰¹æ¬¡"""
                    batch_size = len(X_data)
                    seq_len = X_data.shape[1]
                    num_features = X_data.shape[2]
                    
                    # åˆå§‹åŒ–æ‰¹æ¬¡å¼ é‡ (Batch, Features, Nodes, SeqLen)
                    batch_X = np.zeros((batch_size, num_features, num_nodes, seq_len))
                    batch_y = np.zeros((batch_size, 1, num_nodes, 1))
                    
                    for i in range(batch_size):
                        sup_id = support_data[i]
                        node_idx = support_to_idx[sup_id]
                        
                        # X: (seq_len, features) -> (features, 1, seq_len)
                        # å°†å•ä¸ªæ”¯æ¶çš„æ•°æ®æ”¾åˆ°å¯¹åº”èŠ‚ç‚¹ä½ç½®
                        batch_X[i, :, node_idx, :] = X_data[i].T
                        batch_y[i, 0, node_idx, 0] = y_data[i, 0]
                    
                    return batch_X, batch_y
                
                # è½¬æ¢è®­ç»ƒæ•°æ®
                st.write("### æ­¥éª¤3: æ•°æ®å½’ä¸€åŒ–ä¸æ ¼å¼è½¬æ¢")
                
                # â­ æ”¹è¿›çš„å½’ä¸€åŒ–ç­–ç•¥
                st.write("æ­£åœ¨è¿›è¡Œæ•°æ®å½’ä¸€åŒ–...")
                
                # å¯¹æ•´ä¸ªè®­ç»ƒé›†è®¡ç®—ç»Ÿè®¡é‡ï¼ˆåŒ…æ‹¬ç‰¹å¾å’Œç›®æ ‡ï¼‰
                # æ–¹æ¡ˆï¼šåªå¯¹ç›®æ ‡å€¼å½’ä¸€åŒ–ï¼Œç‰¹å¾ä¿æŒåŸå§‹å°ºåº¦
                y_mean = y_train.mean()
                y_std = y_train.std()
                if y_std < 1e-6:
                    y_std = 1.0
                
                # MinMax å½’ä¸€åŒ–ç›®æ ‡å€¼åˆ° [0, 1]
                y_min = y_train.min()
                y_max = y_train.max()
                y_range = y_max - y_min
                if y_range < 1e-6:
                    y_range = 1.0
                
                # ä½¿ç”¨ MinMax è€Œä¸æ˜¯ Z-Score
                y_train_normalized = (y_train - y_min) / y_range
                y_val_normalized = (y_val - y_min) / y_range
                
                # ç‰¹å¾å½’ä¸€åŒ–ï¼šé€ç‰¹å¾ MinMax
                X_train_normalized = X_train.copy()
                X_val_normalized = X_val.copy()
                
                for feat_idx in range(X_train.shape[2]):
                    feat_min = X_train[:, :, feat_idx].min()
                    feat_max = X_train[:, :, feat_idx].max()
                    feat_range = feat_max - feat_min
                    if feat_range < 1e-6:
                        feat_range = 1.0
                    
                    X_train_normalized[:, :, feat_idx] = (X_train[:, :, feat_idx] - feat_min) / feat_range
                    X_val_normalized[:, :, feat_idx] = (X_val[:, :, feat_idx] - feat_min) / feat_range
                
                st.write(f"âœ“ MinMaxå½’ä¸€åŒ–å®Œæˆ (yèŒƒå›´: {y_min:.2f} - {y_max:.2f} MPa)")
                
                # æ ¹æ®æ¨¡å‹ç±»å‹å‡†å¤‡æ•°æ®
                if model_type.startswith("LSTM"):
                    # LSTM: ç›´æ¥ä½¿ç”¨åºåˆ—æ•°æ®ï¼Œä¸éœ€è¦å›¾ç»“æ„
                    st.write("### æ­¥éª¤4: å‡†å¤‡åºåˆ—æ•°æ®ï¼ˆLSTMæ¨¡å¼ï¼‰")
                    
                    # æ•°æ®å·²ç»æ˜¯ (samples, seq_len, features) æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                    train_X_tensor = torch.FloatTensor(X_train_normalized)
                    train_y_tensor = torch.FloatTensor(y_train_normalized).unsqueeze(1)  # (N, 1)
                    val_X_tensor = torch.FloatTensor(X_val_normalized)
                    val_y_tensor = torch.FloatTensor(y_val_normalized).unsqueeze(1)
                    
                    st.write(f"è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                    st.write(f"éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                    
                else:
                    # STGCN: éœ€è¦å›¾ç»“æ„
                    st.write("### æ­¥éª¤4: è½¬æ¢ä¸ºå›¾æ•°æ®æ ¼å¼ï¼ˆSTGCNæ¨¡å¼ï¼‰")
                
                with st.spinner("è½¬æ¢è®­ç»ƒæ•°æ®æ ¼å¼..."):
                    if model_type.startswith("STGCN"):
                        train_X_graph, train_y_graph = prepare_batch_data(
                            X_train_normalized, y_train_normalized, train_support_ids, num_nodes
                        )
                        val_X_graph, val_y_graph = prepare_batch_data(
                            X_val_normalized, y_val_normalized, val_support_ids, num_nodes
                        )
                        
                        # è½¬ä¸ºtorchå¼ é‡
                        train_X_tensor = torch.FloatTensor(train_X_graph)
                        train_y_tensor = torch.FloatTensor(train_y_graph)
                        val_X_tensor = torch.FloatTensor(val_X_graph)
                        val_y_tensor = torch.FloatTensor(val_y_graph)
                        A_hat_tensor = torch.FloatTensor(A_hat).to(device)
                        
                        st.write(f"è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                        st.write(f"éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                
                # åˆå§‹åŒ–æ¨¡å‹
                seq_len = X_train.shape[1]
                pred_len = 1
                num_features = X_train.shape[2]
                
                if model_type.startswith("LSTM"):
                    model = SimpleLSTM(
                        num_features=num_features,
                        hidden_dim=hidden_dim * 2,  # LSTM ç”¨æ›´å¤§çš„éšè—å±‚
                        num_layers=2
                    ).to(device)
                else:
                    model = STGCN(
                        num_nodes=num_nodes,
                        num_features=num_features,
                        seq_len=seq_len,
                        pred_len=pred_len,
                        Kt=3
                    ).to(device)
                
                st.write(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
                
                # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
                criterion = nn.MSELoss()
                optimizer = optim.Adam(model.parameters(), lr=learning_rate)
                
                # å­¦ä¹ ç‡è°ƒåº¦å™¨ (éªŒè¯æŸå¤±ä¸ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡)
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode='min', factor=0.5, patience=10
                )
                
                # æ—©åœå‚æ•°
                early_stop_patience = 20
                early_stop_counter = 0
                
                # è®­ç»ƒå¾ªç¯
                progress_bar = st.progress(0)
                status_text = st.empty()
                metrics_placeholder = st.empty()
                
                # å­˜å‚¨æŸå¤±å†å²
                train_losses = []
                val_losses = []
                best_val_loss = float('inf')
                
                # åˆ›å»ºDataLoader (è®­ç»ƒå’ŒéªŒè¯éƒ½ä½¿ç”¨æ‰¹å¤„ç†)
                from torch.utils.data import TensorDataset, DataLoader
                train_dataset = TensorDataset(train_X_tensor, train_y_tensor)
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                
                val_dataset = TensorDataset(val_X_tensor, val_y_tensor)
                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                
                start_time = time.time()
                
                for epoch in range(epochs):
                    # è®­ç»ƒé˜¶æ®µ
                    model.train()
                    epoch_train_loss = 0
                    batch_count = 0
                    
                    for batch_X, batch_y in train_loader:
                        # å°†æ•°æ®ç§»åˆ°GPU
                        batch_X = batch_X.to(device)
                        batch_y = batch_y.to(device)
                        
                        optimizer.zero_grad()
                        
                        # å‰å‘ä¼ æ’­
                        if model_type.startswith("LSTM"):
                            outputs = model(batch_X)  # (B, 1)
                            loss = criterion(outputs, batch_y)
                        else:
                            outputs = model(batch_X, A_hat_tensor)  # (B, 1, N, 1)
                            loss = criterion(outputs, batch_y)
                        
                        # åå‘ä¼ æ’­
                        loss.backward()
                        optimizer.step()
                        
                        epoch_train_loss += loss.item()
                        batch_count += 1
                    
                    avg_train_loss = epoch_train_loss / batch_count
                    train_losses.append(avg_train_loss)
                    
                    # éªŒè¯é˜¶æ®µ (ä½¿ç”¨æ‰¹å¤„ç†é¿å…æ˜¾å­˜æº¢å‡º)
                    model.eval()
                    val_loss_sum = 0
                    all_preds = []
                    all_targets = []
                    val_batch_count = 0
                    
                    with torch.no_grad():
                        for val_batch_X, val_batch_y in val_loader:
                            val_batch_X = val_batch_X.to(device)
                            val_batch_y = val_batch_y.to(device)
                            
                            if model_type.startswith("LSTM"):
                                val_batch_outputs = model(val_batch_X)
                            else:
                                val_batch_outputs = model(val_batch_X, A_hat_tensor)
                            
                            # ç´¯ç§¯æŸå¤±ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰
                            batch_loss = criterion(val_batch_outputs, val_batch_y).item()
                            val_loss_sum += batch_loss * len(val_batch_X)
                            
                            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºåç»­è®¡ç®—
                            all_preds.append(val_batch_outputs.cpu())
                            all_targets.append(val_batch_y.cpu())
                            
                            val_batch_count += len(val_batch_X)
                            
                            # æ¸…ç†æ˜¾å­˜
                            del val_batch_X, val_batch_y, val_batch_outputs
                            torch.cuda.empty_cache()
                        
                        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
                        all_preds = torch.cat(all_preds, dim=0)
                        all_targets = torch.cat(all_targets, dim=0)
                        
                        # åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦ (MinMax åå˜æ¢)
                        all_preds_original = all_preds * y_range + y_min
                        all_targets_original = all_targets * y_range + y_min
                        
                        # è®¡ç®—åŸå§‹å°ºåº¦çš„æŒ‡æ ‡
                        mae = torch.mean(torch.abs(all_preds_original - all_targets_original)).item()
                        rmse = torch.sqrt(torch.mean((all_preds_original - all_targets_original)**2)).item()
                        
                        # RÂ² (åœ¨åŸå§‹å°ºåº¦è®¡ç®—)
                        y_mean_original = torch.mean(all_targets_original)
                        ss_tot = torch.sum((all_targets_original - y_mean_original)**2)
                        ss_res = torch.sum((all_targets_original - all_preds_original)**2)
                        r2 = (1 - ss_res / ss_tot).item() if ss_tot > 0 else 0
                        
                        # è®¡ç®—å¹³å‡æŸå¤±
                        val_loss = val_loss_sum / val_batch_count
                        val_losses.append(val_loss)
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        torch.save(model.state_dict(), 'best_stgcn_model.pth')
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                    
                    # å­¦ä¹ ç‡è°ƒåº¦
                    scheduler.step(val_loss)
                    current_lr = optimizer.param_groups[0]['lr']
                    
                    # æ—©åœæ£€æŸ¥
                    if early_stop_counter >= early_stop_patience:
                        st.warning(f"âš ï¸ éªŒè¯æŸå¤±è¿ç»­ {early_stop_patience} è½®æœªæ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                        break
                    
                    # æ›´æ–°è¿›åº¦
                    progress = (epoch + 1) / epochs
                    progress_bar.progress(progress)
                    
                    elapsed = time.time() - start_time
                    eta = elapsed / (epoch + 1) * (epochs - epoch - 1)
                    
                    status_text.text(
                        f"Epoch {epoch+1}/{epochs} | "
                        f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | "
                        f"éªŒè¯æŸå¤±: {val_loss:.4f} | "
                        f"å­¦ä¹ ç‡: {current_lr:.6f} | "
                        f"å·²ç”¨æ—¶: {elapsed:.1f}s | ETA: {eta:.1f}s"
                    )
                    
                    # æ¯10ä¸ªepochæ›´æ–°ä¸€æ¬¡æŒ‡æ ‡
                    if (epoch + 1) % 10 == 0 or epoch == 0:
                        metrics_placeholder.markdown(f"""
                        ### å½“å‰æŒ‡æ ‡
                        - **è®­ç»ƒæŸå¤±**: {avg_train_loss:.6f}
                        - **éªŒè¯æŸå¤±**: {val_loss:.6f}
                        - **MAE**: {mae:.4f} MPa
                        - **RMSE**: {rmse:.4f} MPa
                        - **RÂ²**: {r2:.4f}
                        """)
                
                # è®­ç»ƒå®Œæˆ
                st.success("âœ… è®­ç»ƒå®Œæˆï¼")
                st.balloons()
                
                # ç»˜åˆ¶æŸå¤±æ›²çº¿
                st.subheader("ğŸ“ˆ è®­ç»ƒå†å²")
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
                ax.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss (MSE)')
                ax.set_title('è®­ç»ƒè¿‡ç¨‹')
                ax.legend()
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                
                # æœ€ç»ˆè¯„ä¼°
                st.subheader("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æœ€ä½³éªŒè¯æŸå¤±", f"{best_val_loss:.6f}")
                with col2:
                    st.metric("MAE", f"{mae:.4f} MPa")
                with col3:
                    st.metric("RMSE", f"{rmse:.4f} MPa")
                with col4:
                    st.metric("RÂ²", f"{r2:.4f}")
                
                st.info("æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: best_stgcn_model.pth")
                
                # é¢„æµ‹ç¤ºä¾‹ (ä½¿ç”¨å°æ‰¹æ¬¡é¿å…æ˜¾å­˜æº¢å‡º)
                st.subheader("ğŸ”® é¢„æµ‹ç¤ºä¾‹")
                model.load_state_dict(torch.load('best_stgcn_model.pth'))
                model.eval()
                
                # éšæœºé€‰æ‹©å‡ ä¸ªéªŒè¯æ ·æœ¬
                num_examples = min(5, len(val_X_tensor))
                indices = np.random.choice(len(val_X_tensor), num_examples, replace=False)
                
                with torch.no_grad():
                    example_X = val_X_tensor[indices].to(device)
                    example_y_true = val_y_tensor[indices].to(device)
                    
                    if model_type.startswith("LSTM"):
                        example_y_pred = model(example_X).unsqueeze(1)  # (B, 1)
                    else:
                        example_y_pred = model(example_X, A_hat_tensor)
                
                # åˆ›å»ºå¯¹æ¯”è¡¨
                comparison_data = []
                for i, idx in enumerate(indices):
                    sup_id = val_support_ids[idx]
                    
                    if model_type.startswith("LSTM"):
                        # LSTM: ç›´æ¥è¾“å‡ºæ ‡é‡
                        true_val_normalized = example_y_true[i, 0].cpu().item()
                        pred_val_normalized = example_y_pred[i, 0].cpu().item()
                    else:
                        # STGCN: ä»å›¾ç»“æ„ä¸­æå–
                        node_idx = support_to_idx[sup_id]
                        true_val_normalized = example_y_true[i, 0, node_idx, 0].cpu().item()
                        pred_val_normalized = example_y_pred[i, 0, node_idx, 0].cpu().item()
                    
                    # åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦ (MinMax åå˜æ¢)
                    true_val = true_val_normalized * y_range + y_min
                    pred_val = pred_val_normalized * y_range + y_min
                    error = abs(pred_val - true_val)
                    
                    comparison_data.append({
                        'æ”¯æ¶ç¼–å·': sup_id,
                        'çœŸå®å€¼ (MPa)': f"{true_val:.2f}",
                        'é¢„æµ‹å€¼ (MPa)': f"{pred_val:.2f}",
                        'è¯¯å·® (MPa)': f"{error:.2f}",
                        'è¯¯å·®ç‡': f"{error/abs(true_val)*100:.1f}%" if abs(true_val) > 1e-6 else "N/A"
                    })
                
                st.table(pd.DataFrame(comparison_data))
                
                # æ¸…ç†æ˜¾å­˜
                del example_X, example_y_true, example_y_pred
                torch.cuda.empty_cache()
                
            except Exception as e:
                st.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
                import traceback
                st.code(traceback.format_exc())
        
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        st.code(traceback.format_exc())

elif data_source == "ä¸Šä¼ CSVæ–‡ä»¶" and data_file:
    st.header("1. æ•°æ®åŠ è½½ä¸å¯¹é½")
    
    # è½½å…¥æ•°æ®
    try:
        # åŠ è½½çŸ¿å‹æ•°æ®
        data, column_names = load_csv_data(data_file)
        
        st.write(f"**çŸ¿å‹æ•°æ®å½¢çŠ¶:** {data.shape}")
        st.write(f"- æ—¶é—´æ­¥æ•°: {data.shape[0]}")
        st.write(f"- æ”¯æ¶æ•°é‡: {data.shape[1]}")
        st.write(f"- ç‰¹å¾æ•°: {data.shape[2]}")
        
        NUM_SAMPLES, NUM_NODES, NUM_FEATURES = data.shape
        
        # æ˜¾ç¤ºæ”¯æ¶åˆ—è¡¨
        with st.expander("ğŸ“‹ æŸ¥çœ‹æ”¯æ¶åˆ—è¡¨"):
            st.write(column_names)
        
        # åæ ‡å¯¹é½
        coords_array = None
        if coord_file:
            st.subheader("ğŸ—ºï¸ åæ ‡å¯¹é½")
            try:
                coord_df = load_coordinate_file(coord_file)
                st.write("**åæ ‡æ–‡ä»¶é¢„è§ˆ:**")
                st.dataframe(coord_df.head(10))
                
                # å¯¹é½åæ ‡
                coords_array, alignment_info = align_data_with_coordinates(column_names, coord_df)
                
                # æ˜¾ç¤ºå¯¹é½ç»“æœ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»æ”¯æ¶æ•°", alignment_info['total_supports'])
                with col2:
                    st.metric("æˆåŠŸåŒ¹é…", alignment_info['matched'], 
                             delta=f"{alignment_info['matched']/alignment_info['total_supports']*100:.1f}%")
                with col3:
                    st.metric("ç»´åº¦", "3D" if alignment_info['has_z'] else "2D")
                
                if alignment_info['missing']:
                    st.warning(f"âš ï¸ ä»¥ä¸‹ {len(alignment_info['missing'])} ä¸ªæ”¯æ¶æœªæ‰¾åˆ°åæ ‡: {', '.join(alignment_info['missing'][:5])}" + 
                              ("..." if len(alignment_info['missing']) > 5 else ""))
                else:
                    st.success("âœ… æ‰€æœ‰æ”¯æ¶åæ ‡å¯¹é½æˆåŠŸ!")
                
                # å¯è§†åŒ–æ”¯æ¶åˆ†å¸ƒ
                st.subheader("æ”¯æ¶ç©ºé—´åˆ†å¸ƒ")
                fig_scatter_data = pd.DataFrame({
                    'Xåæ ‡': coords_array[:, 0],
                    'Yåæ ‡': coords_array[:, 1],
                    'æ”¯æ¶': column_names
                })
                st.scatter_chart(fig_scatter_data, x='Xåæ ‡', y='Yåæ ‡', size=20)
                
            except Exception as e:
                st.error(f"åæ ‡å¯¹é½å¤±è´¥: {e}")
                st.info("å°†ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„åæ ‡")
        else:
            st.warning("âš ï¸ æœªä¸Šä¼ åæ ‡æ–‡ä»¶,å°†ä½¿ç”¨çº¿æ€§æ’åˆ—çš„é»˜è®¤åæ ‡")
            # ç”Ÿæˆé»˜è®¤åæ ‡(çº¿æ€§æ’åˆ—)
            coords_array = np.column_stack([np.arange(NUM_NODES), np.zeros(NUM_NODES)])
        
        # åœ°è´¨ç‰¹å¾èåˆ
        geo_features = None
        if use_geological and geo_file and coords_array is not None:
            st.subheader("ğŸŒ åœ°è´¨ç‰¹å¾èåˆ")
            try:
                geo_features, feature_names = load_geological_features(geo_file, coords_array)
                st.write(f"**åœ°è´¨ç‰¹å¾å½¢çŠ¶:** {geo_features.shape}")
                st.write(f"**ç‰¹å¾åç§°:** {feature_names}")
                
                # æ˜¾ç¤ºåœ°è´¨ç‰¹å¾ç»Ÿè®¡
                geo_df_display = pd.DataFrame(geo_features, columns=feature_names)
                st.write("**åœ°è´¨ç‰¹å¾ç»Ÿè®¡:**")
                st.dataframe(geo_df_display.describe())
                
                # å°†åœ°è´¨ç‰¹å¾æ·»åŠ åˆ°æ•°æ®ä¸­
                # å°†åœ°è´¨ç‰¹å¾æ‰©å±•åˆ°æ‰€æœ‰æ—¶é—´æ­¥
                geo_features_expanded = np.tile(geo_features[np.newaxis, :, :], (NUM_SAMPLES, 1, 1))
                # (num_samples, num_nodes, geo_features)
                
                # åˆå¹¶çŸ¿å‹æ•°æ®å’Œåœ°è´¨ç‰¹å¾
                data = np.concatenate([data, geo_features_expanded], axis=-1)
                NUM_FEATURES = data.shape[2]
                
                st.success(f"âœ… åœ°è´¨ç‰¹å¾å·²èåˆ! æ€»ç‰¹å¾æ•°: {NUM_FEATURES}")
                
            except Exception as e:
                st.error(f"åœ°è´¨ç‰¹å¾åŠ è½½å¤±è´¥: {e}")
                st.info("å°†ä»…ä½¿ç”¨çŸ¿å‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
        
        # ç”Ÿæˆæˆ–åŠ è½½é‚»æ¥çŸ©é˜µ
        st.header("2. å›¾ç»“æ„æ„å»º")
        if adj_method == "upload" and adj_file:
            # ä¸Šä¼ è‡ªå®šä¹‰é‚»æ¥çŸ©é˜µ
            if adj_file.name.endswith('.npy'):
                adj_mx = np.load(adj_file)
            else:  # CSV
                adj_mx = pd.read_csv(adj_file).values
            st.write(f"**é‚»æ¥çŸ©é˜µ (ä¸Šä¼ ) å½¢çŠ¶:** {adj_mx.shape}")
        else:
            # è‡ªåŠ¨ç”Ÿæˆé‚»æ¥çŸ©é˜µ
            if adj_method == "knn" and coords_array is not None:
                # ä½¿ç”¨çœŸå®åæ ‡ç”Ÿæˆ KNN
                adj_params['coords'] = coords_array
                st.info(f"âœ… ä½¿ç”¨çœŸå®æ”¯æ¶åæ ‡ç”Ÿæˆ K={adj_params.get('k', 3)} è¿‘é‚»å›¾")
            elif adj_method == "distance" and coords_array is not None:
                # ä½¿ç”¨è·ç¦»é˜ˆå€¼
                adj_params['coords'] = coords_array
                threshold = st.slider("è·ç¦»é˜ˆå€¼ (å•ä½:ç±³)", 1.0, 100.0, 10.0)
                adj_params['threshold'] = threshold
                st.info(f"âœ… ä½¿ç”¨çœŸå®åæ ‡ç”Ÿæˆè·ç¦»å›¾ (é˜ˆå€¼={threshold}ç±³)")
            elif adj_method in ["knn", "distance"] and coords_array is None:
                st.warning("âš ï¸ éœ€è¦åæ ‡æ–‡ä»¶æ‰èƒ½ä½¿ç”¨è·ç¦»ç›¸å…³æ–¹æ³•,å°†ä½¿ç”¨éšæœºåæ ‡")
                adj_params['coords'] = np.random.rand(NUM_NODES, 2)
            
            adj_mx = generate_adjacency_matrix(NUM_NODES, adj_method, **adj_params)
            st.write(f"**é‚»æ¥çŸ©é˜µ (è‡ªåŠ¨ç”Ÿæˆ):** {adj_mx.shape}")
            st.info(f"ä½¿ç”¨ **{adj_method}** æ–¹æ³•ç”Ÿæˆé‚»æ¥çŸ©é˜µ")
        
        # éªŒè¯é‚»æ¥çŸ©é˜µ
        if NUM_NODES != adj_mx.shape[0] or NUM_NODES != adj_mx.shape[1]:
            st.error(f"æ•°æ®ä¸é‚»æ¥çŸ©é˜µçš„èŠ‚ç‚¹æ•°ä¸åŒ¹é…! (æ•°æ®: {NUM_NODES}, é‚»æ¥çŸ©é˜µ: {adj_mx.shape[0]})")
        else:
            st.success("æ•°æ®æ–‡ä»¶å’Œé‚»æ¥çŸ©é˜µåŠ è½½/ç”ŸæˆæˆåŠŸ!")
            
            # æ˜¾ç¤ºé‚»æ¥çŸ©é˜µçš„è¿æ¥ç»Ÿè®¡
            num_edges = np.sum(adj_mx) / 2  # é™¤ä»¥2å› ä¸ºæ˜¯æ— å‘å›¾
            st.write(f"**å›¾ç»“æ„ç»Ÿè®¡:**")
            st.write(f"- æ€»è¾¹æ•°: {int(num_edges)}")
            st.write(f"- å¹³å‡åº¦æ•°: {np.sum(adj_mx, axis=1).mean():.2f}")
            
            # å¯è§†åŒ–é‚»æ¥çŸ©é˜µ
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("é‚»æ¥çŸ©é˜µå¯è§†åŒ–")
                st.image(adj_mx, use_container_width=True, clamp=True, caption="ç™½è‰²=è¿æ¥,é»‘è‰²=æ— è¿æ¥")
            
            with col2:
                st.subheader("èŠ‚ç‚¹ 0 çš„æ•°æ®é¢„è§ˆ")
                chart_data = pd.DataFrame(data[:min(500, len(data)), 0, 0], columns=['Node 0, Feature 0'])
                st.line_chart(chart_data)

            # --- è®­ç»ƒæ¨¡å— ---
            st.header("3. æ¨¡å‹è®­ç»ƒ")
            if st.button("å¼€å§‹è®­ç»ƒ"):
                
                with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®..."):
                    # 1. è®¡ç®— A_hat
                    A_hat = calculate_normalized_laplacian(adj_mx)
                    
                    # 2. ç”Ÿæˆ Dataloaders
                    train_loader, val_loader, test_loader, scaler = generate_dataloader(
                        data, BATCH_SIZE, SEQ_LEN, PRED_LEN
                    )
                    
                    # 3. åˆå§‹åŒ–æ¨¡å‹
                    device = torch.device(DEVICE)
                    model = STGCN(NUM_NODES, NUM_FEATURES, SEQ_LEN, PRED_LEN).to(device)
                    optimizer = optim.Adam(model.parameters(), lr=LR)
                    loss_fn = nn.MSELoss()
                    
                    st.success("æ¨¡å‹å’Œæ•°æ®åˆå§‹åŒ–å®Œæˆï¼å¼€å§‹è®­ç»ƒ...")
                
                # å‡†å¤‡å®æ—¶æ˜¾ç¤º
                status_placeholder = st.empty()
                loss_chart_placeholder = st.empty()
                loss_df = pd.DataFrame(columns=["Epoch", "Train Loss", "Val Loss"])

                start_time = time.time()
                
                for epoch in range(1, EPOCHS + 1):
                    # è®­ç»ƒ
                    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device, A_hat)
                    
                    # éªŒè¯
                    val_loss = evaluate(model, val_loader, loss_fn, device, A_hat)
                    
                    # æ›´æ–°çŠ¶æ€
                    elapsed = time.time() - start_time
                    status_text = f"""
                    **Epoch: {epoch}/{EPOCHS}**
                    - è®­ç»ƒæŸå¤± (Train Loss): {train_loss:.6f}
                    - éªŒè¯æŸå¤± (Val Loss): {val_loss:.6f}
                    - å·²ç”¨æ—¶é—´ (Elapsed): {elapsed:.2f}s
                    """
                    status_placeholder.markdown(status_text)
                    
                    # æ›´æ–°å›¾è¡¨
                    new_loss_row = pd.DataFrame({
                        "Epoch": [epoch],
                        "Train Loss": [train_loss],
                        "Val Loss": [val_loss]
                    })
                    loss_df = pd.concat([loss_df, new_loss_row], ignore_index=True)
                    loss_chart_placeholder.line_chart(loss_df.set_index("Epoch"))

                st.success("æ¨¡å‹è®­ç»ƒå®Œæˆ!")
                
                # --- ç»“æœå±•ç¤º ---
                st.header("4. è®­ç»ƒç»“æœ")
                st.subheader("æœ€ç»ˆæŸå¤±æ›²çº¿")
                st.line_chart(loss_df.set_index("Epoch"))
                
                st.subheader("æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç° (éšæœºæŠ½æ ·)")
                try:
                    # ä»æµ‹è¯•é›†è·å–ä¸€ä¸ªæ‰¹æ¬¡
                    x_test_batch, y_test_batch = next(iter(test_loader))
                    x_test_batch, y_test_batch = x_test_batch.to(device), y_test_batch.to(device)
                    A_hat_tensor = torch.tensor(A_hat).to(device)
                    
                    model.eval()
                    with torch.no_grad():
                        y_pred = model(x_test_batch, A_hat_tensor) # (B, T_out, N, F_out)
                        y_pred = y_pred.permute(0, 3, 2, 1) # (B, F_out, N, T_out)

                    # åå½’ä¸€åŒ–
                    y_pred_real = (y_pred.cpu().numpy() * scaler['std']) + scaler['mean']
                    y_test_real = (y_test_batch.cpu().numpy() * scaler['std']) + scaler['mean']
                    
                    # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬å’Œä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œæ¯”è¾ƒ (Batch 0, Node 0)
                    pred_series = y_pred_real[0, :, 0, 0] # é¢„æµ‹å€¼
                    true_series = y_test_real[0, :, 0, 0] # çœŸå®å€¼
                    
                    if PRED_LEN == 1:
                        st.write("é¢„æµ‹å€¼ (Test Sample 0, Node 0):", pred_series[0])
                        st.write("çœŸå®å€¼ (Test Sample 0, Node 0):", true_series[0])
                    else:
                        result_df = pd.DataFrame({
                            'Predicted': pred_series,
                            'True': true_series
                        }, index=[f't+{i+1}' for i in range(PRED_LEN)])
                        
                        st.dataframe(result_df)
                        st.line_chart(result_df)
                        
                except Exception as e:
                    st.error(f"åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ—¶å‡ºé”™: {e}")

    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        st.error("è¯·ç¡®ä¿æ‚¨ä¸Šä¼ äº†æ­£ç¡®æ ¼å¼çš„ CSV æ–‡ä»¶ã€‚")
        st.info("""
        **CSV æ–‡ä»¶æ ¼å¼æç¤º:**
        - æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ—¶é—´ç‚¹
        - æ¯åˆ—ä»£è¡¨ä¸€ä¸ªæ”¯æ¶(ç›‘æµ‹ç‚¹)
        - å¯ä»¥åŒ…å«æ—¶é—´åˆ—(ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶ç§»é™¤)
        """)
else:
    st.info("è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„ CSV çŸ¿å‹æ•°æ®æ–‡ä»¶ä»¥å¼€å§‹ã€‚")