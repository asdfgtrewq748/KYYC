import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import math
import time
import warnings
from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt

# å¿½ç•¥ CUDA å…¼å®¹æ€§è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, message='.*CUDA capability.*')
warnings.filterwarnings('ignore', category=UserWarning, message='.*NVIDIA.*')

# è®¾ç½® CUDA ç¯å¢ƒ
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

# è§£å†³ OpenMP å†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# ----------------------------------------------------------------------
# 1. å¸®åŠ©å‡½æ•° (Utils)
# ----------------------------------------------------------------------

def add_engineered_features(X, feature_names=None):
    """
    æ·»åŠ å·¥ç¨‹ç‰¹å¾ï¼Œæå‡æ¨¡å‹è¡¨ç°
    :param X: è¾“å…¥æ•°æ® (samples, seq_len, features) æˆ– (T, N, seq_len, features)
    :param feature_names: åŸå§‹ç‰¹å¾ååˆ—è¡¨
    :return: å¢å¼ºåçš„X, æ–°ç‰¹å¾ååˆ—è¡¨
    """
    is_spatial = (X.ndim == 4)  # åˆ¤æ–­æ˜¯å¦ä¸ºæ—¶ç©ºæ•°æ®
    
    if is_spatial:
        T, N, seq_len, F = X.shape
        X_new_features = []
    else:
        samples, seq_len, F = X.shape
        X_new_features = []
    
    new_feature_names = feature_names.copy() if feature_names else [f'feat_{i}' for i in range(F)]
    
    # â­ å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹ - ç›®æ ‡RÂ²â‰¥0.8
    
    # 1. ç»Ÿè®¡ç‰¹å¾ï¼ˆé’ˆå¯¹æ—¶åºç»´åº¦ï¼‰- å¤šç§ç»Ÿè®¡é‡
    if is_spatial:
        for feat_idx in range(F):
            feat_data = X[:, :, :, feat_idx]  # (T, N, seq_len)
            
            # åŸºç¡€ç»Ÿè®¡
            feat_mean = feat_data.mean(axis=2, keepdims=True)
            feat_mean = np.repeat(feat_mean, seq_len, axis=2)
            X_new_features.append(feat_mean[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_mean')
            
            feat_std = feat_data.std(axis=2, keepdims=True)
            feat_std = np.repeat(feat_std, seq_len, axis=2)
            X_new_features.append(feat_std[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_std')
            
            # æå€¼ç‰¹å¾
            feat_max = feat_data.max(axis=2, keepdims=True)
            feat_max = np.repeat(feat_max, seq_len, axis=2)
            X_new_features.append(feat_max[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_max')
            
            feat_min = feat_data.min(axis=2, keepdims=True)
            feat_min = np.repeat(feat_min, seq_len, axis=2)
            X_new_features.append(feat_min[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_min')
            
            # â­ æ–°å¢ï¼šèŒƒå›´å’Œååº¦
            feat_range = feat_max - feat_min
            X_new_features.append(feat_range[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_range')
            
            # â­ æ–°å¢ï¼šå˜å¼‚ç³»æ•°ï¼ˆCVï¼‰
            feat_cv = feat_std / (feat_mean + 1e-8)
            X_new_features.append(feat_cv[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_cv')
    else:
        for feat_idx in range(F):
            feat_data = X[:, :, feat_idx]  # (samples, seq_len)
            
            # åŸºç¡€ç»Ÿè®¡
            feat_mean = feat_data.mean(axis=1, keepdims=True)
            feat_mean = np.repeat(feat_mean, seq_len, axis=1)
            X_new_features.append(feat_mean[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_mean')
            
            feat_std = feat_data.std(axis=1, keepdims=True)
            feat_std = np.repeat(feat_std, seq_len, axis=1)
            X_new_features.append(feat_std[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_std')
            
            # æå€¼
            feat_max = feat_data.max(axis=1, keepdims=True)
            feat_max = np.repeat(feat_max, seq_len, axis=1)
            X_new_features.append(feat_max[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_max')
            
            feat_min = feat_data.min(axis=1, keepdims=True)
            feat_min = np.repeat(feat_min, seq_len, axis=1)
            X_new_features.append(feat_min[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_min')
            
            # â­ æ–°å¢ç‰¹å¾
            feat_range = feat_max - feat_min
            X_new_features.append(feat_range[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_range')
            
            feat_cv = feat_std / (feat_mean + 1e-8)
            X_new_features.append(feat_cv[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_cv')
    
    # 2. å·®åˆ†ç‰¹å¾ï¼ˆå¤šé˜¶ï¼‰
    if is_spatial:
        for feat_idx in range(F):
            feat_data = X[:, :, :, feat_idx]
            
            # ä¸€é˜¶å·®åˆ†ï¼ˆå˜åŒ–ç‡ï¼‰
            feat_diff1 = np.diff(feat_data, axis=2)
            feat_diff1 = np.concatenate([np.zeros((T, N, 1)), feat_diff1], axis=2)
            X_new_features.append(feat_diff1[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff1')
            
            # â­ äºŒé˜¶å·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼‰
            feat_diff2 = np.diff(feat_diff1, axis=2)
            feat_diff2 = np.concatenate([np.zeros((T, N, 1)), feat_diff2], axis=2)
            X_new_features.append(feat_diff2[:, :, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff2')
    else:
        for feat_idx in range(F):
            feat_data = X[:, :, feat_idx]
            
            # ä¸€é˜¶å·®åˆ†
            feat_diff1 = np.diff(feat_data, axis=1)
            feat_diff1 = np.concatenate([np.zeros((samples, 1)), feat_diff1], axis=1)
            X_new_features.append(feat_diff1[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff1')
            
            # â­ äºŒé˜¶å·®åˆ†
            feat_diff2 = np.diff(feat_diff1, axis=1)
            feat_diff2 = np.concatenate([np.zeros((samples, 1)), feat_diff2], axis=1)
            X_new_features.append(feat_diff2[:, :, np.newaxis])
            new_feature_names.append(f'{new_feature_names[feat_idx]}_diff2')
    
    # â­ 3. æ»‘åŠ¨çª—å£ç‰¹å¾ï¼ˆçŸ­æœŸå’Œé•¿æœŸï¼‰
    if is_spatial:
        for feat_idx in range(F):
            feat_data = X[:, :, :, feat_idx]
            
            # çŸ­æœŸè¶‹åŠ¿ï¼ˆæœ€è¿‘3æ­¥ï¼‰
            if seq_len >= 3:
                feat_recent_mean = np.zeros_like(feat_data)
                for i in range(seq_len):
                    start = max(0, i - 2)
                    feat_recent_mean[:, :, i] = feat_data[:, :, start:i+1].mean(axis=2)
                X_new_features.append(feat_recent_mean[:, :, :, np.newaxis])
                new_feature_names.append(f'{new_feature_names[feat_idx]}_recent3_mean')
    else:
        for feat_idx in range(F):
            feat_data = X[:, :, feat_idx]
            
            if seq_len >= 3:
                feat_recent_mean = np.zeros_like(feat_data)
                for i in range(seq_len):
                    start = max(0, i - 2)
                    feat_recent_mean[:, i] = feat_data[:, start:i+1].mean(axis=1)
                X_new_features.append(feat_recent_mean[:, :, np.newaxis])
                new_feature_names.append(f'{new_feature_names[feat_idx]}_recent3_mean')
    
    # â­ 4. äº¤å‰ç‰¹å¾ï¼ˆé’ˆå¯¹ç¬¬ä¸€ä¸ªç‰¹å¾ï¼Œé€šå¸¸æ˜¯å‹åŠ›å€¼ï¼‰
    if F > 1 and is_spatial:
        # å‹åŠ›å€¼ä¸å…¶ä»–ç‰¹å¾çš„æ¯”å€¼
        pressure_data = X[:, :, :, 0]  # å‡è®¾ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯å‹åŠ›
        for feat_idx in range(1, min(F, 5)):  # åªå–å‰5ä¸ªç‰¹å¾é¿å…è¿‡å¤š
            other_data = X[:, :, :, feat_idx]
            ratio = pressure_data / (other_data + 1e-8)
            X_new_features.append(ratio[:, :, :, np.newaxis])
            new_feature_names.append(f'pressure_to_{new_feature_names[feat_idx]}_ratio')
    elif F > 1:
        pressure_data = X[:, :, 0]
        for feat_idx in range(1, min(F, 5)):
            other_data = X[:, :, feat_idx]
            ratio = pressure_data / (other_data + 1e-8)
            X_new_features.append(ratio[:, :, np.newaxis])
            new_feature_names.append(f'pressure_to_{new_feature_names[feat_idx]}_ratio')
    
    # åˆå¹¶åŸå§‹ç‰¹å¾å’Œæ–°ç‰¹å¾
    if is_spatial:
        X_enhanced = np.concatenate([X] + X_new_features, axis=3)
    else:
        X_enhanced = np.concatenate([X] + X_new_features, axis=2)
    
    return X_enhanced, new_feature_names

def load_csv_data(csv_file, time_col=None):
    """
    ä» CSV æ–‡ä»¶åŠ è½½çŸ¿å‹æ•°æ®
    :param csv_file: CSV æ–‡ä»¶å¯¹è±¡
    :param time_col: æ—¶é—´åˆ—åç§°(å¦‚æœæœ‰)
    :return: numpy array (num_samples, num_nodes, num_features), column_names
    """
    df = pd.read_csv(csv_file)
    
    # å¦‚æœæœ‰æ—¶é—´åˆ—,åˆ é™¤å®ƒ
    if time_col and time_col in df.columns:
        df = df.drop(columns=[time_col])
    
    # å°è¯•è‡ªåŠ¨æ£€æµ‹æ—¶é—´åˆ—(å¸¸è§åç§°)
    time_cols = ['æ—¶é—´', 'time', 'Time', 'DATE', 'date', 'datetime', 'Datetime', 'æ—¥æœŸ']
    for col in time_cols:
        if col in df.columns:
            df = df.drop(columns=[col])
            break
    
    # ä¿å­˜åˆ—å(æ”¯æ¶åç§°)
    column_names = df.columns.tolist()
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    data = df.values  # (num_samples, num_nodes)
    
    # æ·»åŠ ç‰¹å¾ç»´åº¦ (å‡è®¾åªæœ‰ä¸€ä¸ªç‰¹å¾:å‹åŠ›å€¼)
    data = np.expand_dims(data, axis=-1)  # (num_samples, num_nodes, 1)
    
    return data, column_names

def load_processed_sequence_data(npz_file):
    """
    åŠ è½½é¢„å¤„ç†å¥½çš„åºåˆ—æ•°æ®é›†
    :param npz_file: .npz æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡
    :return: X, y_final, support_ids, feature_names
    """
    if isinstance(npz_file, str):
        data = np.load(npz_file, allow_pickle=True)
    else:
        data = np.load(npz_file, allow_pickle=True)
    
    X = data['X']  # (num_samples, seq_len, num_features)
    y_final = data['y_final']  # (num_samples, pred_len)
    support_ids = data['support_ids']  # (num_samples,)
    feature_names = data['feature_names'].tolist() if 'feature_names' in data else []
    
    return X, y_final, support_ids, feature_names

def reconstruct_spatiotemporal_data(X, y_final, support_ids, num_supports=125):
    """
    å°†å•æ”¯æ¶åºåˆ—æ•°æ®é‡æ„ä¸ºå®Œæ•´çš„æ—¶ç©ºæ•°æ®
    è¿™æ˜¯è§£å†³RÂ²ä½çš„å…³é”®ï¼
    
    :param X: (num_samples, seq_len, num_features) - å•æ”¯æ¶åºåˆ—
    :param y_final: (num_samples,) - å•æ”¯æ¶ç›®æ ‡å€¼
    :param support_ids: (num_samples,) - æ”¯æ¶ID
    :param num_supports: æ”¯æ¶æ€»æ•°
    :return: X_spatial, y_spatial, valid_time_indices
    """
    import pandas as pd
    import streamlit as st
    
    # æ­¥éª¤1ï¼šç¡®å®šæ—¶é—´ç´¢å¼•ï¼ˆå‡è®¾æ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰
    # ç”±äºæ¯ä¸ªæ—¶é—´ç‚¹æœ‰125ä¸ªæ”¯æ¶ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾å‡ºæ—¶é—´æ­¥æ•°
    num_samples = len(X)
    samples_per_timestep = num_supports
    
    # è®¡ç®—å¯èƒ½çš„æ—¶é—´æ­¥æ•°
    num_timesteps = num_samples // samples_per_timestep
    
    # â­ æ·»åŠ è¯¦ç»†æ—¥å¿—
    st.info(f"""
    ğŸ”„ **æ—¶ç©ºæ•°æ®é‡æ„ä¸­...**
    - åŸå§‹æ ·æœ¬æ•°: {num_samples:,}
    - æ”¯æ¶æ•°: {num_supports}
    - é¢„æœŸæ—¶é—´æ­¥æ•°: {num_timesteps}
    - æ¯æ—¶é—´æ­¥æ ·æœ¬æ•°: {samples_per_timestep}
    """)
    
    # æ­¥éª¤2ï¼šåˆ›å»ºæ”¯æ¶IDåˆ°ç´¢å¼•çš„æ˜ å°„
    unique_supports = np.unique(support_ids)
    support_to_idx = {sup_id: idx for idx, sup_id in enumerate(sorted(unique_supports))}
    
    st.write(f"âœ“ æ‰¾åˆ° {len(unique_supports)} ä¸ªå”¯ä¸€æ”¯æ¶")
    
    seq_len = X.shape[1]
    num_features = X.shape[2]
    
    # æ­¥éª¤3ï¼šé‡æ„ä¸ºæ—¶ç©ºæ ¼å¼
    # æ–°æ ¼å¼ï¼š(num_timesteps, num_supports, seq_len, num_features)
    X_spatial = np.zeros((num_timesteps, num_supports, seq_len, num_features))
    y_spatial = np.zeros((num_timesteps, num_supports))
    
    # æ ‡è®°å“ªäº›ä½ç½®æœ‰æœ‰æ•ˆæ•°æ®
    valid_mask = np.zeros((num_timesteps, num_supports), dtype=bool)
    
    # æ­¥éª¤4ï¼šå¡«å……æ•°æ®
    for i in range(num_samples):
        support_id = support_ids[i]
        support_idx = support_to_idx.get(support_id, None)
        
        if support_idx is None:
            continue
        
        # è®¡ç®—è¯¥æ ·æœ¬å±äºå“ªä¸ªæ—¶é—´æ­¥
        time_idx = i // samples_per_timestep
        
        if time_idx >= num_timesteps:
            break
        
        X_spatial[time_idx, support_idx, :, :] = X[i]
        y_spatial[time_idx, support_idx] = y_final[i]
        valid_mask[time_idx, support_idx] = True
    
    # æ­¥éª¤5ï¼šæ‰¾å‡ºæ‰€æœ‰æ”¯æ¶éƒ½æœ‰æ•°æ®çš„æ—¶é—´ç‚¹ï¼ˆå®Œæ•´æ—¶é—´æ­¥ï¼‰
    complete_timesteps = valid_mask.sum(axis=1) == num_supports
    valid_time_indices = np.where(complete_timesteps)[0]
    
    st.write(f"âœ“ æ‰¾åˆ° {len(valid_time_indices)} ä¸ªå®Œæ•´æ—¶é—´æ­¥ï¼ˆæ‰€æœ‰æ”¯æ¶éƒ½æœ‰æ•°æ®ï¼‰")
    
    # â­ æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å®Œæ•´æ—¶é—´æ­¥
    if len(valid_time_indices) < 10:
        st.warning(f"""
        âš ï¸ **å®Œæ•´æ—¶é—´æ­¥æ•°é‡è¾ƒå°‘ ({len(valid_time_indices)})ï¼**
        
        **å¯èƒ½åŸå› ï¼š**
        1. æ•°æ®ä¸­ä¸åŒæ”¯æ¶çš„æ—¶é—´ç‚¹ä¸å¯¹é½
        2. éƒ¨åˆ†æ”¯æ¶ç¼ºå°‘æ•°æ®
        3. æ”¯æ¶æ•°é‡ä¸å®é™…ä¸ç¬¦ï¼ˆé¢„æœŸ{num_supports}ä¸ªï¼‰
        
        **å»ºè®®ï¼š**
        - å¦‚æœ<10ä¸ªæ—¶é—´æ­¥ï¼š**å¼ºçƒˆå»ºè®®ä½¿ç”¨"å•æ ·æœ¬åºåˆ—æ ¼å¼"**
        - å¦‚æœ10-100ä¸ªï¼šå¯ä»¥å°è¯•ï¼Œä½†æ•ˆæœå¯èƒ½å—é™
        - å¦‚æœ>100ä¸ªï¼šæ•ˆæœè¾ƒå¥½
        
        å½“å‰ä¼šç»§ç»­å¤„ç†ï¼Œä½†å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡ã€‚
        """)
    
    # åªä¿ç•™å®Œæ•´çš„æ—¶é—´æ­¥
    X_spatial_complete = X_spatial[valid_time_indices]
    y_spatial_complete = y_spatial[valid_time_indices]
    
    st.success(f"""
    âœ… **æ—¶ç©ºæ•°æ®é‡æ„å®Œæˆï¼**
    - è¾“å‡ºå½¢çŠ¶: {X_spatial_complete.shape}
    - ç›®æ ‡å½¢çŠ¶: {y_spatial_complete.shape}
    - æ•°æ®å®Œæ•´æ€§: {len(valid_time_indices)}/{num_timesteps} ({len(valid_time_indices)/num_timesteps*100:.1f}%)
    """)
    
    return X_spatial_complete, y_spatial_complete, valid_time_indices, support_to_idx

def load_coordinate_file(coord_file):
    """
    åŠ è½½æ”¯æ¶åæ ‡æ–‡ä»¶
    :param coord_file: åæ ‡æ–‡ä»¶å¯¹è±¡ (CSV æˆ– Excel)
    :return: DataFrame with columns [æ”¯æ¶ID/åç§°, Xåæ ‡, Yåæ ‡, (å¯é€‰)Zåæ ‡]
    """
    if coord_file.name.endswith('.csv'):
        df = pd.read_csv(coord_file)
    elif coord_file.name.endswith(('.xls', '.xlsx')):
        df = pd.read_excel(coord_file)
    else:
        raise ValueError("åæ ‡æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ,è¯·ä½¿ç”¨ CSV æˆ– Excel æ ¼å¼")
    
    return df

def align_data_with_coordinates(column_names, coord_df):
    """
    å¯¹é½çŸ¿å‹æ•°æ®å’Œåæ ‡æ•°æ®
    :param column_names: çŸ¿å‹æ•°æ®çš„åˆ—å(æ”¯æ¶åç§°)
    :param coord_df: åæ ‡æ•°æ®DataFrame
    :return: å¯¹é½åçš„åæ ‡æ•°ç»„ (num_nodes, 2 æˆ– 3), å¯¹é½ä¿¡æ¯
    """
    # å°è¯•æ‰¾åˆ°åæ ‡DataFrameä¸­çš„æ”¯æ¶IDåˆ—
    possible_id_cols = ['æ”¯æ¶ID', 'æ”¯æ¶ç¼–å·', 'æ”¯æ¶åç§°', 'ID', 'id', 'Name', 'name', 'ç¼–å·']
    id_col = None
    for col in possible_id_cols:
        if col in coord_df.columns:
            id_col = col
            break
    
    if id_col is None:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°,ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºIDåˆ—
        id_col = coord_df.columns[0]
    
    # å°è¯•æ‰¾åˆ°X,Yåæ ‡åˆ—
    possible_x_cols = ['X', 'x', 'Xåæ ‡', 'xåæ ‡', 'lon', 'longitude', 'ç»åº¦']
    possible_y_cols = ['Y', 'y', 'Yåæ ‡', 'yåæ ‡', 'lat', 'latitude', 'çº¬åº¦']
    possible_z_cols = ['Z', 'z', 'Zåæ ‡', 'zåæ ‡', 'elevation', 'é«˜ç¨‹']
    
    x_col = next((col for col in possible_x_cols if col in coord_df.columns), None)
    y_col = next((col for col in possible_y_cols if col in coord_df.columns), None)
    z_col = next((col for col in possible_z_cols if col in coord_df.columns), None)
    
    if x_col is None or y_col is None:
        raise ValueError("æ— æ³•è¯†åˆ«åæ ‡åˆ—,è¯·ç¡®ä¿åæ ‡æ–‡ä»¶åŒ…å« X å’Œ Y åˆ—")
    
    # åˆ›å»ºåæ ‡å­—å…¸
    coord_dict = {}
    for _, row in coord_df.iterrows():
        support_id = str(row[id_col]).strip()
        if z_col:
            coord_dict[support_id] = [row[x_col], row[y_col], row[z_col]]
        else:
            coord_dict[support_id] = [row[x_col], row[y_col]]
    
    # å¯¹é½åæ ‡
    aligned_coords = []
    missing_coords = []
    
    for col_name in column_names:
        col_name_str = str(col_name).strip()
        if col_name_str in coord_dict:
            aligned_coords.append(coord_dict[col_name_str])
        else:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…
            matched = False
            for key in coord_dict.keys():
                if col_name_str in key or key in col_name_str:
                    aligned_coords.append(coord_dict[key])
                    matched = True
                    break
            if not matched:
                missing_coords.append(col_name_str)
                # ä½¿ç”¨é»˜è®¤åæ ‡æˆ–è·³è¿‡
                if len(aligned_coords) > 0:
                    aligned_coords.append(aligned_coords[-1])  # ä½¿ç”¨ä¸Šä¸€ä¸ªåæ ‡
                else:
                    aligned_coords.append([0, 0] if z_col is None else [0, 0, 0])
    
    coords_array = np.array(aligned_coords)
    
    alignment_info = {
        'total_supports': len(column_names),
        'matched': len(column_names) - len(missing_coords),
        'missing': missing_coords,
        'has_z': z_col is not None
    }
    
    return coords_array, alignment_info

def load_geological_features(geo_file, coords_array, column_names=None):
    """
    åŠ è½½åœ°è´¨ç‰¹å¾æ•°æ®å¹¶æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
    :param geo_file: åœ°è´¨ç‰¹å¾æ–‡ä»¶ (CSV, Excel, æˆ–è·¯å¾„å­—ç¬¦ä¸²)
    :param coords_array: æ”¯æ¶åæ ‡æ•°ç»„ (num_nodes, 2 æˆ– 3)
    :param column_names: æ”¯æ¶åç§°åˆ—è¡¨(å¯é€‰,ç”¨äºç›´æ¥åŒ¹é…é’»å­”åç§°)
    :return: åœ°è´¨ç‰¹å¾çŸ©é˜µ (num_nodes, num_geo_features), ç‰¹å¾åˆ—å
    """
    # æ”¯æŒæ–‡ä»¶å¯¹è±¡æˆ–è·¯å¾„å­—ç¬¦ä¸²
    if isinstance(geo_file, str):
        if geo_file.endswith('.csv'):
            geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        elif geo_file.endswith(('.xls', '.xlsx')):
            geo_df = pd.read_excel(geo_file)
        else:
            raise ValueError("åœ°è´¨æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")
    else:
        if geo_file.name.endswith('.csv'):
            geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        elif geo_file.name.endswith(('.xls', '.xlsx')):
            geo_df = pd.read_excel(geo_file)
        else:
            raise ValueError("åœ°è´¨æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")
    
    from scipy.spatial import KDTree
    
    # æå–åœ°è´¨ç‚¹åæ ‡
    x_col = next((col for col in ['x', 'X', 'Xåæ ‡', 'åæ ‡x'] if col in geo_df.columns), None)
    y_col = next((col for col in ['y', 'Y', 'Yåæ ‡', 'åæ ‡y'] if col in geo_df.columns), None)
    
    if x_col is None or y_col is None:
        raise ValueError("åœ°è´¨æ–‡ä»¶å¿…é¡»åŒ…å« X å’Œ Y åæ ‡åˆ—")
    
    geo_coords = geo_df[[x_col, y_col]].values
    
    # æå–åœ°è´¨ç‰¹å¾åˆ—(æ’é™¤åæ ‡ã€é’»å­”åç­‰IDåˆ—)
    exclude_cols = [x_col, y_col, 'borehole', 'é’»å­”å', 'id', 'ID', 'name']
    feature_cols = [col for col in geo_df.columns if col not in exclude_cols]
    
    # å¦‚æœæ²¡æœ‰æ•°å€¼ç‰¹å¾,è¿”å›ç©ºæ•°ç»„
    if len(feature_cols) == 0:
        return np.zeros((len(coords_array), 1)), ['dummy_feature']
    
    geo_features = geo_df[feature_cols].values
    
    # å¡«å……ç¼ºå¤±å€¼
    geo_features = np.nan_to_num(geo_features, nan=0.0)
    
    # ä½¿ç”¨ KNN æ’å€¼æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
    tree = KDTree(geo_coords)
    distances, indices = tree.query(coords_array[:, :2], k=1)
    
    # ä¸ºæ¯ä¸ªæ”¯æ¶åˆ†é…æœ€è¿‘é’»å­”çš„åœ°è´¨ç‰¹å¾
    support_geo_features = geo_features[indices.flatten()]
    
    return support_geo_features, feature_cols

def generate_adjacency_matrix(num_nodes, method='chain', **kwargs):
    """
    ç”Ÿæˆé‚»æ¥çŸ©é˜µ
    :param num_nodes: èŠ‚ç‚¹æ•°é‡
    :param method: ç”Ÿæˆæ–¹æ³•
        - 'chain': é“¾å¼ç»“æ„(ç›¸é‚»æ”¯æ¶è¿æ¥)
        - 'grid': ç½‘æ ¼ç»“æ„(å¦‚æœæ”¯æ¶æ’åˆ—æˆç½‘æ ¼)
        - 'distance': åŸºäºè·ç¦»(éœ€è¦æä¾›åæ ‡)
        - 'full': å…¨è¿æ¥
        - 'knn': Kè¿‘é‚»
        - 'adaptive': â­è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾(æ¨èç”¨äºRÂ²â‰¥0.8)
    :param kwargs: é¢å¤–å‚æ•°
    :return: é‚»æ¥çŸ©é˜µ (num_nodes, num_nodes)
    """
    adj_mx = np.zeros((num_nodes, num_nodes))
    
    if method == 'adaptive':
        # â­ è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾ - ç›®æ ‡RÂ²â‰¥0.8
        # å‡è®¾æ”¯æ¶çº¿æ€§æ’åˆ—ï¼ˆå¯æ ¹æ®å®é™…å¸ƒå±€è°ƒæ•´ï¼‰
        positions = np.arange(num_nodes).reshape(-1, 1).astype(float)
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distances = squareform(pdist(positions, metric='euclidean'))
        
        # è‡ªé€‚åº”é˜ˆå€¼ï¼šè¿æ¥è·ç¦»åœ¨thresholdä»¥å†…çš„æ”¯æ¶
        threshold = kwargs.get('threshold', 10.0)
        sigma = kwargs.get('sigma', 5.0)  # é«˜æ–¯æ ¸å‚æ•°
        
        # ä½¿ç”¨é«˜æ–¯æ ¸è®¡ç®—æƒé‡ï¼ˆè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
        adj_mx = np.exp(-distances**2 / (2 * sigma**2))
        
        # å¯é€‰ï¼šç¡¬é˜ˆå€¼ï¼Œåªä¿ç•™ä¸€å®šèŒƒå›´å†…çš„è¿æ¥
        adj_mx[distances > threshold] = 0
        
        # ç¡®ä¿å¯¹ç§°æ€§
        adj_mx = (adj_mx + adj_mx.T) / 2
        
        # è‡ªè¿æ¥è®¾ä¸º1
        np.fill_diagonal(adj_mx, 1.0)
        
        return adj_mx
    
    elif method == 'chain':
        # é“¾å¼ç»“æ„:æ¯ä¸ªèŠ‚ç‚¹ä¸ç›¸é‚»èŠ‚ç‚¹è¿æ¥
        for i in range(num_nodes - 1):
            adj_mx[i, i + 1] = 1
            adj_mx[i + 1, i] = 1
    
    elif method == 'grid':
        # ç½‘æ ¼ç»“æ„(éœ€è¦æŒ‡å®šè¡Œåˆ—æ•°)
        rows = kwargs.get('rows', int(np.sqrt(num_nodes)))
        cols = num_nodes // rows
        for i in range(num_nodes):
            row, col = i // cols, i % cols
            # è¿æ¥ä¸Šä¸‹å·¦å³é‚»å±…
            neighbors = []
            if row > 0: neighbors.append((row - 1) * cols + col)
            if row < rows - 1: neighbors.append((row + 1) * cols + col)
            if col > 0: neighbors.append(row * cols + col - 1)
            if col < cols - 1: neighbors.append(row * cols + col + 1)
            for j in neighbors:
                adj_mx[i, j] = 1
    
    elif method == 'distance':
        # åŸºäºè·ç¦»(éœ€è¦æä¾›åæ ‡)
        coords = kwargs.get('coords')
        threshold = kwargs.get('threshold', 1.0)
        if coords is not None:
            distances = squareform(pdist(coords))
            adj_mx = (distances <= threshold).astype(float)
            np.fill_diagonal(adj_mx, 0)
    
    elif method == 'full':
        # å…¨è¿æ¥
        adj_mx = np.ones((num_nodes, num_nodes))
        np.fill_diagonal(adj_mx, 0)
    
    elif method == 'knn':
        # Kè¿‘é‚»(éœ€è¦æä¾›åæ ‡)
        coords = kwargs.get('coords')
        k = kwargs.get('k', 3)
        if coords is not None:
            distances = squareform(pdist(coords))
            for i in range(num_nodes):
                # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»å±…
                neighbors = np.argsort(distances[i])[1:k+1]
                adj_mx[i, neighbors] = 1
                adj_mx[neighbors, i] = 1
    
    return adj_mx

def calculate_normalized_laplacian(adj_mx):
    """
    è®¡ç®—æ ‡å‡†åŒ–çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ A_hat = D^{-1/2} * (A + I) * D^{-1/2}
    
    :param adj_mx: é‚»æ¥çŸ©é˜µ (N, N)
    :return: A_hat (N, N)
    """
    adj_mx = adj_mx + np.eye(adj_mx.shape[0])
    d = np.array(adj_mx.sum(1)).flatten()
    d_inv_sqrt = np.power(d, -0.5)
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = np.diag(d_inv_sqrt)
    normalized_laplacian = d_mat_inv_sqrt.dot(adj_mx).dot(d_mat_inv_sqrt)
    return normalized_laplacian.astype(np.float32)

def generate_dataloader(data, batch_size, seq_len=12, pre_len=1, train_ratio=0.7, val_ratio=0.1):
    """
    ç”Ÿæˆ PyTorch DataLoaders
    :param data: (num_samples, num_nodes, num_features)
    :param batch_size: æ‰¹é‡å¤§å°
    :param seq_len: å†å²æ—¶é—´æ­¥
    :param pre_len: é¢„æµ‹æ—¶é—´æ­¥
    :return: train_loader, val_loader, test_loader, scaler
    """
    # å½’ä¸€åŒ– (è¿™é‡Œä½¿ç”¨ç®€å•çš„ Z-Score)
    mean = data.mean()
    std = data.std()
    scaler = {'mean': mean, 'std': std}
    data = (data - mean) / std

    x, y = [], []
    for i in range(len(data) - seq_len - pre_len + 1):
        x.append(data[i : i + seq_len])
        y.append(data[i + seq_len : i + seq_len + pre_len, :, 0:1]) # å‡è®¾åªé¢„æµ‹ç¬¬ä¸€ä¸ªç‰¹å¾

    x = np.array(x) # (N_samples, seq_len, num_nodes, num_features)
    y = np.array(y) # (N_samples, pre_len, num_nodes, 1)

    # è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹ (Batch, Features, Nodes, Time)
    x = np.transpose(x, (0, 3, 2, 1)) 
    # (Batch, Time_out, Nodes, Features_out) -> (Batch, Features_out, Nodes, Time_out)
    y = np.transpose(y, (0, 3, 2, 1))

    # åˆ’åˆ†æ•°æ®é›†
    num_samples = x.shape[0]
    train_end = int(num_samples * train_ratio)
    val_end = int(num_samples * (train_ratio + val_ratio))

    train_x, train_y = x[:train_end], y[:train_end]
    val_x, val_y = x[train_end:val_end], y[val_end:]
    test_x, test_y = x[val_end:], y[val_end:]

    # åˆ›å»º TensorDataset å’Œ DataLoader
    def create_loader(x_data, y_data):
        tensor_x = torch.tensor(x_data, dtype=torch.float32)
        tensor_y = torch.tensor(y_data, dtype=torch.float32)
        dataset = torch.utils.data.TensorDataset(tensor_x, tensor_y)
        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    train_loader = create_loader(train_x, train_y)
    val_loader = create_loader(val_x, val_y)
    test_loader = create_loader(test_x, test_y)

    return train_loader, val_loader, test_loader, scaler

# ----------------------------------------------------------------------
# 2. STGCN æ¨¡å‹å®šä¹‰ (PyTorch)
# ----------------------------------------------------------------------

class SimpleLSTM(nn.Module):
    """
    ç®€å•çš„ LSTM æ¨¡å‹ï¼ˆä¸ä½¿ç”¨å›¾ç»“æ„ï¼Œç›´æ¥åºåˆ—é¢„æµ‹ï¼‰
    é€‚ç”¨äºç¨€ç–å›¾æ•°æ®
    """
    def __init__(self, num_features, hidden_dim=128, num_layers=2):
        super(SimpleLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTM å±‚
        self.lstm = nn.LSTM(
            num_features, 
            hidden_dim, 
            num_layers, 
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        
        self.dropout = nn.Dropout(0.2)
        self.relu = nn.ReLU()
        
    def forward(self, X):
        """
        X: (Batch, seq_len, num_features)
        è¾“å‡º: (Batch, 1) - é¢„æµ‹å€¼
        """
        # LSTM
        lstm_out, _ = self.lstm(X)  # (B, T, hidden)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_hidden = lstm_out[:, -1, :]  # (B, hidden)
        
        # å…¨è¿æ¥å±‚
        x = self.relu(self.fc1(last_hidden))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        out = self.fc3(x)  # (B, 1)
        
        return out

class AttentionLSTM(nn.Module):
    """
    å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„LSTMæ¨¡å‹ - æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
    """
    def __init__(self, num_features, hidden_dim=128, num_layers=2):
        super(AttentionLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            num_features,
            hidden_dim,
            num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # æ³¨æ„åŠ›å±‚
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, X):
        """
        X: (Batch, seq_len, num_features)
        è¾“å‡º: (Batch, 1)
        """
        # LSTM
        lstm_out, _ = self.lstm(X)  # (B, T, hidden)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = self.attention(lstm_out)  # (B, T, 1)
        attention_weights = torch.softmax(attention_weights, dim=1)  # (B, T, 1)
        
        # åŠ æƒæ±‚å’Œ
        context = torch.sum(lstm_out * attention_weights, dim=1)  # (B, hidden)
        
        # å…¨è¿æ¥å±‚
        x = self.relu(self.fc1(context))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        out = self.fc3(x)  # (B, 1)
        
        return out

class TransformerPredictor(nn.Module):
    """
    â­ Transformeræ—¶åºé¢„æµ‹æ¨¡å‹ - æœ€å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œç›®æ ‡RÂ²â‰¥0.8
    åˆ©ç”¨Self-Attentionæœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–
    """
    def __init__(self, num_features, d_model=128, nhead=8, num_encoder_layers=3, dim_feedforward=512):
        super(TransformerPredictor, self).__init__()
        self.d_model = d_model
        
        # è¾“å…¥æŠ•å½±å±‚ï¼ˆå°†ç‰¹å¾ç»´åº¦æŠ•å½±åˆ°d_modelï¼‰
        self.input_projection = nn.Linear(num_features, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoder = PositionalEncoding(d_model, max_len=50)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
        
        # è¾“å‡ºå±‚
        self.fc1 = nn.Linear(d_model, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        self.layer_norm = nn.LayerNorm(d_model)
    
    def forward(self, X):
        """
        X: (Batch, seq_len, num_features)
        è¾“å‡º: (Batch, 1)
        """
        # è¾“å…¥æŠ•å½±
        X = self.input_projection(X)  # (B, T, d_model)
        X = self.layer_norm(X)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        X = self.pos_encoder(X)  # (B, T, d_model)
        
        # Transformerç¼–ç 
        encoded = self.transformer_encoder(X)  # (B, T, d_model)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼ˆä¹Ÿå¯ä»¥ç”¨å¹³å‡æ± åŒ–ï¼‰
        out = encoded[:, -1, :]  # (B, d_model)
        
        # å…¨è¿æ¥å±‚
        out = self.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.relu(self.fc2(out))
        out = self.dropout(out)
        out = self.fc3(out)  # (B, 1)
        
        return out

class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç å±‚ - ä¸ºTransformeræä¾›åºåˆ—ä½ç½®ä¿¡æ¯
    """
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        x: (Batch, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return x

class TimeBlock(nn.Module):
    """
    æ—¶åºå·ç§¯å— (TCN)
    """
    def __init__(self, in_channels, out_channels, kernel_size=3):
        super(TimeBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))
        self.conv2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))
        self.conv3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), padding=(0, (kernel_size - 1) // 2))

    def forward(self, X):
        # è¾“å…¥ X: (Batch, Channels, Nodes, Time_steps)
        # GLU (Gated Linear Unit)
        # X shape: (B, C_in, N, T)
        return (self.conv1(X) + self.conv3(X)) * torch.sigmoid(self.conv2(X))

class STGCNBlock(nn.Module):
    """
    æ—¶ç©ºå·ç§¯å— (Spatio-Temporal GCN Block)
    """
    def __init__(self, in_channels, spatial_channels, out_channels, num_nodes, Kt):
        super(STGCNBlock, self).__init__()
        # æ—¶åºå·ç§¯ (TCN)
        self.tcn = TimeBlock(in_channels, spatial_channels, Kt)
        # ç©ºé—´å·ç§¯ (GCN)
        self.gcn = nn.Conv2d(spatial_channels, out_channels, (1, 1))
        # æ‰¹é‡å½’ä¸€åŒ– - åº”è¯¥å¯¹ out_channels è¿›è¡Œå½’ä¸€åŒ–
        self.bn = nn.BatchNorm2d(out_channels)
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚(å¦‚æœç»´åº¦ä¸åŒ¹é…)
        self.residual_conv = nn.Conv2d(in_channels, out_channels, (1, 1)) if in_channels != out_channels else None
        
    def forward(self, X, A_hat):
        # X: (B, C_in, N, T)
        # A_hat: (N, N)

        # 1. TCN
        X_tcn = self.tcn(X) # (B, spatial_channels, N, T_out)
        
        # 2. GCN
        # (B, spatial_channels, N, T_out) -> (B, T_out, N, spatial_channels)
        X_gcn_input = X_tcn.permute(0, 3, 2, 1) 
        
        # (B, T_out, N, spatial_channels) x (N, N) -> (B, T_out, N, spatial_channels)
        X_gcn = torch.einsum('btni,nm->btmi', X_gcn_input, A_hat)
        
        # (B, T_out, N, spatial_channels) -> (B, spatial_channels, N, T_out)
        X_gcn = X_gcn.permute(0, 3, 2, 1)

        # 3. 1x1 å·ç§¯
        X_gcn = self.gcn(X_gcn) # (B, out_channels, N, T_out)
        
        # 4. æ‰¹å½’ä¸€åŒ–
        X_gcn = self.bn(X_gcn)
        
        # 5. æ®‹å·®è¿æ¥(éœ€è¦å¤„ç†æ—¶é—´ç»´åº¦å’Œé€šé“ç»´åº¦çš„å˜åŒ–)
        if X.shape[-1] > X_gcn.shape[-1]:  # æ—¶é—´ç»´åº¦å‡å°‘äº†
            # æˆªå– X çš„æœ€åå‡ ä¸ªæ—¶é—´æ­¥ä»¥åŒ¹é…
            X_res = X[:, :, :, -(X_gcn.shape[-1]):]
        else:
            X_res = X
            
        # å¦‚æœé€šé“æ•°ä¸åŒ¹é…,ä½¿ç”¨ 1x1 å·ç§¯æŠ•å½±
        if self.residual_conv is not None:
            X_res = self.residual_conv(X_res)
        
        # 6. æ¿€æ´»
        X_out = F.relu(X_gcn + X_res)
        
        return X_out


class STGCN(nn.Module):
    """
    STGCN å®Œæ•´æ¨¡å‹
    è¾“å…¥ç»´åº¦: (Batch, Features_in, Num_Nodes, Seq_Len)
    è¾“å‡ºç»´åº¦: (Batch, Features_out, Num_Nodes, Pred_Len)
    """
    def __init__(self, num_nodes, num_features, seq_len, pred_len, hidden_dim=64, Kt=3):
        super(STGCN, self).__init__()
        self.num_nodes = num_nodes
        self.pred_len = pred_len
        
        # ä½¿ç”¨hidden_dimå‚æ•°åŒ–æ¨¡å‹å®¹é‡
        # STGCN Block 1
        self.st_block1 = STGCNBlock(num_features, hidden_dim, hidden_dim, num_nodes, Kt)
        
        # STGCN Block 2
        self.st_block2 = STGCNBlock(hidden_dim, hidden_dim, hidden_dim, num_nodes, Kt)
        
        # Dropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(0.2)
        
        # æœ€åä¸€ä¸ªæ—¶åºå·ç§¯ (æ‰©å±•åˆ°2å€hidden_dim)
        self.last_tcn = TimeBlock(hidden_dim, hidden_dim * 2, Kt)
        
        # è®¡ç®—ç»è¿‡æ‰€æœ‰å±‚åçš„æ—¶é—´ç»´åº¦
        # æ¯ä¸ª TimeBlock ä½¿ç”¨ padding=(kernel_size-1)//2, æ‰€ä»¥ä¸æ”¹å˜æ—¶é—´ç»´åº¦
        # ä½†ç”±äºæˆ‘ä»¬åœ¨ forward ä¸­åšäº†æ®‹å·®è¿æ¥çš„æˆªå–,å®é™…ä¼šå‡å°‘
        # å®é™…ä¸Š TimeBlock ä½¿ç”¨ same padding,æ—¶é—´ç»´åº¦åº”è¯¥ä¿æŒä¸å˜
        # è®©æˆ‘ä»¬ä½¿ç”¨è‡ªé€‚åº”æ± åŒ–æ¥å¤„ç†
        
        # è¾“å‡ºå±‚:å°†ç‰¹å¾æ˜ å°„åˆ°é¢„æµ‹é•¿åº¦
        self.output_conv = nn.Conv2d(hidden_dim * 2, hidden_dim * 2, (1, 1))
        self.temporal_conv = nn.Conv2d(hidden_dim * 2, pred_len, (1, 1))
        # æœ€ç»ˆé€šé“å‹ç¼©å±‚: hidden_dim*2 -> 1
        self.final_conv = nn.Conv2d(hidden_dim * 2, 1, (1, 1))
    
    def forward(self, X, A_hat):
        # X: (B, C_in, N, T_in)
        
        # Block 1
        X = self.st_block1(X, A_hat) # (B, hidden_dim, N, T)
        X = self.dropout(X)  # æ·»åŠ dropout
        
        # Block 2
        X = self.st_block2(X, A_hat) # (B, hidden_dim, N, T)
        X = self.dropout(X)  # æ·»åŠ dropout
        
        # Last TCN
        X = self.last_tcn(X) # (B, hidden_dim*2, N, T)
        
        # Output layers
        X = F.relu(self.output_conv(X)) # (B, hidden_dim*2, N, T)
        
        # ä½¿ç”¨è‡ªé€‚åº”å¹³å‡æ± åŒ–å°†æ—¶é—´ç»´åº¦è°ƒæ•´ä¸ºé¢„æµ‹é•¿åº¦
        X = F.adaptive_avg_pool2d(X, (X.shape[2], self.pred_len)) # (B, 128, N, pred_len)
        
        # å°†é€šé“æ•°è½¬æ¢ä¸º1(åªé¢„æµ‹ä¸€ä¸ªç‰¹å¾)
        X = self.final_conv(X) # (B, 1, N, pred_len)
        
        # ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œè®©æ¨¡å‹è‡ªç”±å­¦ä¹ è¾“å‡ºèŒƒå›´
        # åœ¨è®­ç»ƒæ—¶ä¼šé€šè¿‡clampè£å‰ªåˆ°[0,1]
        
        return X # (B, 1, N, pred_len)


# ----------------------------------------------------------------------
# 3. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
# ----------------------------------------------------------------------

def train_epoch(model, train_loader, optimizer, loss_fn, device, A_hat):
    model.train()
    total_loss = 0
    A_hat_tensor = torch.tensor(A_hat, dtype=torch.float32).to(device)
    
    for x_batch, y_batch in train_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        
        # x_batch: (B, F_in, N, T_in)
        # y_batch: (B, F_out, N, T_out)
        y_pred = model(x_batch, A_hat_tensor) # (B, 1, N, pred_len)
        
        # ç¡®ä¿ y_pred å’Œ y_batch ç»´åº¦åŒ¹é…
        # y_batch: (B, F_out, N, T_out)
        # y_pred: (B, 1, N, pred_len)
        loss = loss_fn(y_pred, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
    return total_loss / len(train_loader)

def evaluate(model, val_loader, loss_fn, device, A_hat):
    model.eval()
    total_loss = 0
    A_hat_tensor = torch.tensor(A_hat, dtype=torch.float32).to(device)
    
    with torch.no_grad():
        for x_batch, y_batch in val_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            
            # x_batch: (B, F_in, N, T_in)
            # y_batch: (B, F_out, N, T_out)
            y_pred = model(x_batch, A_hat_tensor) # (B, 1, N, pred_len)
            
            loss = loss_fn(y_pred, y_batch)
            total_loss += loss.item()
            
    return total_loss / len(val_loader)

# ----------------------------------------------------------------------
# 4. Streamlit äº¤äº’å¼ GUI ç•Œé¢
# ----------------------------------------------------------------------

st.title("STGCN çŸ¿å‹é¢„æµ‹æ¨¡å‹ - è®­ç»ƒç•Œé¢")

# --- ä¾§è¾¹æ :å‚æ•°è®¾ç½® ---
st.sidebar.header("æ¨¡å‹å‚æ•°è®¾ç½®")
SEQ_LEN = st.sidebar.slider("å†å²æ—¶é—´æ­¥ (Seq_Len)", 5, 24, 12)
PRED_LEN = st.sidebar.slider("é¢„æµ‹æ—¶é—´æ­¥ (Pred_Len)", 1, 12, 1)
BATCH_SIZE = st.sidebar.slider("æ‰¹é‡å¤§å° (Batch_Size)", 8, 128, 32)
EPOCHS = st.sidebar.slider("è®­ç»ƒè½®æ•° (Epochs)", 10, 200, 50)
LR = st.sidebar.number_input("å­¦ä¹ ç‡ (Learning_Rate)", 0.0001, 0.1, 0.001, format="%.4f")

# GPU æ£€æµ‹å’Œè®¾å¤‡é€‰æ‹©
gpu_available = torch.cuda.is_available()
gpu_usable = False

if gpu_available:
    # æµ‹è¯• GPU æ˜¯å¦çœŸçš„å¯ç”¨
    try:
        test_tensor = torch.rand(10, 10).cuda()
        _ = test_tensor * 2
        torch.cuda.synchronize()
        gpu_usable = True
        del test_tensor
        torch.cuda.empty_cache()
    except Exception as e:
        gpu_usable = False
        st.sidebar.warning(f"âš ï¸ GPU æ£€æµ‹åˆ°ä½†ä¸å¯ç”¨: {str(e)[:50]}...")

if gpu_usable:
    DEVICE = st.sidebar.selectbox("è®¾å¤‡", ["cuda", "cpu"], index=0)
    st.sidebar.success("âœ… GPU å¯ç”¨ä¸”æ­£å¸¸å·¥ä½œ")
else:
    DEVICE = "cpu"
    st.sidebar.info("â„¹ï¸ ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒ")
    if gpu_available:
        st.sidebar.warning("""
        âš ï¸ **GPU å…¼å®¹æ€§é—®é¢˜**
        
        æ‚¨çš„ RTX 5070 Ti (Blackwell æ¶æ„) éœ€è¦æ›´æ–°çš„ PyTorch ç‰ˆæœ¬ã€‚
        
        å½“å‰ PyTorch 2.2.2 ä¸æ”¯æŒè¯¥ GPUã€‚
        
        å»ºè®®:
        1. ä½¿ç”¨ CPU è®­ç»ƒ(å½“å‰é€‰é¡¹)
        2. ç­‰å¾… PyTorch 2.6+ ç‰ˆæœ¬
        3. æˆ–å°è¯• PyTorch nightly ç‰ˆæœ¬
        """)

st.sidebar.header("æ•°æ®ä¸Šä¼ ")

# æ•°æ®æºé€‰æ‹©
data_source = st.sidebar.radio(
    "é€‰æ‹©æ•°æ®æ¥æº",
    ["ä¸Šä¼ CSVæ–‡ä»¶", "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†"],
    help="CSV: åŸå§‹çŸ¿å‹æ—¶é—´åºåˆ— | é¢„å¤„ç†: å·²æå–ç‰¹å¾çš„è®­ç»ƒæ•°æ®"
)

# æ ¹æ®æ•°æ®æºæ˜¾ç¤ºä¸åŒçš„ä¸Šä¼ é€‰é¡¹
if data_source == "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†":
    st.sidebar.subheader("ğŸ“¦ åŠ è½½é¢„å¤„ç†æ•°æ®")
    
    # æ£€æŸ¥é»˜è®¤æ•°æ®é›†
    default_npz_path = os.path.join(os.path.dirname(__file__), 'processed_data', 'sequence_dataset.npz')
    use_default_dataset = False
    
    if os.path.exists(default_npz_path):
        use_default_dataset = st.sidebar.checkbox(
            "ä½¿ç”¨å·²ç”Ÿæˆçš„æ•°æ®é›†",
            value=True,
            help=f"è·¯å¾„: {default_npz_path}"
        )
    
    if use_default_dataset:
        npz_file = default_npz_path
        st.sidebar.success("âœ“ ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†")
        st.sidebar.info(
            """
            **æ•°æ®é›†ä¿¡æ¯:**
            - 195,836 ä¸ªè®­ç»ƒæ ·æœ¬
            - 125 ä¸ªæ”¯æ¶
            - 17 ç»´ç‰¹å¾
            - åºåˆ—é•¿åº¦: 5 â†’ 1
            """
        )
    else:
        npz_file = st.sidebar.file_uploader(
            "ä¸Šä¼ é¢„å¤„ç†æ•°æ®æ–‡ä»¶ (.npz)",
            type=["npz"],
            help="ä½¿ç”¨ preprocess/prepare_training_data.py ç”Ÿæˆçš„æ•°æ®"
        )
    
    # åŠ è½½åæ ‡æ–‡ä»¶
    coord_file_path = os.path.join(os.path.dirname(__file__), 'processed_data', 'support_coordinates.csv')
    if os.path.exists(coord_file_path):
        coord_file = coord_file_path
    else:
        coord_file = None
    
else:  # CSV æ–‡ä»¶ä¸Šä¼ æ¨¡å¼
    npz_file = None
    
    # æ­¥éª¤1: ä¸Šä¼ çŸ¿å‹æ•°æ®
    st.sidebar.subheader("æ­¥éª¤1: çŸ¿å‹æ•°æ®")
    data_file = st.sidebar.file_uploader("ä¸Šä¼ çŸ¿å‹æ•°æ®æ–‡ä»¶ (.csv)", type=["csv"])
    st.sidebar.info(
        """
        **CSV æ ¼å¼è¦æ±‚:**
        - æ¯è¡Œ = ä¸€ä¸ªæ—¶é—´ç‚¹
        - æ¯åˆ— = ä¸€ä¸ªæ”¯æ¶(åˆ—åè¦ä¸åæ ‡æ–‡ä»¶å¯¹åº”)
        
        ç¤ºä¾‹:
        ```
        æ—¶é—´, ZJ001, ZJ002, ZJ003, ...
        2023-01-01, 100.5, 98.3, 102.1, ...
        ```
        """
    )

    # æ­¥éª¤2: ä¸Šä¼ æ”¯æ¶åæ ‡
    st.sidebar.subheader("æ­¥éª¤2: æ”¯æ¶åæ ‡ (é‡è¦!)")
    coord_file = st.sidebar.file_uploader(
        "ä¸Šä¼ æ”¯æ¶åæ ‡æ–‡ä»¶ (.csv/.xlsx)", 
        type=["csv", "xlsx", "xls"],
        help="åæ ‡æ–‡ä»¶åº”åŒ…å«: æ”¯æ¶ID, Xåæ ‡, Yåæ ‡"
    )
    st.sidebar.info(
        """
        **åæ ‡æ–‡ä»¶æ ¼å¼:**
        ```
        æ”¯æ¶ID, Xåæ ‡, Yåæ ‡
        ZJ001, 1000.5, 2000.3
        ZJ002, 1001.2, 2000.5
        ZJ003, 1002.0, 2001.1
        ```
        
        âš ï¸ **æ”¯æ¶IDå¿…é¡»ä¸çŸ¿å‹æ•°æ®çš„åˆ—åå¯¹åº”**
        """
    )

# æ­¥éª¤3: ä¸Šä¼ åœ°è´¨ç‰¹å¾(å¯é€‰) - åªåœ¨CSVæ¨¡å¼ä¸‹æ˜¾ç¤º
use_geological = False  # é»˜è®¤å€¼
geo_file = None
if data_source == "ä¸Šä¼ CSVæ–‡ä»¶":
    st.sidebar.subheader("æ­¥éª¤3: åœ°è´¨ç‰¹å¾ (å¯é€‰)")
    use_geological = st.sidebar.checkbox("èåˆåœ°è´¨ç‰¹å¾æ•°æ®", value=False)
    
if use_geological:
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é»˜è®¤åœ°è´¨ç‰¹å¾æ–‡ä»¶
    default_geo_path = os.path.join(os.path.dirname(__file__), 'geology_features_extracted.csv')
    use_default_geo = False
    
    if os.path.exists(default_geo_path):
        use_default_geo = st.sidebar.checkbox(
            "ä½¿ç”¨æå–çš„é’»å­”åœ°è´¨ç‰¹å¾", 
            value=True,
            help=f"å·²æ£€æµ‹åˆ° geology_features_extracted.csv"
        )
    
    if use_default_geo:
        geo_file = default_geo_path
        st.sidebar.success("âœ“ ä½¿ç”¨é’»å­”åœ°è´¨ç‰¹å¾æ•°æ®")
        st.sidebar.info(
            """
            **åŒ…å«çš„åœ°è´¨ç‰¹å¾:**
            - æ€»åšåº¦ (m)
            - ç…¤å±‚åšåº¦/æ•°é‡ (m/ä¸ª)
            - é¡¶æ¿ç…¤å±‚åŸ‹æ·± (m)
            - å¹³å‡å¼¹æ€§æ¨¡é‡ (GPa)
            - å¹³å‡å®¹é‡ (kN/mÂ³)
            - æœ€å¤§æŠ—æ‹‰å¼ºåº¦ (MPa)
            - ç ‚å²©/æ³¥å²©å æ¯”
            """
        )
    else:
        geo_file = st.sidebar.file_uploader(
            "ä¸Šä¼ åœ°è´¨ç‰¹å¾æ–‡ä»¶ (.csv/.xlsx)",
            type=["csv", "xlsx", "xls"],
            help="åœ°è´¨æ–‡ä»¶åº”åŒ…å«: Xåæ ‡, Yåæ ‡, åœ°è´¨ç‰¹å¾"
        )
        st.sidebar.info(
            """
            **åœ°è´¨æ–‡ä»¶æ ¼å¼:**
            ```
            Xåæ ‡, Yåæ ‡, æ–­å±‚è·ç¦», ç…¤å±‚åšåº¦, ...
            1000.5, 2000.3, 50.2, 3.5, ...
            1001.0, 2000.5, 48.5, 3.6, ...
            ```
            
            ç³»ç»Ÿä¼šæ ¹æ®è·ç¦»å°†åœ°è´¨ç‰¹å¾æ˜ å°„åˆ°æ”¯æ¶ä½ç½®
            """
        )

# é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼
st.sidebar.header("é‚»æ¥çŸ©é˜µè®¾ç½®")
adj_method = st.sidebar.selectbox(
    "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼",
    ["chain", "grid", "distance", "knn", "full", "upload"],
    format_func=lambda x: {
        "chain": "é“¾å¼ç»“æ„ (ç›¸é‚»æ”¯æ¶è¿æ¥)",
        "grid": "ç½‘æ ¼ç»“æ„ (2Dæ’åˆ—)",
        "distance": "è·ç¦»é˜ˆå€¼ (éœ€è¦åæ ‡)",
        "knn": "Kè¿‘é‚» (éœ€è¦åæ ‡)",
        "full": "å…¨è¿æ¥ (æ‰€æœ‰èŠ‚ç‚¹äº’è¿)",
        "upload": "ä¸Šä¼ è‡ªå®šä¹‰é‚»æ¥çŸ©é˜µ"
    }[x]
)

# æ ¹æ®é€‰æ‹©çš„æ–¹æ³•æ˜¾ç¤ºé¢å¤–å‚æ•°
adj_params = {}
if adj_method == "grid":
    adj_params['rows'] = st.sidebar.number_input("ç½‘æ ¼è¡Œæ•°", min_value=1, value=10)
elif adj_method == "knn":
    adj_params['k'] = st.sidebar.number_input("Kå€¼(è¿‘é‚»æ•°é‡)", min_value=1, value=3)
    st.sidebar.warning("Kè¿‘é‚»æ–¹æ³•éœ€è¦æ”¯æ¶åæ ‡ä¿¡æ¯,æš‚æ—¶ä½¿ç”¨éšæœºåæ ‡")

adj_file = None
if adj_method == "upload":
    adj_file = st.sidebar.file_uploader("ä¸Šä¼ é‚»æ¥çŸ©é˜µæ–‡ä»¶ (.npyæˆ–.csv)", type=["npy", "csv"])

# --- ä¸»ç•Œé¢ ---
if data_source == "ä½¿ç”¨é¢„å¤„ç†æ•°æ®é›†" and npz_file:
    st.header("1. åŠ è½½é¢„å¤„ç†æ•°æ®é›†")
    
    try:
        # åŠ è½½é¢„å¤„ç†çš„åºåˆ—æ•°æ®
        X, y_final, support_ids, feature_names = load_processed_sequence_data(npz_file)
        
        st.write(f"**æ•°æ®å½¢çŠ¶:**")
        st.write(f"- æ ·æœ¬æ•°é‡: {X.shape[0]:,}")
        st.write(f"- åºåˆ—é•¿åº¦: {X.shape[1]} (å†å²æ­¥æ•°)")
        st.write(f"- ç‰¹å¾ç»´åº¦: {X.shape[2]}")
        st.write(f"- æ ‡ç­¾å½¢çŠ¶: {y_final.shape}")
        
        NUM_SAMPLES = X.shape[0]
        SEQ_LEN = X.shape[1]
        NUM_FEATURES = X.shape[2]
        PRED_LEN = y_final.shape[1]
        
        # è·å–æ”¯æ¶ä¿¡æ¯
        unique_supports = np.unique(support_ids)
        NUM_NODES = len(unique_supports)
        
        st.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼åŒ…å« {NUM_NODES} ä¸ªæ”¯æ¶ï¼Œ{NUM_SAMPLES:,} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        # â­ æ–°å¢ï¼šæ•°æ®æ ¼å¼é€‰æ‹©
        st.header("1.5 æ•°æ®æ ¼å¼é€‰æ‹© â­ é‡è¦ï¼")
        
        data_format = st.radio(
            "é€‰æ‹©æ•°æ®æ ¼å¼ï¼ˆå½±å“æ¨¡å‹æ€§èƒ½ï¼‰",
            ["å•æ”¯æ¶åºåˆ—ï¼ˆå½“å‰æ ¼å¼ï¼ŒRÂ²â‰ˆ0.3ï¼‰", "å®Œæ•´æ—¶ç©ºæ•°æ®ï¼ˆæ¨èï¼Œé¢„æœŸRÂ²>0.5ï¼‰"],
            help="""
            **å•æ”¯æ¶åºåˆ—**ï¼šæ¯ä¸ªæ ·æœ¬åªåŒ…å«ä¸€ä¸ªæ”¯æ¶çš„å†å²æ•°æ®
            - ä¼˜ç‚¹ï¼šæ•°æ®é‡å¤§ï¼ˆ195,836æ ·æœ¬ï¼‰
            - ç¼ºç‚¹ï¼šä¸¢å¤±ç©ºé—´å…³ç³»ï¼ŒSTGCNæ•ˆæœå·®
            
            **å®Œæ•´æ—¶ç©ºæ•°æ®**ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«æ‰€æœ‰125ä¸ªæ”¯æ¶çš„åŒæ—¶åˆ»æ•°æ®
            - ä¼˜ç‚¹ï¼šä¿ç•™å®Œæ•´æ—¶ç©ºç»“æ„ï¼ŒSTGCN/Transformeræ•ˆæœå¥½
            - ç¼ºç‚¹ï¼šæ ·æœ¬æ•°å‡å°‘ï¼ˆçº¦1,500æ ·æœ¬ï¼‰
            """
        )
        
        use_spatial_reconstruction = data_format.startswith("å®Œæ•´æ—¶ç©ºæ•°æ®")
        
        if use_spatial_reconstruction:
            st.info("ğŸ”„ æ­£åœ¨é‡æ„æ—¶ç©ºæ•°æ®ï¼Œè¿™æ˜¯æå‡RÂ²çš„å…³é”®æ­¥éª¤...")
            
            try:
                X_spatial, y_spatial, valid_time_indices, support_to_idx = reconstruct_spatiotemporal_data(
                    X, y_final, support_ids, num_supports=NUM_NODES
                )
                
                st.success(f"""
                âœ… æ—¶ç©ºæ•°æ®é‡æ„å®Œæˆï¼
                - åŸå§‹æ ·æœ¬æ•°: {NUM_SAMPLES:,} (å•æ”¯æ¶åºåˆ—)
                - é‡æ„åæ—¶é—´æ­¥: {len(X_spatial):,}
                - æ¯ä¸ªæ—¶é—´æ­¥åŒ…å«: {NUM_NODES} ä¸ªæ”¯æ¶çš„å®Œæ•´æ•°æ®
                - æ–°æ•°æ®å½¢çŠ¶: {X_spatial.shape}
                """)
                
                # ç”¨é‡æ„åçš„æ•°æ®æ›¿æ¢åŸå§‹æ•°æ®
                X = X_spatial
                y_final = y_spatial
                NUM_SAMPLES = len(X)
                
                st.warning(f"""
                âš ï¸ **æ³¨æ„**ï¼šæ ·æœ¬æ•°ä» 195,836 å‡å°‘åˆ° {NUM_SAMPLES:,}
                è¿™æ˜¯æ­£å¸¸çš„ï¼å› ä¸ºæˆ‘ä»¬ç°åœ¨çš„æ¯ä¸ªæ ·æœ¬åŒ…å«å®Œæ•´çš„ç©ºé—´ä¿¡æ¯ã€‚
                å¯¹äºæ—¶ç©ºå›¾ç½‘ç»œï¼Œè¿™ç§æ ¼å¼æ›´åˆé€‚ã€‚
                """)
                
            except Exception as e:
                st.error(f"æ—¶ç©ºé‡æ„å¤±è´¥: {str(e)}")
                st.info("å°†ç»§ç»­ä½¿ç”¨å•æ”¯æ¶åºåˆ—æ ¼å¼")
                use_spatial_reconstruction = False
        
        # æ˜¾ç¤ºç‰¹å¾åˆ—è¡¨
        with st.expander("ğŸ“‹ æŸ¥çœ‹ç‰¹å¾åˆ—è¡¨"):
            st.write(f"å…± {len(feature_names)} ä¸ªç‰¹å¾:")
            for i, fname in enumerate(feature_names, 1):
                st.write(f"{i}. {fname}")
        
        # åŠ è½½åæ ‡
        coords_array = None
        if coord_file:
            if isinstance(coord_file, str):
                coord_df = pd.read_csv(coord_file)
            else:
                coord_df = load_coordinate_file(coord_file)
            
            coords_array = coord_df[['x', 'y']].values
            st.write(f"**æ”¯æ¶åæ ‡:** {coords_array.shape}")
        else:
            # ä½¿ç”¨é»˜è®¤çº¿æ€§åæ ‡
            coords_array = np.column_stack([np.arange(NUM_NODES), np.zeros(NUM_NODES)])
            st.info("ä½¿ç”¨é»˜è®¤çº¿æ€§åæ ‡")
        
        # æ•°æ®åˆ†å‰²
        st.header("2. æ•°æ®åˆ†å‰²")
        
        st.info("""
        ğŸ’¡ **æ•°æ®é‡å……è¶³**ï¼šå½“å‰æœ‰ 195,836 ä¸ªæ ·æœ¬ï¼Œæ•°æ®é‡å……è¶³é€‚åˆæ·±åº¦å­¦ä¹ ã€‚
        
        å»ºè®®æ¯”ä¾‹ï¼šè®­ç»ƒ 70% / éªŒè¯ 15% / æµ‹è¯• 15%
        """)
        
        train_ratio = st.slider("è®­ç»ƒé›†æ¯”ä¾‹", 0.5, 0.9, 0.7, 0.05)
        val_ratio = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.05, 0.3, 0.15, 0.05)
        
        train_end = int(NUM_SAMPLES * train_ratio)
        val_end = int(NUM_SAMPLES * (train_ratio + val_ratio))
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("è®­ç»ƒé›†", f"{train_end:,}", f"{train_ratio*100:.0f}%")
        with col2:
            st.metric("éªŒè¯é›†", f"{val_end-train_end:,}", f"{val_ratio*100:.0f}%")
        with col3:
            st.metric("æµ‹è¯•é›†", f"{NUM_SAMPLES-val_end:,}", f"{(1-train_ratio-val_ratio)*100:.0f}%")
        
        # è½¬æ¢ä¸ºå›¾æ•°æ®æ ¼å¼
        # ç”±äºé¢„å¤„ç†æ•°æ®å·²ç»æ˜¯ (num_samples, seq_len, num_features)
        # æˆ‘ä»¬éœ€è¦é‡å¡‘ä¸º (num_samples_per_support, num_supports, seq_len, num_features)
        
        st.header("3. å›¾ç»“æ„æ„å»º")
        
        # ç”Ÿæˆé‚»æ¥çŸ©é˜µ
        adj_method = st.selectbox(
            "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹å¼",
            ["distance", "knn", "chain", "full"],
            help="distance: åŸºäºåæ ‡è·ç¦» | knn: Kè¿‘é‚» | chain: é“¾å¼ | full: å…¨è¿æ¥"
        )
        
        adj_params = {}
        if adj_method == "knn":
            adj_params['k'] = st.number_input("Kå€¼(è¿‘é‚»æ•°é‡)", min_value=1, value=5, max_value=20)
        elif adj_method == "distance":
            adj_params['threshold'] = st.number_input("è·ç¦»é˜ˆå€¼(ç±³)", min_value=0.1, value=5.0, step=0.5)
        
        adj_mx = generate_adjacency_matrix(
            NUM_NODES,
            method=adj_method,
            coords=coords_array,
            **adj_params
        )
        
        # æ˜¾ç¤ºå›¾ä¿¡æ¯
        num_edges = np.sum(adj_mx > 0)
        avg_degree = num_edges / NUM_NODES
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("èŠ‚ç‚¹æ•°", NUM_NODES)
            st.metric("å¹³å‡åº¦æ•°", f"{avg_degree:.1f}")
        with col2:
            st.metric("è¾¹æ•°", int(num_edges))
            st.metric("å›¾å¯†åº¦", f"{num_edges/(NUM_NODES*(NUM_NODES-1))*100:.2f}%")
        
        # å¯è§†åŒ–é‚»æ¥çŸ©é˜µ
        with st.expander("ğŸ” æŸ¥çœ‹é‚»æ¥çŸ©é˜µ"):
            fig, ax = plt.subplots(figsize=(8, 8))
            im = ax.imshow(adj_mx, cmap='Blues', aspect='auto')
            ax.set_title("é‚»æ¥çŸ©é˜µ")
            ax.set_xlabel("æ”¯æ¶ ID")
            ax.set_ylabel("æ”¯æ¶ ID")
            plt.colorbar(im, ax=ax)
            st.pyplot(fig)
        
        # æ¨¡å‹è®­ç»ƒéƒ¨åˆ†
        st.header("4. æ¨¡å‹è®­ç»ƒ")
        
        # â­ ç‰¹å¾å·¥ç¨‹é€‰é¡¹
        use_feature_engineering = st.checkbox(
            "ğŸ”§ å¯ç”¨ç‰¹å¾å·¥ç¨‹ï¼ˆæ¨èï¼‰",
            value=True,
            help="è‡ªåŠ¨æ·»åŠ ç»Ÿè®¡ç‰¹å¾ã€å·®åˆ†ç‰¹å¾ç­‰ï¼Œé¢„æœŸæå‡RÂ² 10-20%"
        )
        
        if use_feature_engineering:
            st.success("âœ… å°†è‡ªåŠ¨æ·»åŠ ï¼šå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§/æœ€å°å€¼ã€å˜åŒ–ç‡ç­‰ç‰¹å¾")
        
        # æ¨¡å‹é€‰æ‹©
        model_type = st.radio(
            "é€‰æ‹©æ¨¡å‹ç±»å‹",
            [
                "LSTM (åŸºç¡€ç‰ˆ)", 
                "AttentionLSTM (æ³¨æ„åŠ›å¢å¼º)â­", 
                "Transformer (æœ€å¼ºè¡¨è¾¾åŠ›)ğŸš€", 
                "STGCN (å›¾ç¥ç»ç½‘ç»œ)"
            ],
            help="""
            LSTM: ç®€å•å¿«é€Ÿï¼Œé€‚åˆç¨€ç–æ•°æ® (RÂ²â‰ˆ0.35)
            AttentionLSTM: æ³¨æ„åŠ›æœºåˆ¶ï¼Œé¢„æœŸæå‡5-15% (RÂ²â‰ˆ0.40-0.50)
            Transformer: è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ€å¼ºè¡¨è¾¾èƒ½åŠ› (RÂ²â‰ˆ0.60-0.80)ğŸ”¥
            STGCN: å›¾ç¥ç»ç½‘ç»œï¼Œéœ€è¦å®Œæ•´æ—¶ç©ºæ•°æ® (RÂ²â‰ˆ0.55-0.70)
            """
        )
        
        # â­ å…¼å®¹æ€§æ£€æŸ¥å’Œè­¦å‘Š
        if "STGCN" in model_type and not use_spatial_reconstruction:
            st.error("""
            âš ï¸ **æ¨¡å‹é…ç½®ä¸å…¼å®¹ï¼**
            
            **é—®é¢˜ï¼š** STGCNæ¨¡å‹éœ€è¦å®Œæ•´çš„ç©ºé—´æ‹“æ‰‘ç»“æ„ï¼Œä½†å½“å‰é€‰æ‹©çš„æ˜¯"å•æ ·æœ¬åºåˆ—æ ¼å¼"
            
            **è§£å†³æ–¹æ¡ˆï¼ˆ2é€‰1ï¼‰ï¼š**
            
            1ï¸âƒ£ **åˆ‡æ¢åˆ°Transformeræ¨¡å‹ï¼ˆå¼ºçƒˆæ¨èï¼‰** â­â­â­
               - ä¿æŒå½“å‰"å•æ ·æœ¬åºåˆ—æ ¼å¼"
               - é€‰æ‹©"Transformer (æœ€å¼ºè¡¨è¾¾åŠ›)ğŸš€"
               - ä¼˜åŠ¿ï¼šä¿ç•™å…¨éƒ¨195,836æ ·æœ¬ + æœ€å¼ºè¡¨è¾¾èƒ½åŠ›
               - é¢„æœŸRÂ²: 0.65-0.80
            
            2ï¸âƒ£ **åˆ‡æ¢åˆ°å®Œæ•´æ—¶ç©ºæ•°æ®æ ¼å¼**
               - åœ¨ä¸Šæ–¹"æ•°æ®æ ¼å¼é€‰æ‹©"ä¸­é€‰æ‹©"å®Œæ•´æ—¶ç©ºæ•°æ®"
               - ç„¶åå¯ä»¥ä½¿ç”¨STGCN
               - âš ï¸ æ³¨æ„ï¼šæ ·æœ¬æ•°ä¼šå¤§å¹…å‡å°‘ï¼ˆå¯èƒ½<100ï¼‰
            
            **æ¨èé€‰æ‹©æ–¹æ¡ˆ1ï¼ˆå•æ ·æœ¬+Transformerï¼‰ä»¥è·å¾—æœ€ä½³æ•ˆæœï¼**
            """)
            st.stop()
        
        elif "STGCN" in model_type and use_spatial_reconstruction:
            st.info("""
            âœ… **é…ç½®æ­£ç¡®ï¼š** STGCN + å®Œæ•´æ—¶ç©ºæ•°æ®
            
            - å°†ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œå­¦ä¹ æ”¯æ¶é—´çš„ç©ºé—´å…³ç³»
            - éœ€è¦adaptiveå›¾ç»“æ„ä»¥è·å¾—æœ€ä½³æ•ˆæœ
            - é¢„æœŸRÂ²: 0.55-0.70ï¼ˆå¦‚æœæ•°æ®å®Œæ•´ï¼‰
            """)
        
        elif "Transformer" in model_type:
            st.info("""
            ğŸš€ **æœ€å¼ºé…ç½®ï¼š** Transformer + å¢å¼ºç‰¹å¾å·¥ç¨‹
            
            - Self-Attentionæœºåˆ¶æ•æ‰é•¿è·ç¦»æ—¶åºä¾èµ–
            - é€‚ç”¨äºå•æ ·æœ¬æ ¼å¼ï¼ˆä¿ç•™å…¨éƒ¨æ ·æœ¬ï¼‰
            - é¢„æœŸRÂ²: 0.65-0.80
            """)

        
        st.info(f"""
        **{model_type}**
        
        {'- åŸºç¡€LSTMæ¨¡å‹ï¼Œç›´æ¥åºåˆ—é¢„æµ‹' if 'LSTM (åŸºç¡€ç‰ˆ)' in model_type else ''}
        {'- â­ å¸¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œè‡ªåŠ¨å­¦ä¹ é‡è¦æ—¶é—´æ­¥' if 'AttentionLSTM' in model_type else ''}
        {'- ğŸš€ Self-Attentionæœºåˆ¶ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Œæœ€å¼ºè¡¨è¾¾èƒ½åŠ›' if 'Transformer' in model_type else ''}
        {'- å›¾å·ç§¯ç½‘ç»œï¼Œå­¦ä¹ ç©ºé—´-æ—¶é—´è”åˆæ¨¡å¼' if 'STGCN' in model_type else ''}
        """)
        
        # è®­ç»ƒå‚æ•°
        col1, col2 = st.columns(2)
        with col1:
            epochs = st.number_input("è®­ç»ƒè½®æ•°", min_value=1, value=100, max_value=500)
            batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=16, value=128, max_value=512, step=16)
        with col2:
            learning_rate = st.number_input("å­¦ä¹ ç‡", min_value=0.0001, value=0.001, max_value=0.1, format="%.4f", step=0.0001)
            hidden_dim = st.number_input("éšè—å±‚ç»´åº¦", min_value=16, value=128, max_value=256, step=16)
        
        # â­ STGCNå›¾ç»“æ„é€‰æ‹©
        # â­ STGCNå›¾ç»“æ„é€‰æ‹©
        adj_method = 'chain'  # é»˜è®¤å€¼
        adj_threshold = 10
        adj_sigma = 5
        adj_rows = 5
        adj_k = 8
        
        if 'STGCN' in model_type:
            st.markdown("### ğŸ”— å›¾ç»“æ„é…ç½®")
            adj_method = st.selectbox(
                "é‚»æ¥çŸ©é˜µç”Ÿæˆæ–¹æ³•",
                ["adaptive", "grid", "chain", "knn"],
                index=0,
                help="""
                adaptive: è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾ï¼ˆæ¨èï¼ŒRÂ²æå‡10-20%ï¼‰
                grid: ç½‘æ ¼ç»“æ„ï¼ˆé€‚åˆè§„åˆ™æ’åˆ—ï¼‰
                chain: é“¾å¼ç»“æ„ï¼ˆç®€å•åœºæ™¯ï¼‰
                knn: Kè¿‘é‚»å›¾ï¼ˆçµæ´»è¿æ¥ï¼‰
                """
            )
            
            if adj_method == 'adaptive':
                col_a, col_b = st.columns(2)
                with col_a:
                    adj_threshold = st.slider("è·ç¦»é˜ˆå€¼", 5, 20, 10, help="è¶…è¿‡æ­¤è·ç¦»çš„æ”¯æ¶ä¸è¿æ¥")
                with col_b:
                    adj_sigma = st.slider("é«˜æ–¯æ ¸å‚æ•°", 2, 10, 5, help="æ§åˆ¶æƒé‡è¡°å‡é€Ÿåº¦")
            elif adj_method == 'grid':
                adj_rows = st.number_input("ç½‘æ ¼è¡Œæ•°", 1, 20, 5)
            elif adj_method == 'knn':
                adj_k = st.number_input("Kè¿‘é‚»æ•°", 1, 20, 8)
        
        # ä¼˜åŒ–å»ºè®®
        with st.expander("ğŸ’¡ è®­ç»ƒä¼˜åŒ–å»ºè®® - ç›®æ ‡RÂ²â‰¥0.8"):
            st.markdown("""
            **â­ æ¨èé…ç½®ï¼ˆå†²å‡»RÂ²â‰¥0.8ï¼‰ï¼š**
            
            1. **æ•°æ®æ ¼å¼** â†’ å®Œæ•´æ—¶ç©ºæ•°æ®ï¼ˆå¿…é€‰ï¼‰
            2. **ç‰¹å¾å·¥ç¨‹** â†’ å¯ç”¨ï¼ˆå¿…é€‰ï¼Œæ–°å¢10+ç‰¹å¾ï¼‰
            3. **æ¨¡å‹é€‰æ‹©** â†’ Transformer æˆ– STGCN + adaptiveå›¾
            4. **è®­ç»ƒè½®æ•°** â†’ 100-150è½®
            5. **æ‰¹æ¬¡å¤§å°** â†’ 128ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œæ•ˆæœï¼‰
            6. **å­¦ä¹ ç‡** â†’ 0.001ï¼ˆå·²å«warmupï¼‰
            
            **å¦‚æœæ•ˆæœä¸å¥½ï¼Œå¯ä»¥å°è¯•ï¼š**
            
            1. **Transformeræ¨¡å‹** â†’ d_model=128, nhead=8ï¼ˆæ¨èï¼‰
            2. **STGCN + adaptiveå›¾** â†’ threshold=10, sigma=5
            3. **å¢åŠ è®­ç»ƒè½®æ•°** â†’ æ”¹ä¸º 150-200 è½®
            4. **è°ƒæ•´å­¦ä¹ ç‡** â†’ å°è¯• 0.0005-0.002 ä¹‹é—´
            
            **å½“å‰ä¼˜åŒ–ï¼š**
            - âœ… å¢å¼ºç‰¹å¾å·¥ç¨‹ï¼ˆç»Ÿè®¡ã€å·®åˆ†ã€æ»‘åŠ¨çª—å£ã€äº¤å‰ç‰¹å¾ï¼‰
            - âœ… è‡ªé€‚åº”è·ç¦»åŠ æƒå›¾ï¼ˆé«˜æ–¯æ ¸æƒé‡ï¼‰
            - âœ… Transformeræ¶æ„ï¼ˆSelf-Attentionæœºåˆ¶ï¼‰
            - âœ… å­¦ä¹ ç‡warmup + ä½™å¼¦é€€ç«
            - âœ… Huber Lossï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
            
            **ç†æƒ³æŒ‡æ ‡ï¼š**
            - MAE < 10 MPa
            - RMSE < 15 MPa  
            - RÂ² â‰¥ 0.8
            """)
        
        if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary"):
            try:
                st.success("å¼€å§‹è®­ç»ƒSTGCNæ¨¡å‹...")
                
                # 1. æ•°æ®åˆ‡åˆ†
                st.write("### æ­¥éª¤1: æ•°æ®åˆ‡åˆ†")
                
                # â­ æ ¹æ®æ•°æ®æ ¼å¼è®¡ç®—å®é™…æ ·æœ¬æ•°
                actual_num_samples = len(X)
                
                # æ£€æŸ¥æ ·æœ¬æ•°æ˜¯å¦è¶³å¤Ÿ
                if actual_num_samples < 100:
                    st.error(f"""
                    âŒ **æ•°æ®é‡ä¸è¶³ï¼**
                    
                    å½“å‰æ ·æœ¬æ•°: {actual_num_samples}
                    æœ€å°‘éœ€è¦: 100æ ·æœ¬
                    
                    **å¯èƒ½åŸå› ï¼š**
                    1. å®Œæ•´æ—¶ç©ºæ•°æ®é‡æ„åæ ·æœ¬æ•°å¤§å¹…å‡å°‘ï¼ˆåŸ195,836 â†’ {actual_num_samples}ï¼‰
                    2. æ•°æ®ä¸­ç¼ºå¤±å€¼è¿‡å¤šï¼Œå¯¼è‡´å®Œæ•´æ—¶é—´æ­¥è¾ƒå°‘
                    
                    **è§£å†³æ–¹æ¡ˆï¼š**
                    1. â­ åˆ‡æ¢åˆ°"å•æ ·æœ¬åºåˆ—æ ¼å¼"ï¼ˆä¸é‡æ„ï¼Œä¿ç•™å…¨éƒ¨æ ·æœ¬ï¼‰
                    2. æ£€æŸ¥åŸå§‹CSVæ•°æ®è´¨é‡
                    3. è°ƒæ•´æ—¶åºçª—å£å‚æ•°ï¼ˆå‡å°seq_lenï¼‰
                    """)
                    st.stop()
                
                # é‡æ–°è®¡ç®—åˆ‡åˆ†ç‚¹ï¼ˆåŸºäºå®é™…æ ·æœ¬æ•°ï¼‰
                train_end_actual = int(actual_num_samples * train_ratio)
                val_end_actual = int(actual_num_samples * (train_ratio + val_ratio))
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›æ ·æœ¬
                if train_end_actual < 10:
                    st.error(f"è®­ç»ƒé›†æ ·æœ¬æ•°å¤ªå°‘({train_end_actual})ï¼Œè¯·å¢åŠ train_ratioæˆ–åˆ‡æ¢æ•°æ®æ ¼å¼")
                    st.stop()
                
                if val_end_actual - train_end_actual < 5:
                    st.error(f"éªŒè¯é›†æ ·æœ¬æ•°å¤ªå°‘({val_end_actual - train_end_actual})ï¼Œè¯·å¢åŠ val_ratio")
                    st.stop()
                
                st.info(f"""
                ğŸ“Š **å®é™…æ•°æ®åˆ‡åˆ†ï¼ˆåŸºäº {actual_num_samples} ä¸ªæ ·æœ¬ï¼‰ï¼š**
                - è®­ç»ƒé›†: {train_end_actual} æ ·æœ¬ ({train_ratio*100:.0f}%)
                - éªŒè¯é›†: {val_end_actual - train_end_actual} æ ·æœ¬ ({val_ratio*100:.0f}%)
                - æµ‹è¯•é›†: {actual_num_samples - val_end_actual} æ ·æœ¬ ({(1-train_ratio-val_ratio)*100:.0f}%)
                """)
                
                # æ ¹æ®æ•°æ®æ ¼å¼ä¸åŒï¼Œé‡‡ç”¨ä¸åŒçš„åˆ‡åˆ†æ–¹å¼
                if use_spatial_reconstruction:
                    # æ—¶ç©ºæ•°æ®æ ¼å¼ï¼š(num_timesteps, num_supports, seq_len, num_features)
                    # ç›®æ ‡ï¼š(num_timesteps, num_supports)
                    st.info("ä½¿ç”¨å®Œæ•´æ—¶ç©ºæ•°æ®æ ¼å¼")
                    
                    X_train = X[:train_end_actual]  # (T_train, N, seq_len, F)
                    y_train = y_final[:train_end_actual]  # (T_train, N)
                    train_support_ids = None  # ä¸å†éœ€è¦
                    
                    X_val = X[train_end_actual:val_end_actual]
                    y_val = y_final[train_end_actual:val_end_actual]
                    val_support_ids = None
                    
                    X_test = X[val_end_actual:]
                    y_test = y_final[val_end_actual:]
                    test_support_ids = None
                    
                else:
                    # å•æ”¯æ¶åºåˆ—æ ¼å¼ï¼š(num_samples, seq_len, num_features)
                    st.info("ä½¿ç”¨å•æ”¯æ¶åºåˆ—æ ¼å¼")
                    
                    X_train = X[:train_end_actual]
                    y_train = y_final[:train_end_actual]
                    train_support_ids = support_ids[:train_end_actual]
                    
                    X_val = X[train_end_actual:val_end_actual]
                    y_val = y_final[train_end_actual:val_end_actual]
                    val_support_ids = support_ids[train_end_actual:val_end_actual]
                    
                    X_test = X[val_end_actual:]
                    y_test = y_final[val_end_actual:]
                    test_support_ids = support_ids[val_end_actual:]
                
                st.write(f"âœ“ è®­ç»ƒé›†: {len(X_train):,} {'æ—¶é—´æ­¥' if use_spatial_reconstruction else 'æ ·æœ¬'}")
                st.write(f"âœ“ éªŒè¯é›†: {len(X_val):,} {'æ—¶é—´æ­¥' if use_spatial_reconstruction else 'æ ·æœ¬'}")
                st.write(f"âœ“ æµ‹è¯•é›†: {len(X_test):,} {'æ—¶é—´æ­¥' if use_spatial_reconstruction else 'æ ·æœ¬'}")
                
                # â­ ç‰¹å¾å·¥ç¨‹
                if use_feature_engineering:
                    st.write("### æ­¥éª¤1.5: ç‰¹å¾å·¥ç¨‹ ğŸ”§")
                    st.info("æ­£åœ¨ç”Ÿæˆå·¥ç¨‹ç‰¹å¾...")
                    
                    original_feature_count = X_train.shape[-1]
                    
                    X_train, new_feature_names = add_engineered_features(X_train, feature_names)
                    X_val, _ = add_engineered_features(X_val, feature_names)
                    X_test, _ = add_engineered_features(X_test, feature_names)
                    
                    enhanced_feature_count = X_train.shape[-1]
                    added_features = enhanced_feature_count - original_feature_count
                    
                    st.success(f"""
                    âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼
                    - åŸå§‹ç‰¹å¾æ•°: {original_feature_count}
                    - æ–°å¢ç‰¹å¾æ•°: {added_features}
                    - æ€»ç‰¹å¾æ•°: {enhanced_feature_count}
                    - é¢„æœŸRÂ²æå‡: +10-20%
                    """)
                    
                    # æ›´æ–°feature_names
                    feature_names = new_feature_names
                
                # è·å–é‚»æ¥çŸ©é˜µ
                A_hat = adj_mx
                
                # 2. æ•°æ®å‡†å¤‡
                st.write("### æ­¥éª¤2: å‡†å¤‡GPUè®¡ç®—")
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                st.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
                
                # â­ æ ¹æ®æ•°æ®æ ¼å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
                if use_spatial_reconstruction:
                    # å®Œæ•´æ—¶ç©ºæ•°æ®ï¼š(T, N, seq_len, F)
                    st.write("### æ­¥éª¤3: æ•°æ®å½’ä¸€åŒ–ï¼ˆæ—¶ç©ºæ ¼å¼ï¼‰")
                    st.info("ä½¿ç”¨å®Œæ•´æ—¶ç©ºæ•°æ®ï¼Œæ— éœ€é‡ç»„å›¾ç»“æ„")
                    
                    # â­ æ•°æ®éªŒè¯
                    if y_train.size == 0:
                        st.error("""
                        âŒ **è®­ç»ƒæ•°æ®ä¸ºç©ºï¼**
                        
                        è¿™é€šå¸¸å‘ç”Ÿåœ¨å®Œæ•´æ—¶ç©ºæ•°æ®é‡æ„æ—¶ï¼Œå¯èƒ½åŸå› ï¼š
                        1. æ•°æ®åˆ‡åˆ†åè®­ç»ƒé›†ä¸ºç©º
                        2. æ—¶ç©ºé‡æ„å¤±è´¥
                        
                        **è§£å†³æ–¹æ¡ˆï¼šè¯·åˆ‡æ¢åˆ°"å•æ ·æœ¬åºåˆ—æ ¼å¼"**
                        """)
                        st.stop()
                    
                    # ä¸ºäº†å½’ä¸€åŒ–ï¼Œéœ€è¦flatten
                    # y_train: (T, N) â†’ flatten to (T*N,)
                    y_train_flat = y_train.reshape(-1)
                    y_val_flat = y_val.reshape(-1)
                    
                    # â­ å†æ¬¡æ£€æŸ¥flattenåæ˜¯å¦ä¸ºç©º
                    if y_train_flat.size == 0:
                        st.error("è®­ç»ƒé›†ç›®æ ‡å€¼ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®")
                        st.stop()
                    
                    # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
                    y_mean = y_train_flat.mean()
                    y_std = y_train_flat.std()
                    y_min = y_train_flat.min()
                    y_max = y_train_flat.max()
                    y_range = y_max - y_min
                    if y_range < 1e-6:
                        y_range = 1.0
                    
                    # MinMaxå½’ä¸€åŒ–
                    y_train_normalized = (y_train - y_min) / y_range  # (T, N)
                    y_val_normalized = (y_val - y_min) / y_range
                    
                    # ç‰¹å¾å½’ä¸€åŒ–
                    X_train_normalized = X_train.copy()
                    X_val_normalized = X_val.copy()
                    
                    seq_len = X_train.shape[2]
                    num_features = X_train.shape[3]
                    
                    for feat_idx in range(num_features):
                        # å¯¹æ‰€æœ‰æ—¶é—´æ­¥å’Œæ‰€æœ‰æ”¯æ¶çš„è¯¥ç‰¹å¾å½’ä¸€åŒ–
                        feat_data = X_train[:, :, :, feat_idx]  # (T, N, seq_len)
                        feat_min = feat_data.min()
                        feat_max = feat_data.max()
                        feat_range = feat_max - feat_min
                        if feat_range < 1e-6:
                            feat_range = 1.0
                        
                        X_train_normalized[:, :, :, feat_idx] = (X_train[:, :, :, feat_idx] - feat_min) / feat_range
                        X_val_normalized[:, :, :, feat_idx] = (X_val[:, :, :, feat_idx] - feat_min) / feat_range
                    
                    support_to_idx = None
                    num_nodes = X_train.shape[1]  # N
                    
                else:
                    # å•æ”¯æ¶åºåˆ—æ•°æ®ï¼šåŸæœ‰é€»è¾‘
                    st.write("### æ­¥éª¤3: æ•°æ®å½’ä¸€åŒ–ï¼ˆå•æ”¯æ¶æ ¼å¼ï¼‰")
                    
                    # ä¸ºæ¯ä¸ªæ ·æœ¬æ‰¾åˆ°å¯¹åº”çš„supportç´¢å¼•
                    unique_supports_list = np.unique(support_ids)
                    support_to_idx = {sup_id: idx for idx, sup_id in enumerate(unique_supports_list)}
                    num_nodes = len(unique_supports_list)
                    
                    st.write(f"å›¾èŠ‚ç‚¹æ•°: {num_nodes}")
                    
                    # åŸæœ‰çš„å½’ä¸€åŒ–é€»è¾‘
                    y_mean = y_train.mean()
                    y_std = y_train.std()
                    y_min = y_train.min()
                    y_max = y_train.max()
                    y_range = y_max - y_min
                    if y_range < 1e-6:
                        y_range = 1.0
                    
                    y_train_normalized = (y_train - y_min) / y_range
                    y_val_normalized = (y_val - y_min) / y_range
                    
                    X_train_normalized = X_train.copy()
                    X_val_normalized = X_val.copy()
                    
                    seq_len = X_train.shape[1]
                    num_features = X_train.shape[2]
                    
                    for feat_idx in range(num_features):
                        feat_min = X_train[:, :, feat_idx].min()
                        feat_max = X_train[:, :, feat_idx].max()
                        feat_range = feat_max - feat_min
                        if feat_range < 1e-6:
                            feat_range = 1.0
                        
                        X_train_normalized[:, :, feat_idx] = (X_train[:, :, feat_idx] - feat_min) / feat_range
                        X_val_normalized[:, :, feat_idx] = (X_val[:, :, feat_idx] - feat_min) / feat_range
                
                # æ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
                st.info(f"""
                **ğŸ“Š ç›®æ ‡å˜é‡ç»Ÿè®¡åˆ†æï¼š**
                - å‡å€¼: {y_mean:.2f} MPa
                - æ ‡å‡†å·®: {y_std:.2f} MPa
                - èŒƒå›´: [{y_min:.2f}, {y_max:.2f}] MPa
                - å˜å¼‚ç³»æ•°(CV): {(y_std/y_mean)*100:.1f}%
                - æ ·æœ¬æ•°: {len(y_train_normalized.flatten()):,}
                
                **ğŸ’¡ å¯é¢„æµ‹æ€§åˆ†æï¼š**
                - CV < 30%: æ•°æ®å˜åŒ–è¾ƒå°ï¼Œè¾ƒéš¾é¢„æµ‹
                - CV 30-50%: ä¸­ç­‰å˜åŒ–ï¼Œé€‚åˆé¢„æµ‹
                - CV > 50%: å˜åŒ–å¤§ï¼Œæ¨¡å¼æ˜æ˜¾
                
                å½“å‰ CV={(y_std/y_mean)*100:.1f}% ({'åä½ï¼Œé¢„æµ‹éš¾åº¦å¤§' if (y_std/y_mean) < 0.3 else 'ä¸­ç­‰' if (y_std/y_mean) < 0.5 else 'è¾ƒé«˜ï¼Œæœ‰åˆ©äºé¢„æµ‹'})
                """)
                
                # æ ¹æ®æ¨¡å‹ç±»å‹å‡†å¤‡æ•°æ®
                st.write("### æ­¥éª¤4: å‡†å¤‡æ¨¡å‹è¾“å…¥æ•°æ®")
                
                if use_spatial_reconstruction:
                    # æ—¶ç©ºæ•°æ®æ ¼å¼å·²ç»æ˜¯å®Œæ•´çš„
                    # (T, N, seq_len, F) â†’ STGCNéœ€è¦ (T, F, N, seq_len)
                    if "LSTM" in model_type or "Transformer" in model_type:
                        # LSTM/Transformer: éœ€è¦flattenç©ºé—´ç»´åº¦ï¼Œæˆ–é€‰æ‹©ç‰¹å®šæ”¯æ¶
                        # è¿™é‡Œæˆ‘ä»¬flattenæ‰€æœ‰æ”¯æ¶ï¼Œå°†å…¶è§†ä¸ºç‹¬ç«‹æ ·æœ¬
                        T, N, seq_len, F = X_train_normalized.shape
                        X_train_flat = X_train_normalized.reshape(T * N, seq_len, F)
                        y_train_flat = y_train_normalized.reshape(T * N, 1)
                        
                        X_val_flat = X_val_normalized.reshape(-1, seq_len, F)
                        y_val_flat = y_val_normalized.reshape(-1, 1)
                        
                        train_X_tensor = torch.FloatTensor(X_train_flat)
                        train_y_tensor = torch.FloatTensor(y_train_flat)
                        val_X_tensor = torch.FloatTensor(X_val_flat)
                        val_y_tensor = torch.FloatTensor(y_val_flat)
                        
                        model_type_short = "LSTM/Transformer" if "Transformer" in model_type else "LSTM"
                        st.write(f"{model_type_short}æ¨¡å¼ - è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                        st.write(f"{model_type_short}æ¨¡å¼ - éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                        
                    else:
                        # STGCN: è½¬æ¢ç»´åº¦ (T, N, seq_len, F) â†’ (T, F, N, seq_len)
                        X_train_stgcn = np.transpose(X_train_normalized, (0, 3, 1, 2))
                        X_val_stgcn = np.transpose(X_val_normalized, (0, 3, 1, 2))
                        
                        # y: (T, N) â†’ (T, 1, N, 1)
                        y_train_stgcn = y_train_normalized[:, np.newaxis, :, np.newaxis]
                        y_val_stgcn = y_val_normalized[:, np.newaxis, :, np.newaxis]
                        
                        train_X_tensor = torch.FloatTensor(X_train_stgcn)
                        train_y_tensor = torch.FloatTensor(y_train_stgcn)
                        val_X_tensor = torch.FloatTensor(X_val_stgcn)
                        val_y_tensor = torch.FloatTensor(y_val_stgcn)
                        A_hat_tensor = torch.FloatTensor(A_hat).to(device)
                        
                        st.write(f"STGCNæ¨¡å¼ - è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                        st.write(f"STGCNæ¨¡å¼ - éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                
                else:
                    # å•æ”¯æ¶åºåˆ—æ ¼å¼ï¼šåŸæœ‰é€»è¾‘
                    if "STGCN" in model_type:
                        # âš ï¸ å•æ ·æœ¬æ ¼å¼ä¸æ”¯æŒSTGCN
                        st.error("""
                        âŒ **å•æ ·æœ¬åºåˆ—æ ¼å¼ä¸æ”¯æŒSTGCNæ¨¡å‹ï¼**
                        
                        **åŸå› ï¼š**
                        - STGCNéœ€è¦å®Œæ•´çš„ç©ºé—´æ‹“æ‰‘ç»“æ„ï¼ˆæ‰€æœ‰125ä¸ªæ”¯æ¶åŒæ—¶å­˜åœ¨ï¼‰
                        - å•æ ·æœ¬æ ¼å¼æ¯ä¸ªæ ·æœ¬åªåŒ…å«1ä¸ªæ”¯æ¶çš„æ•°æ®
                        - å¼ºè¡Œè½¬æ¢ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºï¼ˆéœ€è¦111GB+ï¼‰
                        
                        **è§£å†³æ–¹æ¡ˆï¼ˆ3é€‰1ï¼‰ï¼š**
                        
                        1ï¸âƒ£ **æ¨èï¼šåˆ‡æ¢åˆ°Transformeræ¨¡å‹** â­â­â­
                           - ä¿æŒ"å•æ ·æœ¬åºåˆ—æ ¼å¼"
                           - é€‰æ‹©"Transformer (æœ€å¼ºè¡¨è¾¾åŠ›)ğŸš€"
                           - é¢„æœŸRÂ²: 0.65-0.80
                        
                        2ï¸âƒ£ **åˆ‡æ¢åˆ°AttentionLSTMæ¨¡å‹** â­â­
                           - ä¿æŒ"å•æ ·æœ¬åºåˆ—æ ¼å¼"
                           - é€‰æ‹©"AttentionLSTM (æ³¨æ„åŠ›å¢å¼º)â­"
                           - é¢„æœŸRÂ²: 0.45-0.55
                        
                        3ï¸âƒ£ **åˆ‡æ¢åˆ°å®Œæ•´æ—¶ç©ºæ•°æ®æ ¼å¼**
                           - é€‰æ‹©"å®Œæ•´æ—¶ç©ºæ•°æ®ï¼ˆæ¨èï¼Œé¢„æœŸRÂ²>0.5ï¼‰"
                           - ç„¶åå¯ä»¥ä½¿ç”¨STGCN
                           - âš ï¸ ä½†æ ·æœ¬æ•°ä¼šå¤§å¹…å‡å°‘ï¼ˆå¯èƒ½<100ï¼‰
                        
                        **å½“å‰æœ€ä¼˜é€‰æ‹©ï¼šæ–¹æ¡ˆ1ï¼ˆå•æ ·æœ¬+Transformerï¼‰**
                        """)
                        st.stop()
                    
                    else:
                        # LSTM/AttentionLSTM/Transformer: ç›´æ¥ä½¿ç”¨åºåˆ—æ•°æ®
                        train_X_tensor = torch.FloatTensor(X_train_normalized)
                        train_y_tensor = torch.FloatTensor(y_train_normalized).view(-1, 1)
                        val_X_tensor = torch.FloatTensor(X_val_normalized)
                        val_y_tensor = torch.FloatTensor(y_val_normalized).view(-1, 1)
                        
                        st.write(f"è®­ç»ƒé›†: X {train_X_tensor.shape}, y {train_y_tensor.shape}")
                        st.write(f"éªŒè¯é›†: X {val_X_tensor.shape}, y {val_y_tensor.shape}")
                        st.write(f"y å½’ä¸€åŒ–èŒƒå›´: [{train_y_tensor.min():.4f}, {train_y_tensor.max():.4f}]")
                
                # åˆå§‹åŒ–æ¨¡å‹
                if use_spatial_reconstruction and "STGCN" in model_type:
                    seq_len = X_train_normalized.shape[2]
                    num_features = X_train_normalized.shape[3]
                else:
                    seq_len = X_train_normalized.shape[1 if not use_spatial_reconstruction else 2]
                    num_features = X_train_normalized.shape[2 if not use_spatial_reconstruction else 3]
                
                pred_len = 1
                
                if "LSTM (åŸºç¡€ç‰ˆ)" in model_type:
                    model = SimpleLSTM(
                        num_features=num_features,
                        hidden_dim=hidden_dim * 2,  # LSTM ç”¨æ›´å¤§çš„éšè—å±‚
                        num_layers=2
                    ).to(device)
                elif "AttentionLSTM" in model_type:
                    model = AttentionLSTM(
                        num_features=num_features,
                        hidden_dim=hidden_dim * 2,  # æ³¨æ„åŠ›LSTMç”¨æ›´å¤§çš„éšè—å±‚
                        num_layers=2
                    ).to(device)
                    st.success("âœ¨ ä½¿ç”¨æ³¨æ„åŠ›å¢å¼ºLSTMï¼Œé¢„æœŸæå‡5-15%")
                elif "Transformer" in model_type:
                    # â­ Transformeræ¨¡å‹ - æœ€å¼ºè¡¨è¾¾èƒ½åŠ›
                    model = TransformerPredictor(
                        num_features=num_features,
                        d_model=hidden_dim,  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„hidden_dim
                        nhead=8,
                        num_encoder_layers=3,
                        dim_feedforward=hidden_dim * 4
                    ).to(device)
                    st.success("ğŸš€ ä½¿ç”¨Transformeræ¨¡å‹ï¼Œæœ€å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œé¢„æœŸRÂ²â‰¥0.8")
                else:
                    # STGCNæ¨¡å‹
                    # è·å–å›¾ç»“æ„å‚æ•°
                    adj_params = {}
                    if adj_method == 'adaptive':
                        adj_params = {'threshold': adj_threshold, 'sigma': adj_sigma}
                    elif adj_method == 'grid':
                        adj_params = {'rows': adj_rows}
                    elif adj_method == 'knn':
                        adj_params = {'k': adj_k}
                    
                    model = STGCN(
                        num_nodes=num_nodes,
                        num_features=num_features,
                        seq_len=seq_len,
                        pred_len=pred_len,
                        hidden_dim=hidden_dim,  # ä¼ å…¥hidden_dimå‚æ•°
                        Kt=3
                    ).to(device)
                
                st.write(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
                
                # æ˜¾ç¤ºæ•°æ®å’Œæ¨¡å‹ä¿¡æ¯
                model_name = "SimpleLSTM" if "LSTM (åŸºç¡€ç‰ˆ)" in model_type else \
                             "AttentionLSTM" if "AttentionLSTM" in model_type else \
                             "Transformer" if "Transformer" in model_type else "STGCN"
                st.info(f"""
                **{model_name} æ¨¡å‹é…ç½®ï¼š**
                - è¾“å…¥ç»´åº¦: {train_X_tensor.shape}
                - è¾“å‡ºç»´åº¦: {train_y_tensor.shape}
                - ç‰¹å¾æ•°: {num_features}
                - åºåˆ—é•¿åº¦: {seq_len}
                - éšè—å±‚: {hidden_dim * 2 if 'LSTM' in model_type else hidden_dim}
                {'- Transformerå±‚æ•°: 3, æ³¨æ„åŠ›å¤´æ•°: 8' if 'Transformer' in model_type else ''}
                {'- å›¾ç»“æ„: ' + adj_method if 'STGCN' in model_type else ''}
                """)
                
                # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
                # â­ ä½¿ç”¨Huber Lossæ›¿ä»£MSEï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
                criterion = nn.SmoothL1Loss()  # Huber Lossçš„PyTorchå®ç°
                st.info("âœ… ä½¿ç”¨Huber Lossï¼ˆå¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼‰")
                
                # æ·»åŠ L2æ­£åˆ™åŒ–(weight_decay)æ¥é˜²æ­¢è¿‡æ‹Ÿåˆ
                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
                
                # â­ å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨
                def get_lr_scheduler(optimizer, warmup_epochs=5):
                    """
                    å­¦ä¹ ç‡é¢„çƒ­+ä½™å¼¦é€€ç«
                    """
                    from torch.optim.lr_scheduler import LambdaLR
                    import math
                    
                    def lr_lambda(epoch):
                        if epoch < warmup_epochs:
                            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿
                            return (epoch + 1) / warmup_epochs
                        else:
                            # ä½™å¼¦é€€ç«
                            progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)
                            return 0.5 * (1 + math.cos(math.pi * progress))
                    
                    return LambdaLR(optimizer, lr_lambda)
                
                scheduler = get_lr_scheduler(optimizer, warmup_epochs=5)
                st.info("âœ… ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­+ä½™å¼¦é€€ç«ç­–ç•¥")
                
                # æ—©åœå‚æ•° - æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´patience
                early_stop_patience = 30 if "STGCN" in model_type else 25
                early_stop_counter = 0
                
                # è®­ç»ƒå¾ªç¯
                progress_bar = st.progress(0)
                status_text = st.empty()
                metrics_placeholder = st.empty()
                
                # å­˜å‚¨æŸå¤±å†å²
                train_losses = []
                val_losses = []
                best_val_loss = float('inf')
                
                # åˆ›å»ºDataLoader (è®­ç»ƒå’ŒéªŒè¯éƒ½ä½¿ç”¨æ‰¹å¤„ç†)
                from torch.utils.data import TensorDataset, DataLoader
                train_dataset = TensorDataset(train_X_tensor, train_y_tensor)
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                
                val_dataset = TensorDataset(val_X_tensor, val_y_tensor)
                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                
                start_time = time.time()
                
                for epoch in range(epochs):
                    # è®­ç»ƒé˜¶æ®µ
                    model.train()
                    epoch_train_loss = 0
                    batch_count = 0
                    
                    for batch_X, batch_y in train_loader:
                        # å°†æ•°æ®ç§»åˆ°GPU
                        batch_X = batch_X.to(device)
                        batch_y = batch_y.to(device)
                        
                        optimizer.zero_grad()
                        
                        # å‰å‘ä¼ æ’­
                        if "STGCN" in model_type:
                            outputs = model(batch_X, A_hat_tensor)  # (B, 1, N, 1)
                            loss = criterion(outputs, batch_y)
                        else:
                            # LSTM/AttentionLSTM/Transformer
                            outputs = model(batch_X)  # (B, 1)
                            loss = criterion(outputs, batch_y)
                        
                        # æ³¨æ„ï¼šä¸åœ¨è®­ç»ƒæ—¶clampï¼Œè®©æ¨¡å‹è‡ªç”±å­¦ä¹ 
                        # åªåœ¨éªŒè¯/æµ‹è¯•æ—¶clampç”¨äºè¯„ä¼°æŒ‡æ ‡
                        
                        # åå‘ä¼ æ’­
                        loss.backward()
                        
                        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        
                        optimizer.step()
                        
                        epoch_train_loss += loss.item()
                        batch_count += 1
                    
                    avg_train_loss = epoch_train_loss / batch_count
                    train_losses.append(avg_train_loss)
                    
                    # éªŒè¯é˜¶æ®µ (ä½¿ç”¨æ‰¹å¤„ç†é¿å…æ˜¾å­˜æº¢å‡º)
                    model.eval()
                    val_loss_sum = 0
                    all_preds = []
                    all_targets = []
                    val_batch_count = 0
                    
                    with torch.no_grad():
                        for val_batch_X, val_batch_y in val_loader:
                            val_batch_X = val_batch_X.to(device)
                            val_batch_y = val_batch_y.to(device)
                            
                            if "STGCN" in model_type:
                                val_batch_outputs = model(val_batch_X, A_hat_tensor)
                            else:
                                # LSTM/AttentionLSTM/Transformer
                                val_batch_outputs = model(val_batch_X)
                            
                            # ç´¯ç§¯æŸå¤±ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰- ä¸clampï¼Œä½¿ç”¨åŸå§‹è¾“å‡º
                            batch_loss = criterion(val_batch_outputs, val_batch_y).item()
                            val_loss_sum += batch_loss * len(val_batch_X)
                            
                            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºåç»­è®¡ç®—
                            all_preds.append(val_batch_outputs.cpu())
                            all_targets.append(val_batch_y.cpu())
                            
                            val_batch_count += len(val_batch_X)
                            
                            # æ¸…ç†æ˜¾å­˜
                            del val_batch_X, val_batch_y, val_batch_outputs
                            torch.cuda.empty_cache()
                        
                        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
                        all_preds = torch.cat(all_preds, dim=0)  # (N, ...)
                        all_targets = torch.cat(all_targets, dim=0)  # (N, ...)
                        
                        # å±•å¹³ä¸ºä¸€ç»´å‘é‡ç”¨äºè®¡ç®—æŒ‡æ ‡
                        if "STGCN" in model_type:
                            # STGCN: éœ€è¦æå–éé›¶å€¼
                            # all_preds: (B, 1, N, 1)
                            # all_targets: (B, 1, N, 1)
                            # å‹ç¼©åˆ° (B, N) - æ³¨æ„é¡ºåºå’Œç»´åº¦
                            all_preds_2d = all_preds.squeeze(3).squeeze(1)  # (B, 1, N, 1) -> (B, 1, N) -> (B, N)
                            all_targets_2d = all_targets.squeeze(3).squeeze(1)  # (B, 1, N, 1) -> (B, 1, N) -> (B, N)
                            
                            # åˆ›å»ºmaskæ‰¾å‡ºéé›¶èŠ‚ç‚¹
                            mask = all_targets_2d != 0  # (B, N)
                            
                            # æå–éé›¶å€¼å¹¶å±•å¹³
                            all_preds_flat = all_preds_2d[mask]  # (num_nonzero,)
                            all_targets_flat = all_targets_2d[mask]  # (num_nonzero,)
                            
                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€ä¸ªepochæ˜¾ç¤ºï¼‰
                            if epoch == 0 or (epoch + 1) % 20 == 0:
                                num_nonzero = mask.sum().item()
                                st.write(f"ğŸ“Š STGCNè°ƒè¯•: æå–äº† {num_nonzero} ä¸ªéé›¶èŠ‚ç‚¹é¢„æµ‹å€¼")
                                st.write(f"ğŸ“Š åŸå§‹è¾“å‡ºèŒƒå›´: [{all_preds_flat.min():.4f}, {all_preds_flat.max():.4f}]")
                        else:
                            # LSTM/AttentionLSTM/Transformer
                            all_preds_flat = all_preds.squeeze()  # (N,)
                            all_targets_flat = all_targets.squeeze()  # (N,)
                        
                        # è£å‰ªå½’ä¸€åŒ–é¢„æµ‹å€¼åˆ°[0,1]èŒƒå›´ï¼Œä»…ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡
                        # æ³¨æ„ï¼šè¿™ä¸ä¼šå½±å“è®­ç»ƒï¼Œåªå½±å“æ˜¾ç¤ºçš„æŒ‡æ ‡
                        all_preds_flat_clamped = torch.clamp(all_preds_flat, 0.0, 1.0)
                        
                        # æ·»åŠ å½’ä¸€åŒ–ç©ºé—´çš„è°ƒè¯•ä¿¡æ¯ - å‰5ä¸ªepochæ¯æ¬¡éƒ½æ˜¾ç¤º
                        if epoch < 5 or epoch == 0 or (epoch + 1) % 20 == 0:
                            st.write(f"ğŸ“Š å½’ä¸€åŒ–ç©ºé—´(clampå) - é¢„æµ‹å€¼èŒƒå›´: [{all_preds_flat_clamped.min():.4f}, {all_preds_flat_clamped.max():.4f}]")
                            st.write(f"ğŸ“Š å½’ä¸€åŒ–ç©ºé—´ - çœŸå®å€¼èŒƒå›´: [{all_targets_flat.min():.4f}, {all_targets_flat.max():.4f}]")
                            st.write(f"ğŸ“Š å½’ä¸€åŒ–ç©ºé—´ - MSE: {torch.mean((all_preds_flat_clamped - all_targets_flat)**2).item():.6f}")
                        
                        # åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦ (MinMax åå˜æ¢)
                        all_preds_original = all_preds_flat_clamped * y_range + y_min
                        all_targets_original = all_targets_flat * y_range + y_min
                        
                        # è®¡ç®—åŸå§‹å°ºåº¦çš„æŒ‡æ ‡
                        mae = torch.mean(torch.abs(all_preds_original - all_targets_original)).item()
                        rmse = torch.sqrt(torch.mean((all_preds_original - all_targets_original)**2)).item()
                        
                        # RÂ² (åœ¨åŸå§‹å°ºåº¦è®¡ç®—) - ä½¿ç”¨æ›´ç¨³å¥çš„æ–¹å¼
                        y_mean_original = torch.mean(all_targets_original)
                        ss_tot = torch.sum((all_targets_original - y_mean_original)**2).item()
                        ss_res = torch.sum((all_targets_original - all_preds_original)**2).item()
                        
                        # æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯ - å‰5ä¸ªepochæ¯æ¬¡éƒ½æ˜¾ç¤º
                        if epoch < 5 or epoch == 0 or (epoch + 1) % 20 == 0:
                            st.write(f"ğŸ“Š åŸå§‹å°ºåº¦ - é¢„æµ‹å€¼èŒƒå›´: [{all_preds_original.min():.2f}, {all_preds_original.max():.2f}] MPa")
                            st.write(f"ğŸ“Š åŸå§‹å°ºåº¦ - çœŸå®å€¼èŒƒå›´: [{all_targets_original.min():.2f}, {all_targets_original.max():.2f}] MPa")
                            st.write(f"ğŸ“Š çœŸå®å€¼å‡å€¼: {y_mean_original:.2f} MPa")
                            st.write(f"ğŸ“Š ss_tot={ss_tot:.2f}, ss_res={ss_res:.2f}, æ¯”ä¾‹={ss_res/ss_tot:.2f}")
                            st.write(f"ğŸ“Š åŸå§‹RÂ²å€¼(æœªè£å‰ª): {1 - ss_res/ss_tot:.4f}")
                        
                        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œåˆç†æ€§çº¦æŸ
                        if ss_tot < 1e-6:
                            # ç›®æ ‡æ–¹å·®å¤ªå°ï¼ŒRÂ²æ— æ„ä¹‰
                            r2 = 0.0
                        else:
                            r2_raw = 1 - ss_res / ss_tot
                            # å°†RÂ²é™åˆ¶åœ¨åˆç†èŒƒå›´ [-1, 1]ï¼Œé¿å…æ•°å€¼å¼‚å¸¸
                            if r2_raw < -1.0:
                                r2 = -1.0  # é¢„æµ‹éå¸¸å·®ï¼Œä½†ä¸è‡³äºå´©æºƒ
                            elif r2_raw > 1.0:
                                r2 = 1.0  # ä¸å¯èƒ½è¶…è¿‡1
                            else:
                                r2 = r2_raw
                        
                        # è®¡ç®—å¹³å‡æŸå¤±
                        val_loss = val_loss_sum / val_batch_count
                        val_losses.append(val_loss)
                        
                        # æ·»åŠ é¢„æµ‹å€¼èŒƒå›´ç›‘æ§ï¼ˆåŸå§‹å°ºåº¦ï¼‰
                        pred_min = all_preds_original.min().item()
                        pred_max = all_preds_original.max().item()
                        target_min = all_targets_original.min().item()
                        target_max = all_targets_original.max().item()
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        torch.save(model.state_dict(), 'best_stgcn_model.pth')
                        early_stop_counter = 0
                    else:
                        early_stop_counter += 1
                    
                    # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ¯ä¸ªepochè°ƒç”¨ï¼Œä¸éœ€è¦ä¼ å…¥val_lossï¼‰
                    scheduler.step()
                    current_lr = optimizer.param_groups[0]['lr']
                    
                    # æ—©åœæ£€æŸ¥
                    if early_stop_counter >= early_stop_patience:
                        st.warning(f"âš ï¸ éªŒè¯æŸå¤±è¿ç»­ {early_stop_patience} è½®æœªæ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                        break
                    
                    # æ›´æ–°è¿›åº¦
                    progress = (epoch + 1) / epochs
                    progress_bar.progress(progress)
                    
                    elapsed = time.time() - start_time
                    eta = elapsed / (epoch + 1) * (epochs - epoch - 1)
                    
                    status_text.text(
                        f"Epoch {epoch+1}/{epochs} | "
                        f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | "
                        f"éªŒè¯æŸå¤±: {val_loss:.4f} | "
                        f"RÂ²: {r2:.4f} | "
                        f"å­¦ä¹ ç‡: {current_lr:.6f} | "
                        f"å·²ç”¨æ—¶: {elapsed:.1f}s | ETA: {eta:.1f}s"
                    )
                    
                    # æ¯10ä¸ªepochæ›´æ–°ä¸€æ¬¡æŒ‡æ ‡
                    if (epoch + 1) % 10 == 0 or epoch == 0:
                        metrics_placeholder.markdown(f"""
                        ### å½“å‰æŒ‡æ ‡
                        - **è®­ç»ƒæŸå¤±**: {avg_train_loss:.6f}
                        - **éªŒè¯æŸå¤±**: {val_loss:.6f}
                        - **MAE**: {mae:.4f} MPa
                        - **RMSE**: {rmse:.4f} MPa
                        - **RÂ²**: {r2:.4f}
                        - **é¢„æµ‹èŒƒå›´**: [{pred_min:.2f}, {pred_max:.2f}] MPa
                        - **çœŸå®èŒƒå›´**: [{target_min:.2f}, {target_max:.2f}] MPa
                        - **ss_tot**: {ss_tot:.2f}, **ss_res**: {ss_res:.2f}
                        """)
                
                # è®­ç»ƒå®Œæˆ
                st.success("âœ… è®­ç»ƒå®Œæˆï¼")
                st.balloons()
                
                # ç»˜åˆ¶æŸå¤±æ›²çº¿
                st.subheader("ğŸ“ˆ è®­ç»ƒå†å²")
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
                ax.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss (MSE)')
                ax.set_title('è®­ç»ƒè¿‡ç¨‹')
                ax.legend()
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                
                # æœ€ç»ˆè¯„ä¼°
                st.subheader("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æœ€ä½³éªŒè¯æŸå¤±", f"{best_val_loss:.6f}")
                with col2:
                    st.metric("MAE", f"{mae:.4f} MPa")
                with col3:
                    st.metric("RMSE", f"{rmse:.4f} MPa")
                with col4:
                    st.metric("RÂ²", f"{r2:.4f}")
                
                st.info("æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: best_stgcn_model.pth")
                
                # é¢„æµ‹ç¤ºä¾‹ (ä½¿ç”¨å°æ‰¹æ¬¡é¿å…æ˜¾å­˜æº¢å‡º)
                st.subheader("ğŸ”® é¢„æµ‹ç¤ºä¾‹")
                model.load_state_dict(torch.load('best_stgcn_model.pth'))
                model.eval()
                
                # éšæœºé€‰æ‹©å‡ ä¸ªéªŒè¯æ ·æœ¬
                num_examples = min(5, len(val_X_tensor))
                indices = np.random.choice(len(val_X_tensor), num_examples, replace=False)
                
                with torch.no_grad():
                    example_X = val_X_tensor[indices].to(device)
                    example_y_true = val_y_tensor[indices].to(device)
                    
                    if "STGCN" in model_type:
                        example_y_pred = model(example_X, A_hat_tensor)
                        # è£å‰ªåˆ°[0,1]èŒƒå›´
                        example_y_pred = torch.clamp(example_y_pred, 0.0, 1.0)
                    else:
                        # LSTM/AttentionLSTM/Transformer
                        example_y_pred = model(example_X)  # (B, 1)
                        # è£å‰ªåˆ°[0,1]èŒƒå›´
                        example_y_pred = torch.clamp(example_y_pred, 0.0, 1.0)
                
                # åˆ›å»ºå¯¹æ¯”è¡¨
                comparison_data = []
                for i, idx in enumerate(indices):
                    sup_id = val_support_ids[idx]
                    
                    if "STGCN" in model_type:
                        # STGCN: ä»å›¾ç»“æ„ä¸­æå–
                        node_idx = support_to_idx[sup_id]
                        true_val_normalized = example_y_true[i, 0, node_idx, 0].cpu().item()
                        pred_val_normalized = example_y_pred[i, 0, node_idx, 0].cpu().item()
                    else:
                        # LSTM/AttentionLSTM/Transformer: ç›´æ¥è¾“å‡ºæ ‡é‡
                        true_val_normalized = example_y_true[i].cpu().item()
                        pred_val_normalized = example_y_pred[i].cpu().item()
                    
                    # åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦ (MinMax åå˜æ¢)
                    true_val = true_val_normalized * y_range + y_min
                    pred_val = pred_val_normalized * y_range + y_min
                    error = abs(pred_val - true_val)
                    
                    comparison_data.append({
                        'æ”¯æ¶ç¼–å·': sup_id,
                        'çœŸå®å€¼ (MPa)': f"{true_val:.2f}",
                        'é¢„æµ‹å€¼ (MPa)': f"{pred_val:.2f}",
                        'è¯¯å·® (MPa)': f"{error:.2f}",
                        'è¯¯å·®ç‡': f"{error/abs(true_val)*100:.1f}%" if abs(true_val) > 1e-6 else "N/A"
                    })
                
                st.table(pd.DataFrame(comparison_data))
                
                # æ¸…ç†æ˜¾å­˜
                del example_X, example_y_true, example_y_pred
                torch.cuda.empty_cache()
                
            except Exception as e:
                st.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
                import traceback
                st.code(traceback.format_exc())
        
    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        st.code(traceback.format_exc())

elif data_source == "ä¸Šä¼ CSVæ–‡ä»¶" and data_file:
    st.header("1. æ•°æ®åŠ è½½ä¸å¯¹é½")
    
    # è½½å…¥æ•°æ®
    try:
        # åŠ è½½çŸ¿å‹æ•°æ®
        data, column_names = load_csv_data(data_file)
        
        st.write(f"**çŸ¿å‹æ•°æ®å½¢çŠ¶:** {data.shape}")
        st.write(f"- æ—¶é—´æ­¥æ•°: {data.shape[0]}")
        st.write(f"- æ”¯æ¶æ•°é‡: {data.shape[1]}")
        st.write(f"- ç‰¹å¾æ•°: {data.shape[2]}")
        
        NUM_SAMPLES, NUM_NODES, NUM_FEATURES = data.shape
        
        # æ˜¾ç¤ºæ”¯æ¶åˆ—è¡¨
        with st.expander("ğŸ“‹ æŸ¥çœ‹æ”¯æ¶åˆ—è¡¨"):
            st.write(column_names)
        
        # åæ ‡å¯¹é½
        coords_array = None
        if coord_file:
            st.subheader("ğŸ—ºï¸ åæ ‡å¯¹é½")
            try:
                coord_df = load_coordinate_file(coord_file)
                st.write("**åæ ‡æ–‡ä»¶é¢„è§ˆ:**")
                st.dataframe(coord_df.head(10))
                
                # å¯¹é½åæ ‡
                coords_array, alignment_info = align_data_with_coordinates(column_names, coord_df)
                
                # æ˜¾ç¤ºå¯¹é½ç»“æœ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»æ”¯æ¶æ•°", alignment_info['total_supports'])
                with col2:
                    st.metric("æˆåŠŸåŒ¹é…", alignment_info['matched'], 
                             delta=f"{alignment_info['matched']/alignment_info['total_supports']*100:.1f}%")
                with col3:
                    st.metric("ç»´åº¦", "3D" if alignment_info['has_z'] else "2D")
                
                if alignment_info['missing']:
                    st.warning(f"âš ï¸ ä»¥ä¸‹ {len(alignment_info['missing'])} ä¸ªæ”¯æ¶æœªæ‰¾åˆ°åæ ‡: {', '.join(alignment_info['missing'][:5])}" + 
                              ("..." if len(alignment_info['missing']) > 5 else ""))
                else:
                    st.success("âœ… æ‰€æœ‰æ”¯æ¶åæ ‡å¯¹é½æˆåŠŸ!")
                
                # å¯è§†åŒ–æ”¯æ¶åˆ†å¸ƒ
                st.subheader("æ”¯æ¶ç©ºé—´åˆ†å¸ƒ")
                fig_scatter_data = pd.DataFrame({
                    'Xåæ ‡': coords_array[:, 0],
                    'Yåæ ‡': coords_array[:, 1],
                    'æ”¯æ¶': column_names
                })
                st.scatter_chart(fig_scatter_data, x='Xåæ ‡', y='Yåæ ‡', size=20)
                
            except Exception as e:
                st.error(f"åæ ‡å¯¹é½å¤±è´¥: {e}")
                st.info("å°†ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„åæ ‡")
        else:
            st.warning("âš ï¸ æœªä¸Šä¼ åæ ‡æ–‡ä»¶,å°†ä½¿ç”¨çº¿æ€§æ’åˆ—çš„é»˜è®¤åæ ‡")
            # ç”Ÿæˆé»˜è®¤åæ ‡(çº¿æ€§æ’åˆ—)
            coords_array = np.column_stack([np.arange(NUM_NODES), np.zeros(NUM_NODES)])
        
        # åœ°è´¨ç‰¹å¾èåˆ
        geo_features = None
        if use_geological and geo_file and coords_array is not None:
            st.subheader("ğŸŒ åœ°è´¨ç‰¹å¾èåˆ")
            try:
                geo_features, feature_names = load_geological_features(geo_file, coords_array)
                st.write(f"**åœ°è´¨ç‰¹å¾å½¢çŠ¶:** {geo_features.shape}")
                st.write(f"**ç‰¹å¾åç§°:** {feature_names}")
                
                # æ˜¾ç¤ºåœ°è´¨ç‰¹å¾ç»Ÿè®¡
                geo_df_display = pd.DataFrame(geo_features, columns=feature_names)
                st.write("**åœ°è´¨ç‰¹å¾ç»Ÿè®¡:**")
                st.dataframe(geo_df_display.describe())
                
                # å°†åœ°è´¨ç‰¹å¾æ·»åŠ åˆ°æ•°æ®ä¸­
                # å°†åœ°è´¨ç‰¹å¾æ‰©å±•åˆ°æ‰€æœ‰æ—¶é—´æ­¥
                geo_features_expanded = np.tile(geo_features[np.newaxis, :, :], (NUM_SAMPLES, 1, 1))
                # (num_samples, num_nodes, geo_features)
                
                # åˆå¹¶çŸ¿å‹æ•°æ®å’Œåœ°è´¨ç‰¹å¾
                data = np.concatenate([data, geo_features_expanded], axis=-1)
                NUM_FEATURES = data.shape[2]
                
                st.success(f"âœ… åœ°è´¨ç‰¹å¾å·²èåˆ! æ€»ç‰¹å¾æ•°: {NUM_FEATURES}")
                
            except Exception as e:
                st.error(f"åœ°è´¨ç‰¹å¾åŠ è½½å¤±è´¥: {e}")
                st.info("å°†ä»…ä½¿ç”¨çŸ¿å‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
        
        # ç”Ÿæˆæˆ–åŠ è½½é‚»æ¥çŸ©é˜µ
        st.header("2. å›¾ç»“æ„æ„å»º")
        if adj_method == "upload" and adj_file:
            # ä¸Šä¼ è‡ªå®šä¹‰é‚»æ¥çŸ©é˜µ
            if adj_file.name.endswith('.npy'):
                adj_mx = np.load(adj_file)
            else:  # CSV
                adj_mx = pd.read_csv(adj_file).values
            st.write(f"**é‚»æ¥çŸ©é˜µ (ä¸Šä¼ ) å½¢çŠ¶:** {adj_mx.shape}")
        else:
            # è‡ªåŠ¨ç”Ÿæˆé‚»æ¥çŸ©é˜µ
            if adj_method == "knn" and coords_array is not None:
                # ä½¿ç”¨çœŸå®åæ ‡ç”Ÿæˆ KNN
                adj_params['coords'] = coords_array
                st.info(f"âœ… ä½¿ç”¨çœŸå®æ”¯æ¶åæ ‡ç”Ÿæˆ K={adj_params.get('k', 3)} è¿‘é‚»å›¾")
            elif adj_method == "distance" and coords_array is not None:
                # ä½¿ç”¨è·ç¦»é˜ˆå€¼
                adj_params['coords'] = coords_array
                threshold = st.slider("è·ç¦»é˜ˆå€¼ (å•ä½:ç±³)", 1.0, 100.0, 10.0)
                adj_params['threshold'] = threshold
                st.info(f"âœ… ä½¿ç”¨çœŸå®åæ ‡ç”Ÿæˆè·ç¦»å›¾ (é˜ˆå€¼={threshold}ç±³)")
            elif adj_method in ["knn", "distance"] and coords_array is None:
                st.warning("âš ï¸ éœ€è¦åæ ‡æ–‡ä»¶æ‰èƒ½ä½¿ç”¨è·ç¦»ç›¸å…³æ–¹æ³•,å°†ä½¿ç”¨éšæœºåæ ‡")
                adj_params['coords'] = np.random.rand(NUM_NODES, 2)
            
            adj_mx = generate_adjacency_matrix(NUM_NODES, adj_method, **adj_params)
            st.write(f"**é‚»æ¥çŸ©é˜µ (è‡ªåŠ¨ç”Ÿæˆ):** {adj_mx.shape}")
            st.info(f"ä½¿ç”¨ **{adj_method}** æ–¹æ³•ç”Ÿæˆé‚»æ¥çŸ©é˜µ")
        
        # éªŒè¯é‚»æ¥çŸ©é˜µ
        if NUM_NODES != adj_mx.shape[0] or NUM_NODES != adj_mx.shape[1]:
            st.error(f"æ•°æ®ä¸é‚»æ¥çŸ©é˜µçš„èŠ‚ç‚¹æ•°ä¸åŒ¹é…! (æ•°æ®: {NUM_NODES}, é‚»æ¥çŸ©é˜µ: {adj_mx.shape[0]})")
        else:
            st.success("æ•°æ®æ–‡ä»¶å’Œé‚»æ¥çŸ©é˜µåŠ è½½/ç”ŸæˆæˆåŠŸ!")
            
            # æ˜¾ç¤ºé‚»æ¥çŸ©é˜µçš„è¿æ¥ç»Ÿè®¡
            num_edges = np.sum(adj_mx) / 2  # é™¤ä»¥2å› ä¸ºæ˜¯æ— å‘å›¾
            st.write(f"**å›¾ç»“æ„ç»Ÿè®¡:**")
            st.write(f"- æ€»è¾¹æ•°: {int(num_edges)}")
            st.write(f"- å¹³å‡åº¦æ•°: {np.sum(adj_mx, axis=1).mean():.2f}")
            
            # å¯è§†åŒ–é‚»æ¥çŸ©é˜µ
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("é‚»æ¥çŸ©é˜µå¯è§†åŒ–")
                st.image(adj_mx, use_container_width=True, clamp=True, caption="ç™½è‰²=è¿æ¥,é»‘è‰²=æ— è¿æ¥")
            
            with col2:
                st.subheader("èŠ‚ç‚¹ 0 çš„æ•°æ®é¢„è§ˆ")
                chart_data = pd.DataFrame(data[:min(500, len(data)), 0, 0], columns=['Node 0, Feature 0'])
                st.line_chart(chart_data)

            # --- è®­ç»ƒæ¨¡å— ---
            st.header("3. æ¨¡å‹è®­ç»ƒ")
            if st.button("å¼€å§‹è®­ç»ƒ"):
                
                with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®..."):
                    # 1. è®¡ç®— A_hat
                    A_hat = calculate_normalized_laplacian(adj_mx)
                    
                    # 2. ç”Ÿæˆ Dataloaders
                    train_loader, val_loader, test_loader, scaler = generate_dataloader(
                        data, BATCH_SIZE, SEQ_LEN, PRED_LEN
                    )
                    
                    # 3. åˆå§‹åŒ–æ¨¡å‹
                    device = torch.device(DEVICE)
                    model = STGCN(NUM_NODES, NUM_FEATURES, SEQ_LEN, PRED_LEN).to(device)
                    optimizer = optim.Adam(model.parameters(), lr=LR)
                    loss_fn = nn.MSELoss()
                    
                    st.success("æ¨¡å‹å’Œæ•°æ®åˆå§‹åŒ–å®Œæˆï¼å¼€å§‹è®­ç»ƒ...")
                
                # å‡†å¤‡å®æ—¶æ˜¾ç¤º
                status_placeholder = st.empty()
                loss_chart_placeholder = st.empty()
                loss_df = pd.DataFrame(columns=["Epoch", "Train Loss", "Val Loss"])

                start_time = time.time()
                
                for epoch in range(1, EPOCHS + 1):
                    # è®­ç»ƒ
                    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device, A_hat)
                    
                    # éªŒè¯
                    val_loss = evaluate(model, val_loader, loss_fn, device, A_hat)
                    
                    # æ›´æ–°çŠ¶æ€
                    elapsed = time.time() - start_time
                    status_text = f"""
                    **Epoch: {epoch}/{EPOCHS}**
                    - è®­ç»ƒæŸå¤± (Train Loss): {train_loss:.6f}
                    - éªŒè¯æŸå¤± (Val Loss): {val_loss:.6f}
                    - å·²ç”¨æ—¶é—´ (Elapsed): {elapsed:.2f}s
                    """
                    status_placeholder.markdown(status_text)
                    
                    # æ›´æ–°å›¾è¡¨
                    new_loss_row = pd.DataFrame({
                        "Epoch": [epoch],
                        "Train Loss": [train_loss],
                        "Val Loss": [val_loss]
                    })
                    loss_df = pd.concat([loss_df, new_loss_row], ignore_index=True)
                    loss_chart_placeholder.line_chart(loss_df.set_index("Epoch"))

                st.success("æ¨¡å‹è®­ç»ƒå®Œæˆ!")
                
                # --- ç»“æœå±•ç¤º ---
                st.header("4. è®­ç»ƒç»“æœ")
                st.subheader("æœ€ç»ˆæŸå¤±æ›²çº¿")
                st.line_chart(loss_df.set_index("Epoch"))
                
                st.subheader("æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç° (éšæœºæŠ½æ ·)")
                try:
                    # ä»æµ‹è¯•é›†è·å–ä¸€ä¸ªæ‰¹æ¬¡
                    x_test_batch, y_test_batch = next(iter(test_loader))
                    x_test_batch, y_test_batch = x_test_batch.to(device), y_test_batch.to(device)
                    A_hat_tensor = torch.tensor(A_hat).to(device)
                    
                    model.eval()
                    with torch.no_grad():
                        y_pred = model(x_test_batch, A_hat_tensor) # (B, T_out, N, F_out)
                        y_pred = y_pred.permute(0, 3, 2, 1) # (B, F_out, N, T_out)

                    # åå½’ä¸€åŒ–
                    y_pred_real = (y_pred.cpu().numpy() * scaler['std']) + scaler['mean']
                    y_test_real = (y_test_batch.cpu().numpy() * scaler['std']) + scaler['mean']
                    
                    # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬å’Œä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œæ¯”è¾ƒ (Batch 0, Node 0)
                    pred_series = y_pred_real[0, :, 0, 0] # é¢„æµ‹å€¼
                    true_series = y_test_real[0, :, 0, 0] # çœŸå®å€¼
                    
                    if PRED_LEN == 1:
                        st.write("é¢„æµ‹å€¼ (Test Sample 0, Node 0):", pred_series[0])
                        st.write("çœŸå®å€¼ (Test Sample 0, Node 0):", true_series[0])
                    else:
                        result_df = pd.DataFrame({
                            'Predicted': pred_series,
                            'True': true_series
                        }, index=[f't+{i+1}' for i in range(PRED_LEN)])
                        
                        st.dataframe(result_df)
                        st.line_chart(result_df)
                        
                except Exception as e:
                    st.error(f"åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ—¶å‡ºé”™: {e}")

    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        st.error("è¯·ç¡®ä¿æ‚¨ä¸Šä¼ äº†æ­£ç¡®æ ¼å¼çš„ CSV æ–‡ä»¶ã€‚")
        st.info("""
        **CSV æ–‡ä»¶æ ¼å¼æç¤º:**
        - æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ—¶é—´ç‚¹
        - æ¯åˆ—ä»£è¡¨ä¸€ä¸ªæ”¯æ¶(ç›‘æµ‹ç‚¹)
        - å¯ä»¥åŒ…å«æ—¶é—´åˆ—(ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶ç§»é™¤)
        """)
else:
    st.info("è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„ CSV çŸ¿å‹æ•°æ®æ–‡ä»¶ä»¥å¼€å§‹ã€‚")