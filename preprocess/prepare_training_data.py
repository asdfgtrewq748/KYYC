"""
çŸ¿å‹æ•°æ®é¢„å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼šåˆå¹¶åˆæ’‘åŠ›å’Œæœ«é˜»åŠ›æ•°æ®ï¼Œæå–å·¥ä½œå¾ªç¯ç‰¹å¾ï¼Œæ„å»ºSTGCNè®­ç»ƒæ•°æ®é›†
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime

# æ–‡ä»¶è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))  # KYYCé¡¹ç›®æ ¹ç›®å½•
DATA_DIR = os.path.join(PROJECT_ROOT, 'kaungya')
INIT_FILE = os.path.join(DATA_DIR, 'åˆæ’‘åŠ›æ•°æ®1-9 (2).csv')
FINAL_FILE = os.path.join(DATA_DIR, 'æœ«é˜»åŠ›æ•°æ®1-9 (2).csv')
GEO_FILE = os.path.join(PROJECT_ROOT, 'geology_features_extracted.csv')
COORD_FILE = os.path.join(PROJECT_ROOT, 'zuobiao.csv')

OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'processed_data')
os.makedirs(OUTPUT_DIR, exist_ok=True)


def load_and_clean_data(init_file, final_file):
    """
    åŠ è½½å¹¶æ¸…æ´—åˆæ’‘åŠ›å’Œæœ«é˜»åŠ›æ•°æ®
    """
    print("=" * 60)
    print("æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®")
    print("=" * 60)
    
    # è¯»å–æ•°æ®ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜ï¼‰
    init_df = pd.read_csv(init_file, skiprows=1, encoding='utf-8-sig')
    final_df = pd.read_csv(final_file, skiprows=1, encoding='utf-8-sig')
    
    print(f"åˆæ’‘åŠ›æ•°æ®: {init_df.shape}")
    print(f"æœ«é˜»åŠ›æ•°æ®: {final_df.shape}")
    
    # æ¸…ç†åˆ—å
    init_df.columns = init_df.columns.str.strip()
    final_df.columns = final_df.columns.str.strip()
    
    # è½¬æ¢æ—¶é—´åˆ—
    time_cols = ['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´', 'å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´', 'åˆæ’‘åŠ›æ—¶é—´']
    for col in time_cols:
        if col in init_df.columns:
            init_df[col] = pd.to_datetime(init_df[col], errors='coerce')
    
    time_cols_final = ['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´', 'å·¥ä½œç»“æŸæ—¶é—´', 'æœ«é˜»åŠ›æ—¶é—´']
    for col in time_cols_final:
        if col in final_df.columns:
            final_df[col] = pd.to_datetime(final_df[col], errors='coerce')
    
    # é‡å‘½åä»¥ä¾¿åˆå¹¶
    final_df = final_df.rename(columns={'å·¥ä½œç»“æŸæ—¶é—´': 'å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'})
    
    # åˆ é™¤æ— æ•ˆæ•°æ®
    init_df = init_df.dropna(subset=['æ”¯æ¶å·', 'åˆæ’‘åŠ›å€¼'])
    final_df = final_df.dropna(subset=['æ”¯æ¶å·', 'æœ«é˜»åŠ›å€¼'])
    
    print(f"æ¸…æ´—ååˆæ’‘åŠ›æ•°æ®: {init_df.shape}")
    print(f"æ¸…æ´—åæœ«é˜»åŠ›æ•°æ®: {final_df.shape}")
    
    return init_df, final_df


def merge_and_extract_features(init_df, final_df):
    """
    åˆå¹¶åˆæ’‘åŠ›å’Œæœ«é˜»åŠ›æ•°æ®å¹¶æå–ç‰¹å¾
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤2: åˆå¹¶æ•°æ®å¹¶æå–ç‰¹å¾")
    print("=" * 60)
    
    # åˆå¹¶æ•°æ®é›†
    merged = pd.merge(
        init_df,
        final_df,
        on=['å·¥ä½œé¢åç§°', 'æ”¯æ¶å·', 'æŸ±ç±»å‹', 'å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´', 'å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'],
        how='inner',
        suffixes=('_init', '_final')
    )
    
    print(f"åˆå¹¶åæ•°æ®é‡: {merged.shape[0]} æ¡è®°å½•")
    print(f"æ¶‰åŠæ”¯æ¶æ•°: {merged['æ”¯æ¶å·'].nunique()} ä¸ª")
    
    # ========== ç‰¹å¾å·¥ç¨‹ ==========
    
    # 1. åŸºç¡€å‹åŠ›ç‰¹å¾
    merged['å‹åŠ›å¢é‡'] = merged['æœ«é˜»åŠ›å€¼'] - merged['åˆæ’‘åŠ›å€¼']
    merged['å‹åŠ›å¢é•¿ç‡'] = merged['å‹åŠ›å¢é‡'] / (merged['åˆæ’‘åŠ›å€¼'] + 1e-6)
    merged['å‹åŠ›å¹³å‡å€¼'] = (merged['åˆæ’‘åŠ›å€¼'] + merged['æœ«é˜»åŠ›å€¼']) / 2
    
    # 2. æ—¶é—´ç‰¹å¾
    merged['å¾ªç¯æ—¶é•¿_ç§’'] = (merged['å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'] - merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´']).dt.total_seconds()
    merged['å‹åŠ›å˜åŒ–é€Ÿç‡'] = merged['å‹åŠ›å¢é‡'] / (merged['å¾ªç¯æ—¶é•¿_ç§’'] + 1)
    
    # åˆæ’‘å“åº”æ—¶é—´
    merged['åˆæ’‘å“åº”æ—¶é—´_ç§’'] = (merged['åˆæ’‘åŠ›æ—¶é—´'] - merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´']).dt.total_seconds()
    
    # 3. æ—¶é—´æˆ³ç‰¹å¾ï¼ˆå‘¨æœŸæ€§ï¼‰
    merged['å°æ—¶'] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.hour
    merged['æ˜ŸæœŸå‡ '] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.dayofweek
    merged['æœˆä»½'] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.month
    merged['æ—¥æœŸ'] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.date
    
    # 4. åˆ é™¤å¼‚å¸¸å€¼
    # æ’é™¤å¾ªç¯æ—¶é•¿å¼‚å¸¸çš„æ•°æ®ï¼ˆ<1åˆ†é’Ÿæˆ–>6å°æ—¶ï¼‰
    merged = merged[(merged['å¾ªç¯æ—¶é•¿_ç§’'] >= 60) & (merged['å¾ªç¯æ—¶é•¿_ç§’'] <= 21600)]
    
    # æ’é™¤å‹åŠ›å€¼å¼‚å¸¸çš„æ•°æ®ï¼ˆè´Ÿå€¼æˆ–è¿‡å¤§å€¼ï¼‰
    merged = merged[(merged['åˆæ’‘åŠ›å€¼'] >= 0) & (merged['åˆæ’‘åŠ›å€¼'] <= 200)]
    merged = merged[(merged['æœ«é˜»åŠ›å€¼'] >= 0) & (merged['æœ«é˜»åŠ›å€¼'] <= 200)]
    
    print(f"å¼‚å¸¸å€¼è¿‡æ»¤åæ•°æ®é‡: {merged.shape[0]} æ¡è®°å½•")
    
    # æŒ‰æ”¯æ¶å·å’Œæ—¶é—´æ’åº
    merged = merged.sort_values(['æ”¯æ¶å·', 'å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´']).reset_index(drop=True)
    
    # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
    print("\nç‰¹å¾ç»Ÿè®¡:")
    feature_cols = ['åˆæ’‘åŠ›å€¼', 'æœ«é˜»åŠ›å€¼', 'å‹åŠ›å¢é‡', 'å‹åŠ›å¢é•¿ç‡', 'å¾ªç¯æ—¶é•¿_ç§’', 'å‹åŠ›å˜åŒ–é€Ÿç‡']
    print(merged[feature_cols].describe())
    
    return merged


def add_geological_features(merged_df, geo_file, coord_file):
    """
    ä¸ºæ¯ä¸ªæ”¯æ¶æ·»åŠ åœ°è´¨ç‰¹å¾
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤3: èåˆåœ°è´¨ç‰¹å¾")
    print("=" * 60)
    
    if not os.path.exists(geo_file):
        print(f"âš ï¸ åœ°è´¨ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {geo_file}")
        print("   è·³è¿‡åœ°è´¨ç‰¹å¾èåˆ")
        return merged_df, []
    
    # è¯»å–åœ°è´¨ç‰¹å¾
    geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
    print(f"åœ°è´¨ç‰¹å¾æ•°æ®: {geo_df.shape}")
    
    # è¯»å–åæ ‡æ˜ å°„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    # è¿™é‡Œå‡è®¾æ”¯æ¶å·å¯ä»¥æ˜ å°„åˆ°æŸä¸ªé’»å­”
    # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®æ”¯æ¶åæ ‡æ‰¾æœ€è¿‘çš„é’»å­”
    
    # ç®€åŒ–å¤„ç†ï¼šä½¿ç”¨å¹³å‡åœ°è´¨ç‰¹å¾ï¼ˆå®é™…åº”æ ¹æ®æ”¯æ¶ä½ç½®æ˜ å°„ï¼‰
    geo_features_cols = [col for col in geo_df.columns if col not in ['borehole', 'x', 'y']]
    geo_mean = geo_df[geo_features_cols].mean()
    
    # ä¸ºæ¯æ¡è®°å½•æ·»åŠ åœ°è´¨ç‰¹å¾
    for col in geo_features_cols:
        merged_df[f'geo_{col}'] = geo_mean[col]
    
    print(f"âœ“ æ·»åŠ äº† {len(geo_features_cols)} ä¸ªåœ°è´¨ç‰¹å¾")
    print(f"  ç‰¹å¾åˆ—è¡¨: {geo_features_cols[:5]}...")
    
    return merged_df, geo_features_cols


def create_support_coordinates(merged_df):
    """
    åˆ›å»ºæ”¯æ¶åæ ‡ï¼ˆç®€åŒ–ç‰ˆï¼šå‡è®¾æ”¯æ¶çº¿æ€§æ’åˆ—ï¼‰
    å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨çœŸå®çš„æ”¯æ¶åæ ‡æ–‡ä»¶
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤4: åˆ›å»ºæ”¯æ¶åæ ‡")
    print("=" * 60)
    
    support_ids = sorted(merged_df['æ”¯æ¶å·'].unique())
    num_supports = len(support_ids)
    
    # ç®€åŒ–åæ ‡ï¼šå‡è®¾æ”¯æ¶æ²¿å·¥ä½œé¢çº¿æ€§æ’åˆ—
    coords = np.zeros((num_supports, 2))
    coords[:, 0] = np.arange(num_supports) * 1.5  # æ”¯æ¶é—´è·1.5ç±³
    coords[:, 1] = 0  # å‡è®¾åœ¨åŒä¸€æ¡çº¿ä¸Š
    
    coord_df = pd.DataFrame({
        'support_id': support_ids,
        'x': coords[:, 0],
        'y': coords[:, 1]
    })
    
    print(f"âœ“ åˆ›å»ºäº† {num_supports} ä¸ªæ”¯æ¶çš„åæ ‡")
    print(f"  æ”¯æ¶ç¼–å·èŒƒå›´: {support_ids[0]} - {support_ids[-1]}")
    
    return coord_df


def create_sequence_dataset(merged_df, seq_len=5, pred_len=1):
    """
    åˆ›å»ºåºåˆ—æ•°æ®é›†ç”¨äºSTGCNè®­ç»ƒ
    
    å‚æ•°:
        seq_len: å†å²åºåˆ—é•¿åº¦ï¼ˆä½¿ç”¨è¿‡å»å¤šå°‘ä¸ªå·¥ä½œå¾ªç¯ï¼‰
        pred_len: é¢„æµ‹é•¿åº¦ï¼ˆé¢„æµ‹æœªæ¥å¤šå°‘ä¸ªå·¥ä½œå¾ªç¯ï¼‰
    
    è¿”å›:
        sequences: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤5: æ„å»ºåºåˆ—æ•°æ®é›†")
    print("=" * 60)
    print(f"é…ç½®: å†å²é•¿åº¦={seq_len}, é¢„æµ‹é•¿åº¦={pred_len}")
    
    # ç¡®å®šç‰¹å¾åˆ—
    pressure_features = ['åˆæ’‘åŠ›å€¼', 'æœ«é˜»åŠ›å€¼', 'å‹åŠ›å¢é‡', 'å‹åŠ›å¢é•¿ç‡', 'å¾ªç¯æ—¶é•¿_ç§’', 'å‹åŠ›å˜åŒ–é€Ÿç‡']
    geo_features = [col for col in merged_df.columns if col.startswith('geo_')]
    time_features = ['å°æ—¶', 'æ˜ŸæœŸå‡ ']
    
    all_features = pressure_features + geo_features + time_features
    print(f"ç‰¹å¾ç»´åº¦: {len(all_features)} (å‹åŠ›={len(pressure_features)}, åœ°è´¨={len(geo_features)}, æ—¶é—´={len(time_features)})")
    
    sequences = []
    support_ids = sorted(merged_df['æ”¯æ¶å·'].unique())
    
    for support_id in support_ids:
        support_data = merged_df[merged_df['æ”¯æ¶å·'] == support_id].copy()
        
        # è‡³å°‘éœ€è¦ seq_len + pred_len æ¡è®°å½•
        if len(support_data) < seq_len + pred_len:
            continue
        
        for i in range(len(support_data) - seq_len - pred_len + 1):
            # å†å²åºåˆ—
            hist_seq = support_data.iloc[i:i+seq_len]
            # é¢„æµ‹ç›®æ ‡
            target_seq = support_data.iloc[i+seq_len:i+seq_len+pred_len]
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            X = hist_seq[all_features].values  # (seq_len, num_features)
            y_init = target_seq['åˆæ’‘åŠ›å€¼'].values  # (pred_len,)
            y_final = target_seq['æœ«é˜»åŠ›å€¼'].values  # (pred_len,)
            
            sequences.append({
                'support_id': support_id,
                'start_time': hist_seq.iloc[0]['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'],
                'end_time': target_seq.iloc[-1]['å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'],
                'X': X,
                'y_init': y_init,
                'y_final': y_final,
                'feature_names': all_features
            })
    
    print(f"âœ“ ç”Ÿæˆäº† {len(sequences)} ä¸ªè®­ç»ƒæ ·æœ¬")
    print(f"  æ¶‰åŠæ”¯æ¶æ•°: {len(support_ids)}")
    print(f"  æ ·æœ¬å½¢çŠ¶: X={sequences[0]['X'].shape}, y_final={sequences[0]['y_final'].shape}")
    
    return sequences, support_ids, all_features


def save_processed_data(merged_df, sequences, coord_df, feature_names, output_dir):
    """
    ä¿å­˜å¤„ç†åçš„æ•°æ®
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤6: ä¿å­˜å¤„ç†åçš„æ•°æ®")
    print("=" * 60)
    
    # 1. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ•°æ®
    merged_file = os.path.join(output_dir, 'merged_pressure_data.csv')
    merged_df.to_csv(merged_file, index=False, encoding='utf-8-sig')
    print(f"âœ“ ä¿å­˜åˆå¹¶æ•°æ®: {merged_file}")
    
    # 2. ä¿å­˜æ”¯æ¶åæ ‡
    coord_file = os.path.join(output_dir, 'support_coordinates.csv')
    coord_df.to_csv(coord_file, index=False, encoding='utf-8-sig')
    print(f"âœ“ ä¿å­˜æ”¯æ¶åæ ‡: {coord_file}")
    
    # 3. ä¿å­˜åºåˆ—æ•°æ®ï¼ˆNumPyæ ¼å¼ï¼Œä¾¿äºå¿«é€ŸåŠ è½½ï¼‰
    X_list = [seq['X'] for seq in sequences]
    y_init_list = [seq['y_init'] for seq in sequences]
    y_final_list = [seq['y_final'] for seq in sequences]
    support_ids = [seq['support_id'] for seq in sequences]
    
    np.savez_compressed(
        os.path.join(output_dir, 'sequence_dataset.npz'),
        X=np.array(X_list),
        y_init=np.array(y_init_list),
        y_final=np.array(y_final_list),
        support_ids=np.array(support_ids),
        feature_names=feature_names
    )
    print(f"âœ“ ä¿å­˜åºåˆ—æ•°æ®: sequence_dataset.npz")
    
    # 4. ä¿å­˜æ•°æ®æ‘˜è¦
    summary = {
        'num_samples': len(sequences),
        'num_supports': coord_df.shape[0],
        'num_features': len(feature_names),
        'seq_len': sequences[0]['X'].shape[0] if sequences else 0,
        'pred_len': sequences[0]['y_final'].shape[0] if sequences else 0,
        'date_range': f"{merged_df['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].min()} to {merged_df['å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'].max()}",
        'feature_names': feature_names
    }
    
    import json
    with open(os.path.join(output_dir, 'dataset_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
    print(f"âœ“ ä¿å­˜æ•°æ®æ‘˜è¦: dataset_summary.json")
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ–‡ä»¶åˆ—è¡¨:")
    for f in os.listdir(output_dir):
        fpath = os.path.join(output_dir, f)
        size = os.path.getsize(fpath) / 1024 / 1024
        print(f"  - {f} ({size:.2f} MB)")


def main():
    """
    ä¸»æµç¨‹
    """
    print("=" * 60)
    print("çŸ¿å‹æ•°æ®é¢„å¤„ç†æµç¨‹")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(INIT_FILE):
        print(f"âŒ åˆæ’‘åŠ›æ–‡ä»¶ä¸å­˜åœ¨: {INIT_FILE}")
        return
    if not os.path.exists(FINAL_FILE):
        print(f"âŒ æœ«é˜»åŠ›æ–‡ä»¶ä¸å­˜åœ¨: {FINAL_FILE}")
        return
    
    # æ­¥éª¤1: åŠ è½½æ•°æ®
    init_df, final_df = load_and_clean_data(INIT_FILE, FINAL_FILE)
    
    # æ­¥éª¤2: åˆå¹¶å’Œç‰¹å¾å·¥ç¨‹
    merged_df = merge_and_extract_features(init_df, final_df)
    
    # æ­¥éª¤3: æ·»åŠ åœ°è´¨ç‰¹å¾
    merged_df, geo_features = add_geological_features(merged_df, GEO_FILE, COORD_FILE)
    
    # æ­¥éª¤4: åˆ›å»ºæ”¯æ¶åæ ‡
    coord_df = create_support_coordinates(merged_df)
    
    # æ­¥éª¤5: æ„å»ºåºåˆ—æ•°æ®é›†
    sequences, support_ids, feature_names = create_sequence_dataset(
        merged_df, 
        seq_len=5,  # ä½¿ç”¨è¿‡å»5ä¸ªå·¥ä½œå¾ªç¯
        pred_len=1   # é¢„æµ‹ä¸‹1ä¸ªå·¥ä½œå¾ªç¯
    )
    
    # æ­¥éª¤6: ä¿å­˜ç»“æœ
    save_processed_data(merged_df, sequences, coord_df, feature_names, OUTPUT_DIR)
    
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºSTGCNè®­ç»ƒ")


if __name__ == '__main__':
    main()
