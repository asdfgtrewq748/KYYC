# -*- coding: utf-8 -*-
"""
çŸ¿å‹æ•°æ®é¢„å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼šåˆå¹¶åˆæ’‘åŠ›å’Œæœ«é˜»åŠ›æ•°æ®ï¼Œæå–å·¥ä½œå¾ªç¯ç‰¹å¾ï¼Œæ„å»ºSTGCNè®­ç»ƒæ•°æ®é›†
"""

import os
import sys
# è®¾ç½®è¾“å‡ºç¼–ç ä¸ºUTF-8
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import os
import pandas as pd
import numpy as np
from datetime import datetime

# æ–‡ä»¶è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))  # KYYCé¡¹ç›®æ ¹ç›®å½•
DATA_DIR = os.path.join(PROJECT_ROOT, 'kaungya')
INIT_FILE = os.path.join(DATA_DIR, 'åˆæ’‘åŠ›æ•°æ®1-9 (2).csv')
FINAL_FILE = os.path.join(DATA_DIR, 'æœ«é˜»åŠ›æ•°æ®1-9 (2).csv')
GEO_FILE = os.path.join(PROJECT_ROOT, 'geology_features_extracted.csv')
COORD_FILE = os.path.join(PROJECT_ROOT, 'zuobiao.csv')

OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'processed_data')
os.makedirs(OUTPUT_DIR, exist_ok=True)


def load_and_clean_data(init_file, final_file):
    """
    åŠ è½½å¹¶æ¸…æ´—åˆæ’‘åŠ›å’Œæœ«é˜»åŠ›æ•°æ®
    """
    print("=" * 60)
    print("æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®")
    print("=" * 60)
    
    # è¯»å–æ•°æ®ï¼ˆè·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜ï¼‰
    init_df = pd.read_csv(init_file, skiprows=1, encoding='utf-8-sig')
    final_df = pd.read_csv(final_file, skiprows=1, encoding='utf-8-sig')
    
    print(f"åˆæ’‘åŠ›æ•°æ®: {init_df.shape}")
    print(f"æœ«é˜»åŠ›æ•°æ®: {final_df.shape}")
    
    # æ¸…ç†åˆ—å
    init_df.columns = init_df.columns.str.strip()
    final_df.columns = final_df.columns.str.strip()
    
    # è½¬æ¢æ—¶é—´åˆ—
    time_cols = ['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´', 'å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´', 'åˆæ’‘åŠ›æ—¶é—´']
    for col in time_cols:
        if col in init_df.columns:
            init_df[col] = pd.to_datetime(init_df[col], errors='coerce')
    
    time_cols_final = ['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´', 'å·¥ä½œç»“æŸæ—¶é—´', 'æœ«é˜»åŠ›æ—¶é—´']
    for col in time_cols_final:
        if col in final_df.columns:
            final_df[col] = pd.to_datetime(final_df[col], errors='coerce')
    
    # é‡å‘½åä»¥ä¾¿åˆå¹¶
    final_df = final_df.rename(columns={'å·¥ä½œç»“æŸæ—¶é—´': 'å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'})
    
    # åˆ é™¤æ— æ•ˆæ•°æ®
    init_df = init_df.dropna(subset=['æ”¯æ¶å·', 'åˆæ’‘åŠ›å€¼'])
    final_df = final_df.dropna(subset=['æ”¯æ¶å·', 'æœ«é˜»åŠ›å€¼'])
    
    print(f"æ¸…æ´—ååˆæ’‘åŠ›æ•°æ®: {init_df.shape}")
    print(f"æ¸…æ´—åæœ«é˜»åŠ›æ•°æ®: {final_df.shape}")
    
    return init_df, final_df


def merge_and_extract_features(init_df, final_df):
    """
    åˆå¹¶åˆæ’‘åŠ›å’Œæœ«é˜»åŠ›æ•°æ®å¹¶æå–ç‰¹å¾
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤2: åˆå¹¶æ•°æ®å¹¶æå–ç‰¹å¾")
    print("=" * 60)
    
    # åˆå¹¶æ•°æ®é›†
    merged = pd.merge(
        init_df,
        final_df,
        on=['å·¥ä½œé¢åç§°', 'æ”¯æ¶å·', 'æŸ±ç±»å‹', 'å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´', 'å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'],
        how='inner',
        suffixes=('_init', '_final')
    )
    
    print(f"åˆå¹¶åæ•°æ®é‡: {merged.shape[0]} æ¡è®°å½•")
    print(f"æ¶‰åŠæ”¯æ¶æ•°: {merged['æ”¯æ¶å·'].nunique()} ä¸ª")
    
    # ========== ç‰¹å¾å·¥ç¨‹ ==========
    
    # 1. åŸºç¡€å‹åŠ›ç‰¹å¾
    merged['å‹åŠ›å¢é‡'] = merged['æœ«é˜»åŠ›å€¼'] - merged['åˆæ’‘åŠ›å€¼']
    merged['å‹åŠ›å¢é•¿ç‡'] = merged['å‹åŠ›å¢é‡'] / (merged['åˆæ’‘åŠ›å€¼'] + 1e-6)
    merged['å‹åŠ›å¹³å‡å€¼'] = (merged['åˆæ’‘åŠ›å€¼'] + merged['æœ«é˜»åŠ›å€¼']) / 2
    
    # 2. æ—¶é—´ç‰¹å¾
    merged['å¾ªç¯æ—¶é•¿_ç§’'] = (merged['å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'] - merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´']).dt.total_seconds()
    merged['å‹åŠ›å˜åŒ–é€Ÿç‡'] = merged['å‹åŠ›å¢é‡'] / (merged['å¾ªç¯æ—¶é•¿_ç§’'] + 1)
    
    # åˆæ’‘å“åº”æ—¶é—´
    merged['åˆæ’‘å“åº”æ—¶é—´_ç§’'] = (merged['åˆæ’‘åŠ›æ—¶é—´'] - merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´']).dt.total_seconds()
    
    # 3. æ—¶é—´æˆ³ç‰¹å¾ï¼ˆå‘¨æœŸæ€§ï¼‰
    merged['å°æ—¶'] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.hour
    merged['æ˜ŸæœŸå‡ '] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.dayofweek
    merged['æœˆä»½'] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.month
    merged['æ—¥æœŸ'] = merged['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].dt.date
    
    # 4. åˆ é™¤å¼‚å¸¸å€¼
    # æ’é™¤å¾ªç¯æ—¶é•¿å¼‚å¸¸çš„æ•°æ®ï¼ˆ<1åˆ†é’Ÿæˆ–>6å°æ—¶ï¼‰
    merged = merged[(merged['å¾ªç¯æ—¶é•¿_ç§’'] >= 60) & (merged['å¾ªç¯æ—¶é•¿_ç§’'] <= 21600)]
    
    # æ’é™¤å‹åŠ›å€¼å¼‚å¸¸çš„æ•°æ®ï¼ˆè´Ÿå€¼æˆ–è¿‡å¤§å€¼ï¼‰
    merged = merged[(merged['åˆæ’‘åŠ›å€¼'] >= 0) & (merged['åˆæ’‘åŠ›å€¼'] <= 200)]
    merged = merged[(merged['æœ«é˜»åŠ›å€¼'] >= 0) & (merged['æœ«é˜»åŠ›å€¼'] <= 200)]
    
    print(f"å¼‚å¸¸å€¼è¿‡æ»¤åæ•°æ®é‡: {merged.shape[0]} æ¡è®°å½•")
    
    # æŒ‰æ”¯æ¶å·å’Œæ—¶é—´æ’åº
    merged = merged.sort_values(['æ”¯æ¶å·', 'å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´']).reset_index(drop=True)
    
    # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
    print("\nç‰¹å¾ç»Ÿè®¡:")
    feature_cols = ['åˆæ’‘åŠ›å€¼', 'æœ«é˜»åŠ›å€¼', 'å‹åŠ›å¢é‡', 'å‹åŠ›å¢é•¿ç‡', 'å¾ªç¯æ—¶é•¿_ç§’', 'å‹åŠ›å˜åŒ–é€Ÿç‡']
    print(merged[feature_cols].describe())
    
    return merged


def add_geological_features(merged_df, geo_file, coord_file):
    """
    ä¸ºæ¯ä¸ªæ”¯æ¶æ·»åŠ å…¶æœ€è¿‘é’»å­”çš„åœ°è´¨ç‰¹å¾ï¼ˆå·²ä¿®å¤ï¼‰
    ä½¿ç”¨KDTreeç²¾ç¡®åŒ¹é…ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¹³å‡å€¼
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤4: èåˆåœ°è´¨ç‰¹å¾ (å·²ä¿®å¤ - ä½¿ç”¨æœ€è¿‘é‚»åŒ¹é…)")
    print("=" * 60)
    
    if not os.path.exists(geo_file):
        print(f"âš ï¸ åœ°è´¨ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {geo_file}")
        print("   è·³è¿‡åœ°è´¨ç‰¹å¾èåˆ")
        return merged_df, []
    
    try:
        from scipy.spatial import KDTree
        
        # 1. åŠ è½½åœ°è´¨ç‰¹å¾ï¼ˆé’»å­”çš„ï¼‰
        geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        geo_coords = geo_df[['x', 'y']].values
        geo_features_cols = [col for col in geo_df.columns if col not in ['borehole', 'x', 'y']]
        geo_features_data = geo_df[geo_features_cols]
        print(f"åœ°è´¨ç‰¹å¾æ•°æ®: {geo_df.shape}")
        print(f"é’»å­”æ•°é‡: {len(geo_df)}")
        
        # 2. åŠ è½½æ”¯æ¶åæ ‡
        if not os.path.exists(coord_file):
            print(f"âš ï¸ æ”¯æ¶åæ ‡æ–‡ä»¶ä¸å­˜åœ¨: {coord_file}")
            print("   æ­£åœ¨ä½¿ç”¨ processed_data/support_coordinates.csv")
            coord_file = os.path.join(OUTPUT_DIR, 'support_coordinates.csv')
            
            if not os.path.exists(coord_file):
                print("   âš ï¸ ä¸´æ—¶åæ ‡æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨ï¼Œå°†å…ˆç”Ÿæˆ")
                # å…ˆåˆ›å»ºä¸´æ—¶åæ ‡
                support_ids = sorted(merged_df['æ”¯æ¶å·'].unique())
                num_supports = len(support_ids)
                coords = np.zeros((num_supports, 2))
                coords[:, 0] = np.arange(num_supports) * 1.5  # æ”¯æ¶é—´è·1.5ç±³
                coords[:, 1] = 0
                coord_df_temp = pd.DataFrame({
                    'support_id': support_ids,
                    'x': coords[:, 0],
                    'y': coords[:, 1]
                })
                os.makedirs(OUTPUT_DIR, exist_ok=True)
                coord_df_temp.to_csv(coord_file, index=False, encoding='utf-8-sig')
                print(f"   âœ“ å·²åˆ›å»ºä¸´æ—¶æ”¯æ¶åæ ‡: {coord_file}")
        
        coord_df = pd.read_csv(coord_file, encoding='utf-8-sig')
        support_coords = coord_df[['x', 'y']].values
        support_ids = coord_df['support_id'].values
        print(f"æ”¯æ¶åæ ‡æ•°æ®: {coord_df.shape}")
        
        # 3. ä½¿ç”¨KDTreeæŸ¥æ‰¾æœ€è¿‘çš„é’»å­”
        #    ä¸ºæ¯ä¸ªæ”¯æ¶(support_coords) æ‰¾åˆ° geo_coords ä¸­æœ€è¿‘çš„ç´¢å¼•
        tree = KDTree(geo_coords)
        distances, indices = tree.query(support_coords)
        
        print(f"âœ“ å·²ä¸ºæ¯ä¸ªæ”¯æ¶åŒ¹é…æœ€è¿‘çš„é’»å­”")
        print(f"  å¹³å‡è·ç¦»: {np.mean(distances):.2f} ç±³")
        print(f"  æœ€å¤§è·ç¦»: {np.max(distances):.2f} ç±³")
        
        # 4. åˆ›å»º æ”¯æ¶ID -> åœ°è´¨ç‰¹å¾ çš„æ˜ å°„
        #    indices æ˜¯ geo_df çš„è¡Œç´¢å¼•
        nearest_geo_features = geo_features_data.iloc[indices]
        
        # å°†æ”¯æ¶IDä¸ç‰¹å¾å…³è”
        support_geo_map = {}
        for i, support_id in enumerate(support_ids):
            support_geo_map[support_id] = nearest_geo_features.iloc[i].to_dict()
        
        print(f"âœ“ å·²ä¸º {len(support_ids)} ä¸ªæ”¯æ¶åˆ›å»ºåœ°è´¨ç‰¹å¾æ˜ å°„")
        
        # 5. å°†ç‰¹å¾æ˜ å°„åˆå¹¶å›ä¸»DataFrame
        #    ä½¿ç”¨æ”¯æ¶å·ï¼ˆå‡è®¾ merged_df ä¸­æœ‰ 'æ”¯æ¶å·' åˆ—ï¼‰
        
        # ç¡®ä¿ 'æ”¯æ¶å·' ä¸ºå¯æ˜ å°„çš„ç±»å‹
        merged_df['æ”¯æ¶å·'] = merged_df['æ”¯æ¶å·'].astype(type(support_ids[0]))
        
        # ä¸ºæ¯ä¸ªåœ°è´¨ç‰¹å¾åˆ—åˆ›å»ºæ˜ å°„
        for geo_col in geo_features_cols:
            col_map = {sid: support_geo_map[sid][geo_col] for sid in support_ids}
            merged_df[f'geo_{geo_col}'] = merged_df['æ”¯æ¶å·'].map(col_map)
        
        print(f"âœ“ æ·»åŠ äº† {len(geo_features_cols)} ä¸ªåœ°è´¨ç‰¹å¾åˆ—")
        
        # éªŒè¯åœ°è´¨ç‰¹å¾çš„å˜åŒ–æ€§
        geo_col_names = [f'geo_{col}' for col in geo_features_cols]
        print(f"\nåœ°è´¨ç‰¹å¾ç»Ÿè®¡ï¼ˆéªŒè¯æ˜¯å¦æœ‰çœŸå®å˜åŒ–ï¼‰:")
        for geo_col in geo_col_names[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªç‰¹å¾
            unique_vals = merged_df[geo_col].nunique()
            print(f"  {geo_col}: {unique_vals} ä¸ªå”¯ä¸€å€¼")
        
        return merged_df, geo_features_cols
        
    except ImportError:
        print("âš ï¸ ç¼ºå°‘ scipy åº“ï¼Œæ— æ³•ä½¿ç”¨KDTree")
        print("   è¯·è¿è¡Œ: pip install scipy")
        print("   æš‚æ—¶è·³è¿‡åœ°è´¨ç‰¹å¾èåˆ")
        return merged_df, []
    except Exception as e:
        print(f"âš ï¸ åœ°è´¨ç‰¹å¾èåˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("   è·³è¿‡åœ°è´¨ç‰¹å¾èåˆ")
        return merged_df, []


def create_support_coordinates(merged_df, geo_file=None):
    """
    åˆ›å»ºæ”¯æ¶åæ ‡ï¼ˆä¸é’»å­”åæ ‡ç³»ç»Ÿå¯¹é½ï¼‰
    ç­–ç•¥ï¼šè®©æ”¯æ¶åœ¨é’»å­”è¦†ç›–åŒºåŸŸå†…å‡åŒ€åˆ†å¸ƒï¼Œä»¥ä¾¿åŒ¹é…åˆ°ä¸åŒé’»å­”
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤3: åˆ›å»ºæ”¯æ¶åæ ‡ï¼ˆä¸é’»å­”åæ ‡ç³»å¯¹é½ï¼‰")
    print("=" * 60)
    
    support_ids = sorted(merged_df['æ”¯æ¶å·'].unique())
    num_supports = len(support_ids)
    
    # å¦‚æœæœ‰åœ°è´¨æ–‡ä»¶ï¼ŒåŸºäºé’»å­”åæ ‡èŒƒå›´ç”Ÿæˆæ”¯æ¶åæ ‡
    if geo_file and os.path.exists(geo_file):
        geo_df = pd.read_csv(geo_file, encoding='utf-8-sig')
        x_min, x_max = geo_df['x'].min(), geo_df['x'].max()
        y_min, y_max = geo_df['y'].min(), geo_df['y'].max()
        
        # è®¡ç®—é’»å­”åŒºåŸŸçš„è·¨åº¦
        x_span = x_max - x_min  # Xæ–¹å‘è·¨åº¦ï¼ˆçº¦2000ç±³ï¼‰
        y_span = y_max - y_min  # Yæ–¹å‘è·¨åº¦ï¼ˆçº¦650ç±³ï¼‰
        
        # ç­–ç•¥ï¼šæ”¯æ¶åœ¨X-Yå¹³é¢ä¸Šå‘ˆç½‘æ ¼çŠ¶æˆ–Så‹åˆ†å¸ƒ
        # è¿™æ ·å¯ä»¥è®©ä¸åŒæ”¯æ¶åŒ¹é…åˆ°ä¸åŒé’»å­”
        
        # æ–¹æ³•1ï¼šæ²¿å¯¹è§’çº¿åˆ†å¸ƒï¼ˆä»è¥¿å—åˆ°ä¸œåŒ—ï¼‰
        coords = np.zeros((num_supports, 2))
        
        # è®©æ”¯æ¶æ²¿ç€é’»å­”åŒºåŸŸçš„å¯¹è§’çº¿åˆ†å¸ƒï¼Œå¹¶æ·»åŠ å°çš„Yæ–¹å‘åç§»
        for i in range(num_supports):
            ratio = i / max(num_supports - 1, 1)  # 0 åˆ° 1
            coords[i, 0] = x_min + ratio * x_span  # Xä»è¥¿åˆ°ä¸œ
            coords[i, 1] = y_min + ratio * y_span  # Yä»å—åˆ°åŒ—
            
            # æ·»åŠ å°çš„Yæ–¹å‘æ‘†åŠ¨ï¼ˆæ¨¡æ‹Ÿå·¥ä½œé¢æ¨è¿›ï¼‰
            # æ”¯æ¶é—´è·çº¦1.5ç±³ï¼Œæ·»åŠ å‘¨æœŸæ€§æ‘†åŠ¨
            y_wiggle = 50 * np.sin(i * 0.3)  # Â±50ç±³çš„æ­£å¼¦æ‘†åŠ¨
            coords[i, 1] += y_wiggle
        
        print(f"âœ“ åŸºäºé’»å­”åæ ‡èŒƒå›´ç”Ÿæˆæ”¯æ¶åæ ‡ï¼ˆå¯¹è§’çº¿åˆ†å¸ƒï¼‰")
        print(f"  é’»å­”XèŒƒå›´: {x_min:.2f} - {x_max:.2f} (è·¨åº¦: {x_span:.2f} ç±³)")
        print(f"  é’»å­”YèŒƒå›´: {y_min:.2f} - {y_max:.2f} (è·¨åº¦: {y_span:.2f} ç±³)")
        print(f"  æ”¯æ¶XèŒƒå›´: {coords[:, 0].min():.2f} - {coords[:, 0].max():.2f}")
        print(f"  æ”¯æ¶YèŒƒå›´: {coords[:, 1].min():.2f} - {coords[:, 1].max():.2f}")
    else:
        # ç®€åŒ–åæ ‡ï¼šå‡è®¾æ”¯æ¶æ²¿å·¥ä½œé¢çº¿æ€§æ’åˆ—
        coords = np.zeros((num_supports, 2))
        coords[:, 0] = np.arange(num_supports) * 1.5  # æ”¯æ¶é—´è·1.5ç±³
        coords[:, 1] = 0  # å‡è®¾åœ¨åŒä¸€æ¡çº¿ä¸Š
        print(f"âš ï¸ æœªæ‰¾åˆ°åœ°è´¨æ–‡ä»¶ï¼Œä½¿ç”¨ç®€åŒ–åæ ‡ç³»ç»Ÿ")
    
    coord_df = pd.DataFrame({
        'support_id': support_ids,
        'x': coords[:, 0],
        'y': coords[:, 1]
    })
    
    print(f"âœ“ åˆ›å»ºäº† {num_supports} ä¸ªæ”¯æ¶çš„åæ ‡")
    print(f"  æ”¯æ¶ç¼–å·èŒƒå›´: {support_ids[0]} - {support_ids[-1]}")
    
    return coord_df


def create_sequence_dataset(merged_df, seq_len=5, pred_len=1):
    """
    åˆ›å»ºåºåˆ—æ•°æ®é›†ç”¨äºSTGCNè®­ç»ƒ
    
    å‚æ•°:
        seq_len: å†å²åºåˆ—é•¿åº¦ï¼ˆä½¿ç”¨è¿‡å»å¤šå°‘ä¸ªå·¥ä½œå¾ªç¯ï¼‰
        pred_len: é¢„æµ‹é•¿åº¦ï¼ˆé¢„æµ‹æœªæ¥å¤šå°‘ä¸ªå·¥ä½œå¾ªç¯ï¼‰
    
    è¿”å›:
        sequences: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤5: æ„å»ºåºåˆ—æ•°æ®é›†")
    print("=" * 60)
    print(f"é…ç½®: å†å²é•¿åº¦={seq_len}, é¢„æµ‹é•¿åº¦={pred_len}")
    
    # ç¡®å®šç‰¹å¾åˆ—
    pressure_features = ['åˆæ’‘åŠ›å€¼', 'æœ«é˜»åŠ›å€¼', 'å‹åŠ›å¢é‡', 'å‹åŠ›å¢é•¿ç‡', 'å¾ªç¯æ—¶é•¿_ç§’', 'å‹åŠ›å˜åŒ–é€Ÿç‡']
    geo_features = [col for col in merged_df.columns if col.startswith('geo_')]
    time_features = ['å°æ—¶', 'æ˜ŸæœŸå‡ ']
    
    all_features = pressure_features + geo_features + time_features
    print(f"ç‰¹å¾ç»´åº¦: {len(all_features)} (å‹åŠ›={len(pressure_features)}, åœ°è´¨={len(geo_features)}, æ—¶é—´={len(time_features)})")
    
    sequences = []
    support_ids = sorted(merged_df['æ”¯æ¶å·'].unique())
    
    for support_id in support_ids:
        support_data = merged_df[merged_df['æ”¯æ¶å·'] == support_id].copy()
        
        # è‡³å°‘éœ€è¦ seq_len + pred_len æ¡è®°å½•
        if len(support_data) < seq_len + pred_len:
            continue
        
        for i in range(len(support_data) - seq_len - pred_len + 1):
            # å†å²åºåˆ—
            hist_seq = support_data.iloc[i:i+seq_len]
            # é¢„æµ‹ç›®æ ‡
            target_seq = support_data.iloc[i+seq_len:i+seq_len+pred_len]
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            X = hist_seq[all_features].values  # (seq_len, num_features)
            y_init = target_seq['åˆæ’‘åŠ›å€¼'].values  # (pred_len,)
            y_final = target_seq['æœ«é˜»åŠ›å€¼'].values  # (pred_len,)
            
            sequences.append({
                'support_id': support_id,
                'start_time': hist_seq.iloc[0]['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'],
                'end_time': target_seq.iloc[-1]['å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'],
                'X': X,
                'y_init': y_init,
                'y_final': y_final,
                'feature_names': all_features
            })
    
    print(f"âœ“ ç”Ÿæˆäº† {len(sequences)} ä¸ªè®­ç»ƒæ ·æœ¬")
    print(f"  æ¶‰åŠæ”¯æ¶æ•°: {len(support_ids)}")
    print(f"  æ ·æœ¬å½¢çŠ¶: X={sequences[0]['X'].shape}, y_final={sequences[0]['y_final'].shape}")
    
    return sequences, support_ids, all_features


def save_processed_data(merged_df, sequences, coord_df, feature_names, output_dir):
    """
    ä¿å­˜å¤„ç†åçš„æ•°æ®
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤6: ä¿å­˜å¤„ç†åçš„æ•°æ®")
    print("=" * 60)
    
    # 1. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ•°æ®
    merged_file = os.path.join(output_dir, 'merged_pressure_data.csv')
    merged_df.to_csv(merged_file, index=False, encoding='utf-8-sig')
    print(f"âœ“ ä¿å­˜åˆå¹¶æ•°æ®: {merged_file}")
    
    # 2. ä¿å­˜æ”¯æ¶åæ ‡
    coord_file = os.path.join(output_dir, 'support_coordinates.csv')
    coord_df.to_csv(coord_file, index=False, encoding='utf-8-sig')
    print(f"âœ“ ä¿å­˜æ”¯æ¶åæ ‡: {coord_file}")
    
    # 3. ä¿å­˜åºåˆ—æ•°æ®ï¼ˆNumPyæ ¼å¼ï¼Œä¾¿äºå¿«é€ŸåŠ è½½ï¼‰
    X_list = [seq['X'] for seq in sequences]
    y_init_list = [seq['y_init'] for seq in sequences]
    y_final_list = [seq['y_final'] for seq in sequences]
    support_ids = [seq['support_id'] for seq in sequences]
    
    X_array = np.array(X_list)  # (N, seq_len, features)
    y_init_array = np.array(y_init_list)
    y_final_array = np.array(y_final_list)
    
    # â­ å…³é”®ä¿®å¤ï¼šå¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼Œè§£å†³æ•°å€¼èŒƒå›´å·®å¼‚è¿‡å¤§çš„é—®é¢˜
    print("\nâ­ å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†...")
    from sklearn.preprocessing import StandardScaler
    
    # ä¿å­˜åŸå§‹æ•°æ®å½¢çŠ¶
    n_samples, seq_len, n_features = X_array.shape
    print(f"  åŸå§‹å½¢çŠ¶: {X_array.shape}")
    
    # é‡å¡‘ä¸º2Dè¿›è¡Œæ ‡å‡†åŒ–
    X_reshaped = X_array.reshape(-1, n_features)
    
    # æ˜¾ç¤ºæ ‡å‡†åŒ–å‰çš„æ•°å€¼èŒƒå›´
    print(f"  æ ‡å‡†åŒ–å‰èŒƒå›´: [{X_reshaped.min():.2f}, {X_reshaped.max():.2f}]")
    print(f"  æ ‡å‡†åŒ–å‰ç‰¹å¾ç»Ÿè®¡:")
    for i, fname in enumerate(feature_names):
        feat_data = X_reshaped[:, i]
        print(f"    {fname}: [{feat_data.min():.2f}, {feat_data.max():.2f}] (std={feat_data.std():.2f})")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_normalized = scaler.fit_transform(X_reshaped)
    
    # æ¢å¤å½¢çŠ¶
    X_normalized = X_normalized.reshape(n_samples, seq_len, n_features)
    
    print(f"  æ ‡å‡†åŒ–åèŒƒå›´: [{X_normalized.min():.2f}, {X_normalized.max():.2f}]")
    print(f"  æ ‡å‡†åŒ–åå‡å€¼: {X_normalized.mean():.4f} (åº”æ¥è¿‘0)")
    print(f"  æ ‡å‡†åŒ–åæ ‡å‡†å·®: {X_normalized.std():.4f} (åº”æ¥è¿‘1)")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
    if np.isnan(X_normalized).any():
        print(f"  âš ï¸ è­¦å‘Šï¼šæ ‡å‡†åŒ–åå‡ºç° {np.isnan(X_normalized).sum()} ä¸ªNaNå€¼")
        X_normalized = np.nan_to_num(X_normalized, nan=0.0)
    if np.isinf(X_normalized).any():
        print(f"  âš ï¸ è­¦å‘Šï¼šæ ‡å‡†åŒ–åå‡ºç° {np.isinf(X_normalized).sum()} ä¸ªInfå€¼")
        X_normalized = np.nan_to_num(X_normalized, posinf=0.0, neginf=0.0)
    
    print(f"  âœ“ ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
    
    # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°ï¼Œä¾›åç»­é¢„æµ‹ä½¿ç”¨
    scaler_params = {
        'mean': scaler.mean_.tolist(),
        'scale': scaler.scale_.tolist(),
        'feature_names': feature_names
    }
    
    import json
    scaler_file = os.path.join(output_dir, 'feature_scaler.json')
    with open(scaler_file, 'w', encoding='utf-8') as f:
        json.dump(scaler_params, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ ä¿å­˜æ ‡å‡†åŒ–å‚æ•°: feature_scaler.json")
    
    # ä¿å­˜æ ‡å‡†åŒ–åçš„æ•°æ®
    np.savez_compressed(
        os.path.join(output_dir, 'sequence_dataset.npz'),
        X=X_normalized,  # ä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®
        y_init=y_init_array,
        y_final=y_final_array,
        support_ids=np.array(support_ids),
        feature_names=feature_names
    )
    print(f"âœ“ ä¿å­˜åºåˆ—æ•°æ®: sequence_dataset.npz (å·²æ ‡å‡†åŒ–)")
    
    # 4. ä¿å­˜æ•°æ®æ‘˜è¦
    summary = {
        'num_samples': len(sequences),
        'num_supports': coord_df.shape[0],
        'num_features': len(feature_names),
        'seq_len': sequences[0]['X'].shape[0] if sequences else 0,
        'pred_len': sequences[0]['y_final'].shape[0] if sequences else 0,
        'date_range': f"{merged_df['å·¥ä½œå¾ªç¯å¼€å§‹æ—¶é—´'].min()} to {merged_df['å·¥ä½œå¾ªç¯ç»“æŸæ—¶é—´'].max()}",
        'feature_names': feature_names
    }
    
    import json
    with open(os.path.join(output_dir, 'dataset_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
    print(f"âœ“ ä¿å­˜æ•°æ®æ‘˜è¦: dataset_summary.json")
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ–‡ä»¶åˆ—è¡¨:")
    for f in os.listdir(output_dir):
        fpath = os.path.join(output_dir, f)
        size = os.path.getsize(fpath) / 1024 / 1024
        print(f"  - {f} ({size:.2f} MB)")


def main():
    """
    ä¸»æµç¨‹
    """
    print("=" * 60)
    print("çŸ¿å‹æ•°æ®é¢„å¤„ç†æµç¨‹")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(INIT_FILE):
        print(f"âŒ åˆæ’‘åŠ›æ–‡ä»¶ä¸å­˜åœ¨: {INIT_FILE}")
        return
    if not os.path.exists(FINAL_FILE):
        print(f"âŒ æœ«é˜»åŠ›æ–‡ä»¶ä¸å­˜åœ¨: {FINAL_FILE}")
        return
    
    # æ­¥éª¤1: åŠ è½½æ•°æ®
    init_df, final_df = load_and_clean_data(INIT_FILE, FINAL_FILE)
    
    # æ­¥éª¤2: åˆå¹¶å’Œç‰¹å¾å·¥ç¨‹
    merged_df = merge_and_extract_features(init_df, final_df)
    
    # æ­¥éª¤3: å…ˆåˆ›å»ºæ”¯æ¶åæ ‡ï¼ˆéœ€è¦åœ¨æ·»åŠ åœ°è´¨ç‰¹å¾ä¹‹å‰ï¼‰
    coord_df = create_support_coordinates(merged_df, GEO_FILE)
    
    # ä¿å­˜ä¸´æ—¶åæ ‡æ–‡ä»¶ä¾›åœ°è´¨ç‰¹å¾åŒ¹é…ä½¿ç”¨
    temp_coord_file = os.path.join(OUTPUT_DIR, 'support_coordinates.csv')
    coord_df.to_csv(temp_coord_file, index=False, encoding='utf-8-sig')
    
    # æ­¥éª¤4: æ·»åŠ åœ°è´¨ç‰¹å¾ï¼ˆä½¿ç”¨åˆšåˆ›å»ºçš„åæ ‡ï¼‰
    merged_df, geo_features = add_geological_features(merged_df, GEO_FILE, temp_coord_file)
    
    # æ­¥éª¤5: æ„å»ºåºåˆ—æ•°æ®é›†
    sequences, support_ids, feature_names = create_sequence_dataset(
        merged_df, 
        seq_len=5,  # ä½¿ç”¨è¿‡å»5ä¸ªå·¥ä½œå¾ªç¯
        pred_len=1   # é¢„æµ‹ä¸‹1ä¸ªå·¥ä½œå¾ªç¯
    )
    
    # æ­¥éª¤6: ä¿å­˜ç»“æœ
    save_processed_data(merged_df, sequences, coord_df, feature_names, OUTPUT_DIR)
    
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºSTGCNè®­ç»ƒ")


if __name__ == '__main__':
    main()
