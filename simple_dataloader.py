# -*- coding: utf-8 -*-
"""
æç®€æ•°æ®åŠ è½½å™¨ - é›¶ç‰¹å¾å·¥ç¨‹ï¼Œçº¯ç²¹åŠ è½½NPZ
ä½œç”¨ï¼šåªåšæ•°æ®åŠ è½½å’ŒåŸºæœ¬å¼ é‡è½¬æ¢ï¼Œé¿å…ä»»ä½•æ•°å€¼é£é™©
"""

import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


class SafeDataLoader:
    """é˜²å¼¹æ•°æ®åŠ è½½å™¨ - å†…ç½®å…¨é¢æ•°å€¼æ£€æŸ¥"""
    
    def __init__(self, npz_path='processed_data/sequence_dataset.npz'):
        """
        å‚æ•°:
            npz_path: NPZæ–‡ä»¶è·¯å¾„
        """
        self.npz_path = npz_path
        self.scaler_X = None
        self.scaler_y = None
        
    def load_and_split(self, train_ratio=0.7, val_ratio=0.15, random_seed=42):
        """
        åŠ è½½æ•°æ®å¹¶åˆ‡åˆ†ï¼ŒåŒ…å«å®Œæ•´çš„æ•°å€¼å®‰å…¨æ£€æŸ¥
        
        è¿”å›:
            (X_train, y_train, X_val, y_val, X_test, y_test)
            å…¨éƒ¨ä¸º numpy.ndarray æ ¼å¼
        """
        print("=" * 70)
        print("ğŸ“‚ æ­¥éª¤1: åŠ è½½æ•°æ®æ–‡ä»¶")
        print("=" * 70)
        
        # åŠ è½½NPZ
        data = np.load(self.npz_path, allow_pickle=True)
        X = data['X']  # å½¢çŠ¶: (N, seq_len, features)
        y = data['y_final']  # å½¢çŠ¶: (N, 1) - ä½¿ç”¨æœ«é˜»åŠ›ä½œä¸ºç›®æ ‡
        
        print(f"âœ“ åŸå§‹æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        print(f"âœ“ ç‰¹å¾æ•°: {X.shape[-1]}")
        
        # å…³é”®æ£€æŸ¥1: NaN/Inf
        print("\nğŸ” æ•°å€¼å¥åº·æ£€æŸ¥...")
        if np.isnan(X).any() or np.isinf(X).any():
            raise ValueError("âŒ Xä¸­åŒ…å«NaNæˆ–Infï¼")
        if np.isnan(y).any() or np.isinf(y).any():
            raise ValueError("âŒ yä¸­åŒ…å«NaNæˆ–Infï¼")
        print("âœ“ æ— NaN/Inf")
        
        # å…³é”®æ£€æŸ¥2: æ•°å€¼èŒƒå›´
        print(f"âœ“ XèŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")
        print(f"âœ“ yèŒƒå›´: [{y.min():.2f}, {y.max():.2f}] MPa")
        
        # åˆ‡åˆ†æ•°æ®
        print("\n" + "=" * 70)
        print("âœ‚ï¸ æ­¥éª¤2: åˆ‡åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†")
        print("=" * 70)
        
        np.random.seed(random_seed)
        n_samples = len(X)
        indices = np.random.permutation(n_samples)
        
        train_end = int(n_samples * train_ratio)
        val_end = train_end + int(n_samples * val_ratio)
        
        train_idx = indices[:train_end]
        val_idx = indices[train_end:val_end]
        test_idx = indices[val_end:]
        
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        X_test, y_test = X[test_idx], y[test_idx]
        
        print(f"âœ“ è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬ ({train_ratio*100:.0f}%)")
        print(f"âœ“ éªŒè¯é›†: {len(X_val):,} æ ·æœ¬ ({val_ratio*100:.0f}%)")
        print(f"âœ“ æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬ ({(1-train_ratio-val_ratio)*100:.0f}%)")
        
        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def normalize_data(self, X_train, y_train, X_val, y_val, X_test, y_test):
        """
        æ ‡å‡†åŒ–æ•°æ®ï¼ˆå‡å€¼0ï¼Œæ ‡å‡†å·®1ï¼‰
        
        æ³¨æ„ï¼šåªç”¨è®­ç»ƒé›†æ‹Ÿåˆï¼Œç„¶ååº”ç”¨åˆ°éªŒè¯/æµ‹è¯•é›†
        """
        print("\n" + "=" * 70)
        print("ğŸ“Š æ­¥éª¤3: æ•°æ®æ ‡å‡†åŒ–")
        print("=" * 70)
        
        # é‡å¡‘Xç”¨äºæ ‡å‡†åŒ–
        n_train, seq_len, n_features = X_train.shape
        X_train_flat = X_train.reshape(-1, n_features)
        X_val_flat = X_val.reshape(-1, n_features)
        X_test_flat = X_test.reshape(-1, n_features)
        
        # æ ‡å‡†åŒ–X
        self.scaler_X = StandardScaler()
        X_train_norm = self.scaler_X.fit_transform(X_train_flat)
        X_val_norm = self.scaler_X.transform(X_val_flat)
        X_test_norm = self.scaler_X.transform(X_test_flat)
        
        # æ¢å¤å½¢çŠ¶
        X_train_norm = X_train_norm.reshape(X_train.shape)
        X_val_norm = X_val_norm.reshape(X_val.shape)
        X_test_norm = X_test_norm.reshape(X_test.shape)
        
        print(f"âœ“ Xæ ‡å‡†åŒ–å®Œæˆ: å‡å€¼â‰ˆ0, æ ‡å‡†å·®â‰ˆ1")
        print(f"  è®­ç»ƒé›†èŒƒå›´: [{X_train_norm.min():.2f}, {X_train_norm.max():.2f}]")
        
        # æ ‡å‡†åŒ–y
        self.scaler_y = StandardScaler()
        y_train_norm = self.scaler_y.fit_transform(y_train)
        y_val_norm = self.scaler_y.transform(y_val)
        y_test_norm = self.scaler_y.transform(y_test)
        
        print(f"âœ“ yæ ‡å‡†åŒ–å®Œæˆ")
        print(f"  åŸå§‹èŒƒå›´: [{y_train.min():.2f}, {y_train.max():.2f}] MPa")
        print(f"  å½’ä¸€åŒ–èŒƒå›´: [{y_train_norm.min():.2f}, {y_train_norm.max():.2f}]")
        
        # æœ€ç»ˆæ£€æŸ¥
        for name, X_norm in [('è®­ç»ƒ', X_train_norm), ('éªŒè¯', X_val_norm), ('æµ‹è¯•', X_test_norm)]:
            if np.isnan(X_norm).any() or np.isinf(X_norm).any():
                raise ValueError(f"âŒ {name}é›†Xæ ‡å‡†åŒ–åå‡ºç°NaN/Infï¼")
        
        print("âœ“ æ ‡å‡†åŒ–åæ— NaN/Inf")
        
        return X_train_norm, y_train_norm, X_val_norm, y_val_norm, X_test_norm, y_test_norm
    
    def create_dataloaders(self, X_train, y_train, X_val, y_val, X_test, y_test,
                          batch_size=32, num_workers=0):
        """
        åˆ›å»ºPyTorch DataLoader
        
        å‚æ•°:
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼ˆWindowså»ºè®®è®¾ä¸º0ï¼‰
        
        è¿”å›:
            (train_loader, val_loader, test_loader)
        """
        print("\n" + "=" * 70)
        print("ğŸ”„ æ­¥éª¤4: åˆ›å»ºDataLoader")
        print("=" * 70)
        
        # è½¬æ¢ä¸ºTensor
        X_train_t = torch.FloatTensor(X_train)
        y_train_t = torch.FloatTensor(y_train)
        X_val_t = torch.FloatTensor(X_val)
        y_val_t = torch.FloatTensor(y_val)
        X_test_t = torch.FloatTensor(X_test)
        y_test_t = torch.FloatTensor(y_test)
        
        print(f"âœ“ Tensorè½¬æ¢å®Œæˆ")
        print(f"  è®­ç»ƒé›†: X={X_train_t.shape}, y={y_train_t.shape}")
        
        # åˆ›å»ºDataset
        train_dataset = TensorDataset(X_train_t, y_train_t)
        val_dataset = TensorDataset(X_val_t, y_val_t)
        test_dataset = TensorDataset(X_test_t, y_test_t)
        
        # åˆ›å»ºDataLoader
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True  # åŠ é€ŸGPUä¼ è¾“
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )
        
        print(f"âœ“ DataLoaderåˆ›å»ºå®Œæˆ")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        
        return train_loader, val_loader, test_loader
    
    def inverse_transform_y(self, y_norm):
        """åæ ‡å‡†åŒ–yï¼ˆç”¨äºæœ€ç»ˆé¢„æµ‹ç»“æœï¼‰"""
        if self.scaler_y is None:
            raise ValueError("å¿…é¡»å…ˆè°ƒç”¨normalize_data()ï¼")
        return self.scaler_y.inverse_transform(y_norm)


def quick_load(batch_size=32, data_path='processed_data/sequence_dataset.npz'):
    """
    ä¸€é”®åŠ è½½å‡½æ•° - æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼
    
    ä½¿ç”¨ç¤ºä¾‹:
        train_loader, val_loader, test_loader, loader = quick_load(batch_size=32)
        
        for X_batch, y_batch in train_loader:
            # è®­ç»ƒä»£ç 
            pass
    
    è¿”å›:
        train_loader, val_loader, test_loader, loaderå¯¹è±¡
    """
    loader = SafeDataLoader(data_path)
    
    # åŠ è½½å’Œåˆ‡åˆ†
    X_train, y_train, X_val, y_val, X_test, y_test = loader.load_and_split()
    
    # æ ‡å‡†åŒ–
    X_train, y_train, X_val, y_val, X_test, y_test = loader.normalize_data(
        X_train, y_train, X_val, y_val, X_test, y_test
    )
    
    # åˆ›å»ºDataLoader
    train_loader, val_loader, test_loader = loader.create_dataloaders(
        X_train, y_train, X_val, y_val, X_test, y_test,
        batch_size=batch_size
    )
    
    print("\n" + "=" * 70)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒ")
    print("=" * 70)
    
    return train_loader, val_loader, test_loader, loader


if __name__ == '__main__':
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨...\n")
    
    # å¿«é€ŸåŠ è½½
    train_loader, val_loader, test_loader, loader = quick_load(batch_size=32)
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    print("\nğŸ” æµ‹è¯•æ‰¹æ¬¡æ•°æ®...")
    for X_batch, y_batch in train_loader:
        print(f"âœ“ æ‰¹æ¬¡å½¢çŠ¶: X={X_batch.shape}, y={y_batch.shape}")
        print(f"âœ“ XèŒƒå›´: [{X_batch.min():.2f}, {X_batch.max():.2f}]")
        print(f"âœ“ yèŒƒå›´: [{y_batch.min():.2f}, {y_batch.max():.2f}]")
        
        # æ£€æŸ¥NaN
        if torch.isnan(X_batch).any() or torch.isinf(X_batch).any():
            print("âŒ Xæ‰¹æ¬¡åŒ…å«NaN/Infï¼")
        else:
            print("âœ“ Xæ‰¹æ¬¡å¥åº·")
        
        if torch.isnan(y_batch).any() or torch.isinf(y_batch).any():
            print("âŒ yæ‰¹æ¬¡åŒ…å«NaN/Infï¼")
        else:
            print("âœ“ yæ‰¹æ¬¡å¥åº·")
        
        break  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
    
    print("\nâœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•é€šè¿‡ï¼")
