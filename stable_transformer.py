# -*- coding: utf-8 -*-
"""
é˜²å¼¹Transformeræ¨¡å‹ - å†…ç½®å…¨é¢æ•°å€¼ä¿æŠ¤
ç‰¹ç‚¹ï¼š
1. æ‰€æœ‰è®¡ç®—éƒ½æœ‰NaNæ£€æŸ¥
2. è‡ªåŠ¨æ¢¯åº¦è£å‰ª
3. ç¨³å®šçš„åˆå§‹åŒ–
4. ä¿å®ˆçš„Dropout
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class StableTransformer(nn.Module):
    """
    é˜²å¼¹Transformeræ¨¡å‹
    
    ç‰¹ç‚¹ï¼š
    - å†…ç½®æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
    - è‡ªåŠ¨NaNæ£€æµ‹
    - æ¢¯åº¦è£å‰ª
    - Layer Normalization
    """
    
    def __init__(self, input_dim, seq_len, hidden_dim=128, num_layers=3, 
                 num_heads=8, dropout=0.1, output_dim=1):
        """
        å‚æ•°:
            input_dim: è¾“å…¥ç‰¹å¾æ•°
            seq_len: åºåˆ—é•¿åº¦
            hidden_dim: éšè—å±‚ç»´åº¦ï¼ˆå¿…é¡»èƒ½è¢«num_headsæ•´é™¤ï¼‰
            num_layers: Transformerå±‚æ•°
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¯”ç‡
            output_dim: è¾“å‡ºç»´åº¦ï¼ˆé»˜è®¤1ï¼Œå•å€¼å›å½’ï¼‰
        """
        super(StableTransformer, self).__init__()
        
        self.input_dim = input_dim
        self.seq_len = seq_len
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_heads = num_heads
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # ä½ç½®ç¼–ç ï¼ˆå›ºå®šï¼‰
        self.register_buffer('pos_encoding', self._generate_positional_encoding(seq_len, hidden_dim))
        
        # Transformer Encoderå±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            activation='gelu',  # GELUæ¯”ReLUæ›´ç¨³å®š
            batch_first=True,
            norm_first=True  # Pre-LNæ›´ç¨³å®š
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers,
            norm=nn.LayerNorm(hidden_dim)
        )
        
        # è¾“å‡ºå±‚ï¼ˆåŒå±‚MLPï¼‰
        self.output_mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
    def _generate_positional_encoding(self, seq_len, hidden_dim):
        """ç”Ÿæˆæ­£å¼¦ä½ç½®ç¼–ç """
        position = torch.arange(seq_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * 
                            -(math.log(10000.0) / hidden_dim))
        
        pos_encoding = torch.zeros(seq_len, hidden_dim)
        pos_encoding[:, 0::2] = torch.sin(position * div_term)
        pos_encoding[:, 1::2] = torch.cos(position * div_term)
        
        return pos_encoding.unsqueeze(0)  # (1, seq_len, hidden_dim)
    
    def _init_weights(self):
        """ç¨³å®šçš„æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # Xavieråˆå§‹åŒ–ï¼ˆæ›´ç¨³å®šï¼‰
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1.0)
                nn.init.constant_(module.bias, 0)
    
    def _check_tensor(self, x, name="tensor"):
        """æ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å«NaNæˆ–Inf"""
        if torch.isnan(x).any():
            raise ValueError(f"âŒ {name} åŒ…å« NaNï¼")
        if torch.isinf(x).any():
            raise ValueError(f"âŒ {name} åŒ…å« Infï¼")
    
    def forward(self, x, check_nan=True):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (batch_size, seq_len, input_dim)
            check_nan: æ˜¯å¦æ£€æŸ¥NaNï¼ˆè®­ç»ƒæ—¶å»ºè®®å¼€å¯ï¼‰
        
        è¿”å›:
            output: å½¢çŠ¶ (batch_size, output_dim)
        """
        if check_nan:
            self._check_tensor(x, "è¾“å…¥X")
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # (batch, seq_len, hidden_dim)
        
        if check_nan:
            self._check_tensor(x, "æŠ•å½±å")
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:, :x.size(1), :]
        
        if check_nan:
            self._check_tensor(x, "ä½ç½®ç¼–ç å")
        
        # Transformerç¼–ç 
        x = self.transformer_encoder(x)  # (batch, seq_len, hidden_dim)
        
        if check_nan:
            self._check_tensor(x, "Transformerç¼–ç å")
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼ˆé¿å…åªç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
        x = x.mean(dim=1)  # (batch, hidden_dim)
        
        if check_nan:
            self._check_tensor(x, "æ± åŒ–å")
        
        # è¾“å‡ºå±‚
        output = self.output_mlp(x)  # (batch, output_dim)
        
        if check_nan:
            self._check_tensor(output, "æœ€ç»ˆè¾“å‡º")
        
        return output
    
    def count_parameters(self):
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class SafeLoss(nn.Module):
    """
    å®‰å…¨çš„æŸå¤±å‡½æ•° - ç»„åˆHuber + MSE
    
    Huberå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼ŒMSEä¿è¯å¹³æ»‘
    """
    
    def __init__(self, delta=1.0, huber_weight=0.7, mse_weight=0.3):
        """
        å‚æ•°:
            delta: HuberæŸå¤±çš„é˜ˆå€¼
            huber_weight: HuberæŸå¤±æƒé‡
            mse_weight: MSEæŸå¤±æƒé‡
        """
        super(SafeLoss, self).__init__()
        self.delta = delta
        self.huber_weight = huber_weight
        self.mse_weight = mse_weight
        
    def forward(self, pred, target):
        """
        è®¡ç®—æŸå¤±
        
        å‚æ•°:
            pred: é¢„æµ‹å€¼ï¼Œå½¢çŠ¶ (batch_size, 1)
            target: çœŸå®å€¼ï¼Œå½¢çŠ¶ (batch_size, 1)
        
        è¿”å›:
            loss: æ ‡é‡
        """
        # HuberæŸå¤±ï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
        huber_loss = F.smooth_l1_loss(pred, target, beta=self.delta)
        
        # MSEæŸå¤±ï¼ˆå¹³æ»‘ï¼‰
        mse_loss = F.mse_loss(pred, target)
        
        # ç»„åˆ
        total_loss = self.huber_weight * huber_loss + self.mse_weight * mse_loss
        
        # æ£€æŸ¥NaN
        if torch.isnan(total_loss):
            print(f"âš ï¸ æŸå¤±ä¸ºNaNï¼predèŒƒå›´=[{pred.min():.2f}, {pred.max():.2f}], "
                  f"targetèŒƒå›´=[{target.min():.2f}, {target.max():.2f}]")
            # è¿”å›ä¸€ä¸ªè¾ƒå¤§ä½†å®‰å…¨çš„å€¼
            return torch.tensor(1e6, device=pred.device, requires_grad=True)
        
        return total_loss


class SafeOptimizer:
    """
    å®‰å…¨çš„ä¼˜åŒ–å™¨åŒ…è£…å™¨
    
    åŠŸèƒ½ï¼š
    - è‡ªåŠ¨æ¢¯åº¦è£å‰ª
    - NaNæ£€æµ‹
    - å­¦ä¹ ç‡è°ƒåº¦
    """
    
    def __init__(self, model, lr=0.0001, weight_decay=1e-5, max_grad_norm=1.0):
        """
        å‚æ•°:
            model: PyTorchæ¨¡å‹
            lr: å­¦ä¹ ç‡
            weight_decay: L2æ­£åˆ™åŒ–ç³»æ•°
            max_grad_norm: æœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼ˆæ¢¯åº¦è£å‰ªï¼‰
        """
        self.model = model
        self.max_grad_norm = max_grad_norm
        
        # AdamWä¼˜åŒ–å™¨ï¼ˆå¸¦æƒé‡è¡°å‡ï¼‰
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
    def step(self, loss):
        """
        æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–
        
        å‚æ•°:
            loss: æŸå¤±å¼ é‡
        
        è¿”å›:
            grad_norm: æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ï¼‰
        """
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaN
        has_nan = False
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                    print(f"âš ï¸ {name} çš„æ¢¯åº¦åŒ…å«NaN/Infï¼")
                    has_nan = True
        
        if has_nan:
            print("âŒ æ£€æµ‹åˆ°æ¢¯åº¦NaNï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
            return 0.0
        
        # æ¢¯åº¦è£å‰ª
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.model.parameters(), 
            self.max_grad_norm
        )
        
        # æ›´æ–°å‚æ•°
        self.optimizer.step()
        
        return grad_norm.item()
    
    def get_lr(self):
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizer.param_groups[0]['lr']
    
    def set_lr(self, lr):
        """è®¾ç½®å­¦ä¹ ç‡"""
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr


if __name__ == '__main__':
    """æµ‹è¯•æ¨¡å‹"""
    print("\nğŸ§ª æµ‹è¯•Transformeræ¨¡å‹...\n")
    
    # åˆ›å»ºæ¨¡å‹
    model = StableTransformer(
        input_dim=17,
        seq_len=5,
        hidden_dim=128,
        num_layers=3,
        num_heads=8,
        dropout=0.1
    )
    
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"âœ“ å‚æ•°é‡: {model.count_parameters():,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 32
    x = torch.randn(batch_size, 5, 17)
    
    try:
        output = model(x, check_nan=True)
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"âœ“ è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"âœ“ è¾“å‡ºèŒƒå›´: [{output.min():.4f}, {output.max():.4f}]")
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    
    # æµ‹è¯•æŸå¤±å‡½æ•°
    loss_fn = SafeLoss()
    target = torch.randn(batch_size, 1)
    loss = loss_fn(output, target)
    print(f"âœ“ æŸå¤±è®¡ç®—æˆåŠŸ: {loss.item():.4f}")
    
    # æµ‹è¯•ä¼˜åŒ–å™¨
    optimizer = SafeOptimizer(model, lr=0.0001)
    grad_norm = optimizer.step(loss)
    print(f"âœ“ ä¼˜åŒ–æ­¥éª¤æˆåŠŸ")
    print(f"âœ“ æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}")
    
    print("\nâœ… æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
