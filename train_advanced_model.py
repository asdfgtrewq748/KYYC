# -*- coding: utf-8 -*-
"""
é«˜çº§åœ°è´¨æ„ŸçŸ¥æ¨¡å‹è®­ç»ƒè„šæœ¬
========================================
ä½¿ç”¨æ–°çš„AdvancedGeoPressureModelè¿›è¡Œè®­ç»ƒ
"""

import torch
import numpy as np
import time
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from simple_dataloader import SafeDataLoader
from advanced_geo_pressure_model import AdvancedGeoPressureModel


def calculate_r2(y_true, y_pred):
    """è®¡ç®—RÂ²åˆ†æ•°"""
    ss_res = torch.sum((y_true - y_pred) ** 2)
    ss_tot = torch.sum((y_true - y_true.mean()) ** 2)
    r2 = 1 - ss_res / ss_tot
    return r2.item()


def train_one_epoch(model, train_loader, optimizer, loss_fn, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    total_loss = 0.0
    total_samples = 0
    
    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ Epoch {epoch} - è®­ç»ƒé˜¶æ®µ")
    print(f"{'='*70}")
    
    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):
        # ç§»åˆ°GPU
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        
        try:
            # å‰å‘ä¼ æ’­
            pred = model(X_batch, return_attention=False)
            
            # è®¡ç®—æŸå¤±
            loss = loss_fn(pred, y_batch)
            
            # æ£€æŸ¥æŸå¤±
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âš ï¸ Batch {batch_idx}: æŸå¤±ä¸ºNaN/Infï¼Œè·³è¿‡")
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # æ›´æ–°å‚æ•°
            optimizer.step()
            
            # ç´¯è®¡æŸå¤±
            total_loss += loss.item() * len(X_batch)
            total_samples += len(X_batch)
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 50 == 0 or batch_idx == 0:
                avg_loss = total_loss / total_samples
                print(f"  Batch {batch_idx+1}/{len(train_loader)}: "
                      f"Loss={loss.item():.4f}, AvgLoss={avg_loss:.4f}")
        
        except Exception as e:
            print(f"âŒ Batch {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
            continue
    
    avg_loss = total_loss / total_samples if total_samples > 0 else float('inf')
    
    print(f"\nâœ“ Epoch {epoch} è®­ç»ƒå®Œæˆ - å¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    return avg_loss


def validate(model, val_loader, loss_fn, device, data_loader, epoch):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    
    total_loss = 0.0
    all_preds = []
    all_targets = []
    
    print(f"\n{'='*70}")
    print(f"ğŸ” Epoch {epoch} - éªŒè¯é˜¶æ®µ")
    print(f"{'='*70}")
    
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            
            # å‰å‘ä¼ æ’­
            pred = model(X_batch, return_attention=False)
            
            # è®¡ç®—æŸå¤±
            loss = loss_fn(pred, y_batch)
            
            if torch.isnan(loss) or torch.isinf(loss):
                continue
            
            total_loss += loss.item() * len(X_batch)
            
            # æ”¶é›†é¢„æµ‹å€¼
            all_preds.append(pred.cpu())
            all_targets.append(y_batch.cpu())
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / len(val_loader.dataset)
    
    all_preds = torch.cat(all_preds, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    r2 = calculate_r2(all_targets, all_preds)
    
    # åæ ‡å‡†åŒ–
    all_preds_mpa = data_loader.inverse_transform_y(all_preds.numpy())
    all_targets_mpa = data_loader.inverse_transform_y(all_targets.numpy())
    
    mae_mpa = np.mean(np.abs(all_preds_mpa - all_targets_mpa))
    
    print(f"\nâœ“ éªŒè¯å®Œæˆ")
    print(f"  éªŒè¯æŸå¤±: {avg_loss:.4f}")
    print(f"  RÂ² åˆ†æ•°: {r2:.4f}")
    print(f"  MAE: {mae_mpa:.2f} MPa")
    
    return avg_loss, r2, mae_mpa


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("\n" + "="*70)
    print("ğŸš€ é«˜çº§åœ°è´¨æ„ŸçŸ¥çŸ¿å‹é¢„æµ‹æ¨¡å‹ - è®­ç»ƒç³»ç»Ÿ")
    print("="*70)
    
    # ============ é…ç½®å‚æ•° ============
    EPOCHS = 300
    BATCH_SIZE = 256  # å‡å°æ‰¹æ¬¡å¢åŠ æ›´æ–°é¢‘ç‡
    LEARNING_RATE = 0.0001  # æé«˜å­¦ä¹ ç‡
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"\nâš™ï¸ è®­ç»ƒé…ç½®:")
    print(f"  è®¾å¤‡: {DEVICE}")
    print(f"  è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    
    # ============ åŠ è½½æ•°æ® ============
    print(f"\n{'='*70}")
    print("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®")
    print("="*70)
    
    data_loader = SafeDataLoader('processed_data/sequence_dataset.npz')
    X_train, y_train, X_val, y_val, X_test, y_test = data_loader.load_and_split(
        train_ratio=0.7,
        val_ratio=0.15,
        random_seed=42
    )
    
    # æ ‡å‡†åŒ–
    X_train_norm, y_train_norm, X_val_norm, y_val_norm, X_test_norm, y_test_norm = \
        data_loader.normalize_data(X_train, y_train, X_val, y_val, X_test, y_test)
    
    # åˆ›å»ºDataLoader
    train_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(X_train_norm),
        torch.FloatTensor(y_train_norm)
    )
    val_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(X_val_norm),
        torch.FloatTensor(y_val_norm)
    )
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        pin_memory=True if DEVICE.type == 'cuda' else False
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True if DEVICE.type == 'cuda' else False
    )
    
    print(f"\nâœ“ æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # ============ åˆ›å»ºæ¨¡å‹ ============
    print(f"\n{'='*70}")
    print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹")
    print("="*70)
    
    model = AdvancedGeoPressureModel(
        seq_len=5,
        num_pressure_features=6,
        num_geology_features=9,
        num_time_features=2,
        hidden_dim=512,  # å¤§å¹…å¢åŠ å®¹é‡
        num_stgcn_layers=4,
        num_heads=16,
        dropout=0.2
    )
    
    model = model.to(DEVICE)
    
    print(f"\nâœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  æ¨¡å‹å‚æ•°é‡: {model.count_parameters():,}")
    print(f"  æ¨¡å‹ç±»å‹: AdvancedGeoPressureModel")
    
    # ============ è®­ç»ƒé…ç½® ============
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=5e-4,  # å¢å¼ºæ­£åˆ™åŒ–
        betas=(0.9, 0.999)
    )
    
    # ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=30,
        T_mult=2,
        eta_min=1e-6
    )
    
    # ä½¿ç”¨Huber Lossï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
    loss_fn = torch.nn.HuberLoss(delta=1.0)
    
    # ============ è®­ç»ƒå¾ªç¯ ============
    print(f"\n{'='*70}")
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("="*70)
    
    best_val_loss = float('inf')
    best_r2 = -float('inf')
    patience_counter = 0
    MAX_PATIENCE = 50  # ç»™æ›´å¤šæ—¶é—´ä¼˜åŒ–
    
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_r2': [],
        'val_mae': [],
        'learning_rate': []
    }
    
    start_time = time.time()
    
    for epoch in range(1, EPOCHS + 1):
        # è®­ç»ƒ
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, DEVICE, epoch)
        
        # éªŒè¯
        val_loss, val_r2, val_mae = validate(model, val_loader, loss_fn, DEVICE, data_loader, epoch)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_r2'].append(val_r2)
        history['val_mae'].append(val_mae)
        history['learning_rate'].append(current_lr)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_r2 = val_r2
            patience_counter = 0
            
            # ä¿å­˜æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_r2': val_r2,
                'val_mae': val_mae,
                'model_config': {
                    'seq_len': 5,
                    'num_pressure_features': 6,
                    'num_geology_features': 9,
                    'num_time_features': 2,
                    'hidden_dim': 512,
                    'num_stgcn_layers': 4,
                    'num_heads': 16
                }
            }, 'advanced_best_model.pth')
            
            print(f"\nğŸ‰ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  RÂ²: {val_r2:.4f}")
            print(f"  MAE: {val_mae:.2f} MPa")
        else:
            patience_counter += 1
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= MAX_PATIENCE:
            print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼{MAX_PATIENCE}è½®æœªæ”¹å–„")
            break
        
        # æ˜¾ç¤ºè¿›åº¦
        elapsed = time.time() - start_time
        print(f"\nğŸ“Š Epoch {epoch} æ€»ç»“:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} (æœ€ä½³: {best_val_loss:.4f})")
        print(f"  RÂ²: {val_r2:.4f} (æœ€ä½³: {best_r2:.4f})")
        print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
        print(f"  å·²ç”¨æ—¶é—´: {elapsed/60:.1f} åˆ†é’Ÿ")
        print(f"  è€å¿ƒå€¼: {patience_counter}/{MAX_PATIENCE}")
    
    # ============ è®­ç»ƒå®Œæˆ ============
    total_time = time.time() - start_time
    
    print(f"\n{'='*70}")
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"  æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"  æœ€ä½³RÂ²: {best_r2:.4f}")
    print(f"  æ¨¡å‹ä¿å­˜ä½ç½®: advanced_best_model.pth")
    
    # ä¿å­˜è®­ç»ƒå†å²ï¼ˆè½¬æ¢numpyç±»å‹ä¸ºPythonç±»å‹ï¼‰
    history_to_save = {
        'train_loss': [float(x) for x in history['train_loss']],
        'val_loss': [float(x) for x in history['val_loss']],
        'val_r2': [float(x) for x in history['val_r2']],
        'val_mae': [float(x) for x in history['val_mae']],
        'learning_rate': [float(x) for x in history['learning_rate']]
    }
    with open('advanced_training_history.json', 'w', encoding='utf-8') as f:
        json.dump(history_to_save, f, indent=2, ensure_ascii=False)
    print(f"  è®­ç»ƒå†å²: advanced_training_history.json")
    
    # ============ æµ‹è¯•é›†è¯„ä¼° ============
    print(f"\n{'='*70}")
    print("ğŸ”¬ æµ‹è¯•é›†è¯„ä¼°")
    print("="*70)
    
    model.eval()
    test_dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(X_test_norm),
        torch.FloatTensor(y_test_norm)
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False
    )
    
    test_preds = []
    test_targets = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(DEVICE)
            pred = model(X_batch, return_attention=False)
            test_preds.append(pred.cpu())
            test_targets.append(y_batch.cpu())
    
    test_preds = torch.cat(test_preds, dim=0)
    test_targets = torch.cat(test_targets, dim=0)
    
    test_r2 = calculate_r2(test_targets, test_preds)
    test_preds_mpa = data_loader.inverse_transform_y(test_preds.numpy())
    test_targets_mpa = data_loader.inverse_transform_y(test_targets.numpy())
    test_mae = np.mean(np.abs(test_preds_mpa - test_targets_mpa))
    
    print(f"\nâœ“ æµ‹è¯•é›†ç»“æœ:")
    print(f"  RÂ²: {test_r2:.4f}")
    print(f"  MAE: {test_mae:.2f} MPa")
    
    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼")
    print("="*70)


if __name__ == '__main__':
    main()
