"""
ä¼˜åŒ–æ¨¡å‹è®­ç»ƒè„šæœ¬
é’ˆå¯¹RÂ²=0.5é—®é¢˜çš„æ”¹è¿›è®­ç»ƒ
"""

import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
import json
import time
from datetime import datetime
from optimized_model import SimpleButEffectiveModel
from simple_dataloader import SafeDataLoader

def train_one_epoch(model, train_loader, optimizer, loss_fn, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    batch_count = 0
    
    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = loss_fn(predictions, y_batch)
        
        loss.backward()
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        batch_count += 1
        
        if (batch_idx + 1) % 100 == 0:
            print(f"  Batch {batch_idx+1}/{len(train_loader)}: Loss={loss.item():.4f}, AvgLoss={total_loss/batch_count:.4f}")
    
    return total_loss / batch_count

def validate(model, val_loader, loss_fn, device):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            
            predictions = model(X_batch)
            loss = loss_fn(predictions, y_batch)
            
            total_loss += loss.item()
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(y_batch.cpu().numpy())
    
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    # è®¡ç®—RÂ²
    ss_res = np.sum((targets - predictions) ** 2)
    ss_tot = np.sum((targets - targets.mean()) ** 2)
    r2 = 1 - ss_res / ss_tot
    
    # è®¡ç®—MAE (åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦)
    mae = np.mean(np.abs(targets - predictions))
    mae_mpa = mae * 17.31  # è¿‘ä¼¼åå½’ä¸€åŒ–
    
    return total_loss / len(val_loader), r2, mae_mpa

def main():
    print("="*70)
    print("ğŸš€ ä¼˜åŒ–æ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    print("="*70)
    
    # ============ é…ç½®å‚æ•° ============
    EPOCHS = 200
    BATCH_SIZE = 512
    LEARNING_RATE = 0.001  # æé«˜å­¦ä¹ ç‡
    WEIGHT_DECAY = 1e-3  # å¢å¼ºæ­£åˆ™åŒ–
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"\nâš™ï¸ è®­ç»ƒé…ç½®:")
    print(f"  è®¾å¤‡: {DEVICE}")
    print(f"  è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"  æƒé‡è¡°å‡: {WEIGHT_DECAY}")
    
    # ============ åŠ è½½æ•°æ® ============
    print("\n" + "="*70)
    print("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®")
    print("="*70)
    
    loader = SafeDataLoader(npz_path='processed_data/sequence_dataset.npz')
    data_tuple = loader.load_and_split()
    
    # load_and_splitè¿”å›çš„æ˜¯å½’ä¸€åŒ–åçš„6ä¸ªæ•°ç»„
    X_train, y_train, X_val, y_val, X_test, y_test = data_tuple
    
    print(f"\næ•°æ®å½¢çŠ¶æ£€æŸ¥:")
    print(f"  X_train: {X_train.shape}, y_train: {y_train.shape}")
    print(f"  X_val: {X_val.shape}, y_val: {y_val.shape}")
    print(f"  X_test: {X_test.shape}, y_test: {y_test.shape}")
    
    # åˆ›å»ºDataLoader
    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    print(f"\nâœ“ æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # ============ åˆ›å»ºæ¨¡å‹ ============
    print("\n" + "="*70)
    print("ğŸ—ï¸ åˆ›å»ºä¼˜åŒ–æ¨¡å‹")
    print("="*70)
    
    model = SimpleButEffectiveModel(
        seq_len=5,
        num_pressure_features=6,
        num_geology_features=9,
        num_time_features=2,
        hidden_dim=128,
        num_lstm_layers=2,
        dropout=0.3
    ).to(DEVICE)
    
    param_count = sum(p.numel() for p in model.parameters())
    print(f"\nâœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  æ¨¡å‹å‚æ•°é‡: {param_count:,}")
    print(f"  æ¨¡å‹ç±»å‹: SimpleButEffectiveModel (LSTM-based)")
    
    # ============ è®­ç»ƒè®¾ç½® ============
    loss_fn = nn.MSELoss()
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY
    )
    
    # ä½¿ç”¨OneCycleLR - æ›´æ¿€è¿›çš„å­¦ä¹ ç‡ç­–ç•¥
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=LEARNING_RATE,
        epochs=EPOCHS,
        steps_per_epoch=len(train_loader),
        pct_start=0.3,  # 30%çš„æ—¶é—´ç”¨äºwarmup
        anneal_strategy='cos'
    )
    
    # ============ å¼€å§‹è®­ç»ƒ ============
    print("\n" + "="*70)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("="*70)
    
    best_val_loss = float('inf')
    best_r2 = -float('inf')
    patience_counter = 0
    MAX_PATIENCE = 40
    
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_r2': [],
        'val_mae': [],
        'learning_rate': []
    }
    
    start_time = time.time()
    
    for epoch in range(1, EPOCHS + 1):
        epoch_start = time.time()
        
        print(f"\n{'='*70}")
        print(f"ğŸ“ˆ Epoch {epoch} - è®­ç»ƒé˜¶æ®µ")
        print(f"{'='*70}")
        
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, DEVICE)
        
        print(f"\nâœ“ Epoch {epoch} è®­ç»ƒå®Œæˆ - å¹³å‡æŸå¤±: {train_loss:.4f}")
        
        print(f"\n{'='*70}")
        print(f"ğŸ” Epoch {epoch} - éªŒè¯é˜¶æ®µ")
        print(f"{'='*70}\n")
        
        val_loss, r2, mae_mpa = validate(model, val_loader, loss_fn, DEVICE)
        
        print(f"âœ“ éªŒè¯å®Œæˆ")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"  RÂ² åˆ†æ•°: {r2:.4f}")
        print(f"  MAE: {mae_mpa:.2f} MPa")
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_r2'].append(r2)
        history['val_mae'].append(mae_mpa)
        history['learning_rate'].append(optimizer.param_groups[0]['lr'])
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if r2 > best_r2:
            best_r2 = r2
            best_val_loss = val_loss
            patience_counter = 0
            
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'r2': r2,
                'mae': mae_mpa,
                'model_config': {
                    'seq_len': 5,
                    'num_pressure_features': 6,
                    'num_geology_features': 9,
                    'num_time_features': 2,
                    'hidden_dim': 128,
                    'num_lstm_layers': 2,
                    'dropout': 0.3
                }
            }
            
            torch.save(checkpoint, 'optimized_best_model.pth')
            print(f"\nğŸ‰ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  RÂ²: {r2:.4f}")
            print(f"  MAE: {mae_mpa:.2f} MPa")
        else:
            patience_counter += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ¯ä¸ªbatchè°ƒç”¨ä¸€æ¬¡ï¼‰
        current_lr = optimizer.param_groups[0]['lr']
        
        elapsed = (time.time() - start_time) / 60
        
        print(f"\nğŸ“Š Epoch {epoch} æ€»ç»“:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} (æœ€ä½³: {best_val_loss:.4f})")
        print(f"  RÂ²: {r2:.4f} (æœ€ä½³: {best_r2:.4f})")
        print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
        print(f"  å·²ç”¨æ—¶é—´: {elapsed:.1f} åˆ†é’Ÿ")
        print(f"  è€å¿ƒå€¼: {patience_counter}/{MAX_PATIENCE}")
        
        # æ—©åœ
        if patience_counter >= MAX_PATIENCE:
            print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼{MAX_PATIENCE}è½®æœªæ”¹å–„")
            break
    
    # ============ è®­ç»ƒå®Œæˆ ============
    total_time = (time.time() - start_time) / 60
    
    print(f"\n{'='*70}")
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*70}")
    print(f"  æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f} åˆ†é’Ÿ")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"  æœ€ä½³RÂ²: {best_r2:.4f}")
    print(f"  æ¨¡å‹ä¿å­˜ä½ç½®: optimized_best_model.pth")
    
    # ä¿å­˜è®­ç»ƒå†å²ï¼ˆè½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼‰
    history_serializable = {
        'train_loss': [float(x) for x in history['train_loss']],
        'val_loss': [float(x) for x in history['val_loss']],
        'val_r2': [float(x) for x in history['val_r2']],
        'val_mae': [float(x) for x in history['val_mae']],
        'learning_rate': [float(x) for x in history['learning_rate']]
    }
    
    with open('optimized_training_history.json', 'w') as f:
        json.dump(history_serializable, f, indent=2)
    print(f"  è®­ç»ƒå†å²: optimized_training_history.json")
    
    # ============ æµ‹è¯•é›†è¯„ä¼° ============
    print(f"\n{'='*70}")
    print("ğŸ”¬ æµ‹è¯•é›†è¯„ä¼°")
    print(f"{'='*70}\n")
    
    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint = torch.load('optimized_best_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    test_loss, test_r2, test_mae = validate(model, test_loader, loss_fn, DEVICE)
    
    print(f"âœ“ æµ‹è¯•é›†ç»“æœ:")
    print(f"  RÂ²: {test_r2:.4f}")
    print(f"  MAE: {test_mae:.2f} MPa")
    
    print(f"\n{'='*70}")
    print("ğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()
