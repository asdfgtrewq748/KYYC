# -*- coding: utf-8 -*-
"""
å®‰å…¨è®­ç»ƒè„šæœ¬ - å‘½ä»¤è¡Œç‰ˆæœ¬
ç‰¹ç‚¹ï¼š
1. å®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¾èµ–STGCN.py
2. é›¶ç‰¹å¾å·¥ç¨‹
3. å…¨é¢æ•°å€¼ä¿æŠ¤
4. å®æ—¶ç›‘æ§NaN
"""

import torch
import numpy as np
import time
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from simple_dataloader import quick_load
from stable_transformer import StableTransformer, SafeLoss, SafeOptimizer


def calculate_r2(y_true, y_pred):
    """è®¡ç®—RÂ²åˆ†æ•°"""
    ss_res = torch.sum((y_true - y_pred) ** 2)
    ss_tot = torch.sum((y_true - y_true.mean()) ** 2)
    r2 = 1 - ss_res / ss_tot
    return r2.item()


def train_one_epoch(model, train_loader, optimizer, loss_fn, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    total_loss = 0.0
    total_samples = 0
    nan_count = 0
    
    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ Epoch {epoch} - è®­ç»ƒé˜¶æ®µ")
    print(f"{'='*70}")
    
    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):
        # ç§»åˆ°GPU
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        
        try:
            # å‰å‘ä¼ æ’­ï¼ˆå¸¦NaNæ£€æŸ¥ï¼‰
            pred = model(X_batch, check_nan=True)
            
            # è®¡ç®—æŸå¤±
            loss = loss_fn(pred, y_batch)
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âš ï¸ Batch {batch_idx}: æŸå¤±ä¸ºNaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                nan_count += 1
                if nan_count > 10:
                    raise ValueError("âŒ è¿ç»­10ä¸ªæ‰¹æ¬¡å‡ºç°NaNï¼Œè®­ç»ƒä¸­æ­¢ï¼")
                continue
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            grad_norm = optimizer.step(loss)
            
            # ç´¯è®¡æŸå¤±
            total_loss += loss.item() * len(X_batch)
            total_samples += len(X_batch)
            
            # æ‰“å°è¿›åº¦ï¼ˆæ¯50ä¸ªæ‰¹æ¬¡ï¼‰
            if (batch_idx + 1) % 50 == 0 or batch_idx == 0:
                avg_loss = total_loss / total_samples
                print(f"  Batch {batch_idx+1}/{len(train_loader)}: "
                      f"Loss={loss.item():.4f}, "
                      f"AvgLoss={avg_loss:.4f}, "
                      f"GradNorm={grad_norm:.4f}")
        
        except Exception as e:
            print(f"âŒ Batch {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
            nan_count += 1
            if nan_count > 10:
                raise
            continue
    
    avg_loss = total_loss / total_samples if total_samples > 0 else float('inf')
    
    print(f"\nâœ“ Epoch {epoch} è®­ç»ƒå®Œæˆ")
    print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"  NaNæ‰¹æ¬¡æ•°: {nan_count}/{len(train_loader)}")
    
    return avg_loss


def validate(model, val_loader, loss_fn, device, data_loader, epoch):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    
    total_loss = 0.0
    all_preds = []
    all_targets = []
    
    print(f"\n{'='*70}")
    print(f"ğŸ” Epoch {epoch} - éªŒè¯é˜¶æ®µ")
    print(f"{'='*70}")
    
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            
            # å‰å‘ä¼ æ’­ï¼ˆä¸æ£€æŸ¥NaNï¼Œæé€Ÿï¼‰
            pred = model(X_batch, check_nan=False)
            
            # è®¡ç®—æŸå¤±
            loss = loss_fn(pred, y_batch)
            
            if torch.isnan(loss) or torch.isinf(loss):
                print("âš ï¸ éªŒè¯æ—¶å‡ºç°NaNæŸå¤±")
                continue
            
            total_loss += loss.item() * len(X_batch)
            
            # æ”¶é›†é¢„æµ‹å€¼å’ŒçœŸå®å€¼
            all_preds.append(pred.cpu())
            all_targets.append(y_batch.cpu())
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / len(val_loader.dataset)
    
    # è®¡ç®—RÂ²
    all_preds = torch.cat(all_preds, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    r2 = calculate_r2(all_targets, all_preds)
    
    # åæ ‡å‡†åŒ–ï¼ˆè½¬æ¢ä¸ºåŸå§‹MPaï¼‰
    all_preds_mpa = data_loader.inverse_transform_y(all_preds.numpy())
    all_targets_mpa = data_loader.inverse_transform_y(all_targets.numpy())
    
    # è®¡ç®—MAEï¼ˆMPaï¼‰
    mae_mpa = np.mean(np.abs(all_preds_mpa - all_targets_mpa))
    
    print(f"\nâœ“ éªŒè¯å®Œæˆ")
    print(f"  éªŒè¯æŸå¤±: {avg_loss:.4f}")
    print(f"  RÂ² åˆ†æ•°: {r2:.4f}")
    print(f"  MAE: {mae_mpa:.2f} MPa")
    
    return avg_loss, r2, mae_mpa


def train_model(config):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    å‚æ•°:
        config: è®­ç»ƒé…ç½®å­—å…¸
    """
    print("\n" + "="*70)
    print("ğŸš€ å®‰å…¨è®­ç»ƒç³»ç»Ÿå¯åŠ¨")
    print("="*70)
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"  å­¦ä¹ ç‡: {config['lr']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['epochs']}")
    print(f"  æ¢¯åº¦è£å‰ª: {config['max_grad_norm']}")
    print(f"  è®¾å¤‡: {config['device']}")
    
    # 1. åŠ è½½æ•°æ®
    print("\n" + "="*70)
    print("ğŸ“‚ åŠ è½½æ•°æ®")
    print("="*70)
    
    train_loader, val_loader, test_loader, data_loader = quick_load(
        batch_size=config['batch_size'],
        data_path=config['data_path']
    )
    
    # è·å–æ•°æ®ç»´åº¦
    X_sample, _ = next(iter(train_loader))
    input_dim = X_sample.shape[-1]
    seq_len = X_sample.shape[1]
    
    print(f"\nâœ“ æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"  åºåˆ—é•¿åº¦: {seq_len}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\n" + "="*70)
    print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹")
    print("="*70)
    
    model = StableTransformer(
        input_dim=input_dim,
        seq_len=seq_len,
        hidden_dim=config['hidden_dim'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        dropout=config['dropout']
    ).to(config['device'])
    
    print(f"âœ“ Transformeræ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  å‚æ•°é‡: {model.count_parameters():,}")
    
    # 3. åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_fn = SafeLoss()
    optimizer = SafeOptimizer(
        model,
        lr=config['lr'],
        weight_decay=config['weight_decay'],
        max_grad_norm=config['max_grad_norm']
    )
    
    print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  ç±»å‹: AdamW")
    print(f"  æƒé‡è¡°å‡: {config['weight_decay']}")
    
    # 4. è®­ç»ƒå¾ªç¯
    print("\n" + "="*70)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("="*70)
    
    best_r2 = -float('inf')
    best_epoch = 0
    patience = config['patience']
    patience_counter = 0
    
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_r2': [],
        'val_mae': []
    }
    
    start_time = time.time()
    
    for epoch in range(1, config['epochs'] + 1):
        epoch_start = time.time()
        
        try:
            # è®­ç»ƒ
            train_loss = train_one_epoch(
                model, train_loader, optimizer, loss_fn, 
                config['device'], epoch
            )
            
            # éªŒè¯
            val_loss, val_r2, val_mae = validate(
                model, val_loader, loss_fn, config['device'],
                data_loader, epoch
            )
            
            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['val_r2'].append(val_r2)
            history['val_mae'].append(val_mae)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_r2 > best_r2:
                best_r2 = val_r2
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æ¨¡å‹
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.optimizer.state_dict(),
                    'val_r2': val_r2,
                    'val_mae': val_mae,
                    'config': config
                }, config['save_path'])
                
                print(f"\nğŸŒŸ æ–°çš„æœ€ä½³æ¨¡å‹ï¼RÂ² = {val_r2:.4f}")
            else:
                patience_counter += 1
                print(f"\n  (æ— æ”¹è¿›ï¼Œpatience: {patience_counter}/{patience})")
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼å·²è¿ç»­{patience}è½®æ— æ”¹è¿›")
                break
            
            epoch_time = time.time() - epoch_start
            print(f"\n  Epochç”¨æ—¶: {epoch_time:.1f}ç§’")
        
        except Exception as e:
            print(f"\nâŒ Epoch {epoch} å¤±è´¥: {e}")
            print("ç»§ç»­ä¸‹ä¸€è½®...")
            continue
    
    total_time = time.time() - start_time
    
    # 5. è®­ç»ƒæ€»ç»“
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆ")
    print("="*70)
    print(f"\næ€»ç»“:")
    print(f"  æ€»ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"  æœ€ä½³Epoch: {best_epoch}")
    print(f"  æœ€ä½³RÂ²: {best_r2:.4f}")
    print(f"  æœ€ä½³MAE: {min(history['val_mae']):.2f} MPa")
    
    # 6. æœ€ç»ˆæµ‹è¯•
    print("\n" + "="*70)
    print("ğŸ§ª æœ€ç»ˆæµ‹è¯•")
    print("="*70)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆPyTorch 2.9å…¼å®¹ï¼‰
    checkpoint = torch.load(config['save_path'], weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # æµ‹è¯•
    test_loss, test_r2, test_mae = validate(
        model, test_loader, loss_fn, config['device'],
        data_loader, epoch=-1
    )
    
    print(f"\næœ€ç»ˆæµ‹è¯•ç»“æœ:")
    print(f"  æµ‹è¯•RÂ²: {test_r2:.4f}")
    print(f"  æµ‹è¯•MAE: {test_mae:.2f} MPa")
    
    # 7. ä¿å­˜è®­ç»ƒå†å²
    history_path = Path(config['save_path']).parent / 'training_history.json'
    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump({
            'history': history,
            'best_epoch': best_epoch,
            'best_val_r2': best_r2,
            'test_r2': test_r2,
            'test_mae': test_mae,
            'total_time_minutes': total_time / 60,
            'config': config
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")
    
    return model, history


def main():
    """ä¸»å‡½æ•°"""
    
    # è®­ç»ƒé…ç½®
    config = {
        # æ•°æ®
        'data_path': 'processed_data/sequence_dataset.npz',
        'batch_size': 32,
        
        # æ¨¡å‹
        'hidden_dim': 128,
        'num_layers': 3,
        'num_heads': 8,
        'dropout': 0.1,
        
        # ä¼˜åŒ–
        'lr': 0.0001,  # ä¿å®ˆçš„å­¦ä¹ ç‡
        'weight_decay': 1e-5,
        'max_grad_norm': 1.0,  # æ¢¯åº¦è£å‰ª
        
        # è®­ç»ƒ
        'epochs': 50,
        'patience': 10,  # æ—©åœè€å¿ƒå€¼
        
        # è®¾å¤‡
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        
        # ä¿å­˜
        'save_path': 'safe_best_model.pth'
    }
    
    # æ‰“å°é…ç½®
    print("\n" + "="*70)
    print("âš™ï¸ è®­ç»ƒé…ç½®")
    print("="*70)
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # å¼€å§‹è®­ç»ƒ
    try:
        model, history = train_model(config)
        
        print("\n" + "="*70)
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("="*70)
        print(f"\næ¨¡å‹å·²ä¿å­˜: {config['save_path']}")
        print(f"å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åŠ è½½æ¨¡å‹:")
        print(f"  checkpoint = torch.load('{config['save_path']}')")
        print(f"  model.load_state_dict(checkpoint['model_state_dict'])")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
