"""æœ€ç»ˆéªŒè¯ï¼šç¡®è®¤æ•°æ®é›†å¯ä»¥å®‰å…¨è®­ç»ƒ"""
import numpy as np
import json

print("=" * 60)
print("ğŸ” æœ€ç»ˆè®­ç»ƒå‰éªŒè¯")
print("=" * 60)

# 1. åŠ è½½æ•°æ®é›†
print("\n1ï¸âƒ£ åŠ è½½æ•°æ®é›†...")
data = np.load('processed_data/sequence_dataset.npz', allow_pickle=True)
X = data['X']
y_final = data['y_final']
feature_names = data['feature_names']

print(f"   âœ“ Xå½¢çŠ¶: {X.shape}")
print(f"   âœ“ yå½¢çŠ¶: {y_final.shape}")
print(f"   âœ“ ç‰¹å¾æ•°é‡: {len(feature_names)}")

# 2. æ£€æŸ¥æ ‡å‡†åŒ–
print("\n2ï¸âƒ£ éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ...")
print(f"   Xçš„å‡å€¼: {X.mean():.6f} (ç›®æ ‡: â‰ˆ0)")
print(f"   Xçš„æ ‡å‡†å·®: {X.std():.6f} (ç›®æ ‡: â‰ˆ1)")
print(f"   Xçš„èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")

all_good = True

if abs(X.mean()) > 0.01:
    print(f"   âš ï¸ è­¦å‘Šï¼šå‡å€¼åç¦»0è¾ƒå¤š")
    all_good = False
else:
    print(f"   âœ“ å‡å€¼æ­£å¸¸")

if abs(X.std() - 1.0) > 0.1:
    print(f"   âš ï¸ è­¦å‘Šï¼šæ ‡å‡†å·®åç¦»1è¾ƒå¤š")
    all_good = False
else:
    print(f"   âœ“ æ ‡å‡†å·®æ­£å¸¸")

# 3. æ£€æŸ¥NaN/Inf
print("\n3ï¸âƒ£ æ£€æŸ¥æ•°å€¼é—®é¢˜...")
has_nan = np.isnan(X).any()
has_inf = np.isinf(X).any()

print(f"   æ˜¯å¦æœ‰NaN: {'âŒ æœ‰' if has_nan else 'âœ“ æ— '}")
print(f"   æ˜¯å¦æœ‰Inf: {'âŒ æœ‰' if has_inf else 'âœ“ æ— '}")

if has_nan or has_inf:
    all_good = False

# 4. æ£€æŸ¥æ ‡å‡†åŒ–å‚æ•°
print("\n4ï¸âƒ£ æ£€æŸ¥æ ‡å‡†åŒ–å‚æ•°...")
try:
    with open('processed_data/feature_scaler.json', 'r', encoding='utf-8') as f:
        scaler_params = json.load(f)
    print(f"   âœ“ æ ‡å‡†åŒ–å‚æ•°æ–‡ä»¶å­˜åœ¨")
    print(f"   âœ“ åŒ…å« {len(scaler_params['mean'])} ä¸ªç‰¹å¾çš„å‚æ•°")
except:
    print(f"   âš ï¸ æ ‡å‡†åŒ–å‚æ•°æ–‡ä»¶ç¼ºå¤±æˆ–æŸå")
    all_good = False

# 5. æ£€æŸ¥åœ°è´¨ç‰¹å¾
print("\n5ï¸âƒ£ æ£€æŸ¥åœ°è´¨ç‰¹å¾...")
geo_feature_count = sum(1 for name in feature_names if 'geo_' in name)
print(f"   åœ°è´¨ç‰¹å¾æ•°é‡: {geo_feature_count}")

if geo_feature_count == 9:
    print(f"   âœ“ åœ°è´¨ç‰¹å¾å®Œæ•´")
else:
    print(f"   âš ï¸ åœ°è´¨ç‰¹å¾æ•°é‡å¼‚å¸¸ï¼ˆåº”è¯¥æ˜¯9ä¸ªï¼‰")
    all_good = False

# æ£€æŸ¥åœ°è´¨ç‰¹å¾çš„å˜åŒ–æ€§
geo_indices = [i for i, name in enumerate(feature_names) if 'geo_' in name]
if geo_indices:
    first_geo = X[:, :, geo_indices[0]].flatten()
    unique_count = np.unique(first_geo).shape[0]
    print(f"   ç¬¬ä¸€ä¸ªåœ°è´¨ç‰¹å¾çš„å”¯ä¸€å€¼æ•°é‡: {unique_count}")
    if unique_count > 1:
        print(f"   âœ“ åœ°è´¨ç‰¹å¾æœ‰çœŸå®å˜åŒ–")
    else:
        print(f"   âš ï¸ åœ°è´¨ç‰¹å¾å¯èƒ½å…¨éƒ¨ç›¸åŒ")
        all_good = False

# 6. ç›®æ ‡å˜é‡æ£€æŸ¥
print("\n6ï¸âƒ£ æ£€æŸ¥ç›®æ ‡å˜é‡...")
print(f"   yçš„èŒƒå›´: [{y_final.min():.2f}, {y_final.max():.2f}]")
print(f"   yçš„å‡å€¼: {y_final.mean():.2f}")

if np.isnan(y_final).any() or np.isinf(y_final).any():
    print(f"   âš ï¸ ç›®æ ‡å˜é‡æœ‰NaNæˆ–Inf")
    all_good = False
else:
    print(f"   âœ“ ç›®æ ‡å˜é‡æ­£å¸¸")

# 7. æ¨¡æ‹Ÿä¸€ä¸ªå°æ‰¹æ¬¡è®¡ç®—
print("\n7ï¸âƒ£ æ¨¡æ‹Ÿæ‰¹æ¬¡è®¡ç®—...")
try:
    batch_size = 32
    sample_batch = X[:batch_size]
    sample_y = y_final[:batch_size]
    
    # ç®€å•çš„çº¿æ€§è®¡ç®—
    mean_input = sample_batch.mean()
    mean_output = sample_y.mean()
    
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   æ‰¹æ¬¡Xå‡å€¼: {mean_input:.6f}")
    print(f"   æ‰¹æ¬¡yå‡å€¼: {mean_output:.6f}")
    print(f"   âœ“ æ‰¹æ¬¡è®¡ç®—æ­£å¸¸")
except Exception as e:
    print(f"   âš ï¸ æ‰¹æ¬¡è®¡ç®—å¤±è´¥: {e}")
    all_good = False

# æœ€ç»ˆç»“è®º
print("\n" + "=" * 60)
if all_good:
    print("âœ… éªŒè¯é€šè¿‡ï¼æ•°æ®é›†å¯ä»¥å®‰å…¨è®­ç»ƒ")
    print("\nğŸ“‹ æ¨èé…ç½®:")
    print("   æ¨¡å‹: Transformer")
    print("   å­¦ä¹ ç‡: 0.0001")
    print("   æ‰¹æ¬¡å¤§å°: 32")
    print("   ç‰¹å¾å·¥ç¨‹: âŒ å…³é—­")
    print("   åœ°è´¨ç‰¹å¾: âœ… å¯ç”¨")
    print("\nğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
else:
    print("âš ï¸ éªŒè¯æœªå®Œå…¨é€šè¿‡ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é—®é¢˜")

print("=" * 60)
